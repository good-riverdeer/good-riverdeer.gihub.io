<!DOCTYPE html><html lang="ko" mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="[paper-review] Learning to Understand Traffic Signs" /><meta property="og:locale" content="ko" /><meta name="description" content="Guo, Y., Feng, W., Yin, F., Xue, T., Mei, S., &amp; Liu, C. L. (2021, October). Learning to Understand Traffic Signs. In Proceedings of the 29th ACM International Conference on Multimedia (pp. 2076-2084)." /><meta property="og:description" content="Guo, Y., Feng, W., Yin, F., Xue, T., Mei, S., &amp; Liu, C. L. (2021, October). Learning to Understand Traffic Signs. In Proceedings of the 29th ACM International Conference on Multimedia (pp. 2076-2084)." /><link rel="canonical" href="https://good-riverdeer.github.io/posts/Learning_to_Understand_Traffic_Signs/" /><meta property="og:url" content="https://good-riverdeer.github.io/posts/Learning_to_Understand_Traffic_Signs/" /><meta property="og:site_name" content="good-riverdeer" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-12-15T12:10:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[paper-review] Learning to Understand Traffic Signs" /><meta name="twitter:site" content="@twitter_username" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"description":"Guo, Y., Feng, W., Yin, F., Xue, T., Mei, S., &amp; Liu, C. L. (2021, October). Learning to Understand Traffic Signs. In Proceedings of the 29th ACM International Conference on Multimedia (pp. 2076-2084).","url":"https://good-riverdeer.github.io/posts/Learning_to_Understand_Traffic_Signs/","@type":"BlogPosting","headline":"[paper-review] Learning to Understand Traffic Signs","dateModified":"2022-01-17T15:56:29+09:00","datePublished":"2021-12-15T12:10:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://good-riverdeer.github.io/posts/Learning_to_Understand_Traffic_Signs/"},"@context":"https://schema.org"}</script><title>[paper-review] Learning to Understand Traffic Signs | good-riverdeer</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="good-riverdeer"><meta name="application-name" content="good-riverdeer"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/school.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">good-riverdeer</a></div><div class="site-subtitle font-italic">딥러닝을 공부하는</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/good-riverdeer" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['riverdeer.youn','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>[paper-review] Learning to Understand Traffic Signs</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>[paper-review] Learning to Understand Traffic Signs</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> riverdeer </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Wed, Dec 15, 2021, 12:10 PM +0900" >Dec 15, 2021<i class="unloaded">2021-12-15T12:10:00+09:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Mon, Jan 17, 2022, 3:56 PM +0900" >Jan 17, 2022<i class="unloaded">2022-01-17T15:56:29+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3156 words">17 min read</span></div></div><div class="post-content"><p><a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475362">Guo, Y., Feng, W., Yin, F., Xue, T., Mei, S., &amp; Liu, C. L. (2021, October). Learning to Understand Traffic Signs. In Proceedings of the 29th ACM International Conference on Multimedia (pp. 2076-2084).</a></p><p><em>개인적인 논문해석을 포함하고 있으며, 의역 및 오역이 남발할 수 있습니다. 올바르지 못한 내용에 대한 피드백을 환영합니다 :)</em></p><hr /><h2 id="1-introduction">1. Introduction</h2><ul><li>최근 교통 표지판(traffic sign)에서 텍스트나 기호를 인식하는 task의 눈에 띄는 발전이 있었음<li>각각의 요소를 따로 인지하는 것은 교통 표지판을 이해하는 첫 단계에 불과<li>본 논문에서는 <strong>“traffic sign understanding”</strong>이라는 새로운 task를 소개<ol><li>교통 표지판에 포함된 요소들을 인식하고<li>그 요소들 간의 관계를 파악하여<li>“semantic description”(<code class="language-plaintext highlighter-rouge">&lt;key: value&gt;</code>의 형태)을 생성하는 것<ul><li><code class="language-plaintext highlighter-rouge">key</code>: 표시 정보 (예: 현재 위치, 차선 번호, 전방 오른쪽 방향)<li><code class="language-plaintext highlighter-rouge">value</code>: 특정 내용 (예: 장소 이름, 도로 이름, 설명 단어)<li>대부분의 교통 정보가 “indicative information + content”의 형태로 구성</ul></ol><li>자율주행, positioning assistance, map correction과 같은 애플리케이션에 활용될 수 있음</ul><p><img data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/146305612-774c9b44-c657-4a14-9619-a4571d99b90a.png" alt="img-description" /></p><p><strong>[@ Traffic sign understanding]</strong></p><ul><li>Traffic sign understaning은 크게 세 가지 subtask로 구성<ul><li>텍스트 및 기호의 위치와 semantic 정보를 추출하는 <strong>Component detection task</strong><li>텍스트와 기호의 인식 결과를 어떻게 조합할 것인지, 구성요소들 간의 관계를 모델링할 수 있는 <strong>Relation reasoning task</strong><ul><li>Graph Convolutional Network과 같은 관계 예측 모델을 사용</ul><li>교통 표지판의 다양한 유형을 분류하는 <strong>Sign classification task</strong><ul><li>다양한 유형의 표지판에 따라같은 기호가 다른 의미를 나타낼 수 있음<li>예) 차선 정보 표지판(그림 1(b))에서의 윗방향 화살표는 “직진”을 의미하고 안내 정보 표지판(그림 1(a))에서는 “전면”을 의미한다.<li>교통 표지판의 유형을 예측함으로써 기호 간의 관계를 파악하는데에도 도움을 줄 수 있을 것</ul></ul></ul><p><strong>[@ CASIA-Tencent Chinese Traffic Sign Understanding Dataset (CTSU Dataset)]</strong></p><ul><li>복잡한 형태의 교통 표지판과 그 semantic description 레이블을 포함하는 첫 번째 데이터셋<li>실제 dashcam 영상에서 잘라낸 5000개의 교통 표지판 이미지를 포함<ul><li>이미지 description, 기호, bounding box, 이미지 내의 텍스트, 기호의 카테고리<li>요소들 간의 관계에 대한 레이블도 지정</ul></ul><p><strong>[@ Contributions]</strong></p><ul><li>교통 표지판 이미지 및 레이블 정보를 포함하는 데이터셋 CTSU 제안<li>다양한 유형의 교통 표지판을 이해하고 description을 생성하기 위한 새로운 unified 모델 제안<li>위 모델의 접근 방식이 traffic sign understanding에 효과적임을 실험을 통해 밝힘</ul><hr /><h2 id="2-related-work">2. Related Work</h2><h3 id="21-traffic-sign-recognition">2.1. Traffic Sign Recognition</h3><ul><li>이전 교통 표지판에 관한 연구는 텍스트의 detection에 관한 연구가 주를 이룸<ul><li><a href="https://iopscience.iop.org/article/10.1088/1757-899X/768/7/072039/meta">Peng et al.</a> proposed a two-stage cascade detection deep learning model, which used improved EAST for text line detection, and changed the size of feature maps to suit the text size of traffic signs.<li><a href="https://ieeexplore.ieee.org/abstract/document/9113429">Hou et al.</a> introduced an Attention Anchor Mechanism (AAM), which is used to weight the bounding boxes and anchor points to detect text in scene and traffic signs.</ul><li>Traffic sign에 대한 공개 datasets는 주로 원형이나 삼각형 형태의 몇몇 간단한 형태의 분류에 대해서만 존재했음<ul><li>German Traffic Sign Recognition Benchmark - <a href="https://www.sciencedirect.com/science/article/pii/S0893608012000457">citation</a><li>DFG Traffic Sign Data Set - <a href="https://ieeexplore.ieee.org/abstract/document/8709983">citation</a><li>Chinese Traffic Sign Database (CTSD) - <a href="https://ieeexplore.ieee.org/abstract/document/7296660">citation</a><li>위 데이터셋들은 복잡한 교통 표지판 속 요소들 사이의 관계에 대한 정보가 없음</ul></ul><h3 id="22-scene-understanding">2.2. Scene Understanding</h3><ul><li>이미지를 기반으로 semantic description을 생성하는 task를 총칭하여 <strong>image caption</strong>이라 할 수 있음<ul><li>Recurrent Neural Network (RNN) 기반<ul><li>CNN만으로는 맥락 정보를 반영할 수 없음<li><a href="https://openaccess.thecvf.com/content_cvpr_2018/html/Zellers_Neural_Motifs_Scene_CVPR_2018_paper.html">Rowan et al.</a> first used LSTM to predict the object categories and then sent the object features and category information into LSTM for relationship prediction.<li><a href="https://openaccess.thecvf.com/content_iccv_2017/html/Li_Scene_Graph_Generation_ICCV_2017_paper.html">Li et al.</a> proposed a network that leveraged the feature of region, phrase, and object to generate scene graph and caption.<li>위 연구들은 RNN이나 LSTM을 사용해 맥락 정보를 반영했음<li>하지만, 공간적 정보(spatial information)을 반영하지 못함</ul><li>Graph Neural Network (GNN) 기반<ul><li>더 나은 scene graph 구조를 생성하기 위해 GNN 기반의 모델들이 제안되었음<li><a href="https://openaccess.thecvf.com/content_ECCV_2018/html/Jianwei_Yang_Graph_R-CNN_for_ECCV_2018_paper.html">Yang et al.</a> proposed an attentional Graph Convolutional Network (aGCN) that uses contextual information to better reason about the relationship between objects<li>이러한 방법들은 이미지 내의 맥락 정보(contextual information)를 포함할 수 있지만, 위치정보나 의미적 정보(semantic information)을 반영할 수 없음</ul></ul></ul><hr /><h2 id="3-ctsu-dataset">3. CTSU Dataset</h2><p>– 중략 –</p><h3 id="33-evaluation-metric">3.3. Evaluation Metric</h3><ul><li>Image caption에서 이용되던 척도들은 자연어 처리에서 비롯된 척도<ul><li>주로, 정답 값과 sequence matching의 형태이거나 유사도를 측정하는 형태<li>교통 표지판의 semantic description은 문법적인 규칙을 신경쓰지 않아도 되는 형태이므로 비효율적임</ul></ul><p><strong>[@ Information Matching (IM)]</strong></p><ol><li>먼저 indicative information(<code class="language-plaintext highlighter-rouge">&lt;key: value&gt;</code>에서 <code class="language-plaintext highlighter-rouge">key</code>)에 대해 ground truth와 예측 값을 매칭<ul><li>각각 예측된 predicted description에 오직 하나의 ground truth description을 매칭할 수 있음</ul><li>매칭 결과에 따라 각각 specific content(<code class="language-plaintext highlighter-rouge">&lt;key: value&gt;</code>에서 <code class="language-plaintext highlighter-rouge">value</code>)가 일치할 때 True Positive를 부여<li>위 결과에 따라 각 이미지당 하나씩의 <code class="language-plaintext highlighter-rouge">recall</code>, <code class="language-plaintext highlighter-rouge">precision</code>, <code class="language-plaintext highlighter-rouge">F1-Measure</code>를 계산</ol><h3 id="34-statistics-and-analysis">3.4. Statistics and Analysis</h3><ul><li>CTSU Dataset contains 5,000 traffic signs<ul><li>16,463 <code class="language-plaintext highlighter-rouge">&lt;key: value&gt;</code> descriptions<li>31,536 relationship instances<li>43,722 components, including 18,280 texts</ul></ul><p><strong>[@ Category Statistics]</strong></p><ul><li>교통 표지판 클래스 별로 복잡도(Complexity)를 계산하고 이를 해석함<ul><li>클래스 $i$에 대한 Complexity $C(c_i)$는 각 클래스 내 샘플들의 components 수($a_{c_i}$)와 각 샘플 내의 정보 엔트로피(information entropy; $H(c_i)$)의 곱으로 정의</ul>\[\begin{matrix} C(c_i) = a_{c_i}*H(c_i) = {1\over \lvert c_i \rvert} \sum_{p\in c_i} N_p * {1\over\lvert c_i \rvert} \sum_{p\in c_i} H_p\\ H_p = -P_{pa}\log P_{pa}-P_{pp}\log P_{pp}-P_{pn}\log P_{pn}\\ \\ \text{where } \lvert c_i \rvert \text{ is the total number of images of class } i\\ N_p \text{ is the total number of components in a particular sign } p\\ P_{pa}, P_{pp}, \text{and } P_{pn} \text{ are the frequency of association relation, pointing relation, }\\ \text{and no relation in the sign } p \end{matrix}\]<li>CTSU 데이터셋은 샘플 수가 많은 클래스에서는 복잡도가 높고, 샘플 수가 비교적 적은 클래스에서는 복잡도가 낮음</ul><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 400 300'%3E%3C/svg%3E" data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/146180434-5a099587-b94d-4041-9785-880d95ee90ba.png" alt="img-description" width="400" height="300" /> <em>sign 클래스 당 복잡도 분석 결과</em></p><p><strong>[@ Detection Imbalance]</strong></p><ul><li>교통 표지판 내의 구성요소들의 분포도 불균형<ul><li>‘text’ 요소는 수가 많고 다른 요소들은 비교적 수가 적음<li>이를 잘 다룰 수 있어야 함</ul></ul><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 400 300'%3E%3C/svg%3E" data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/146180908-282e9783-8892-4f20-994e-7a3ff361ed53.png" alt="image" width="400" height="300" /></p><p><strong>[@ Description Bias]</strong></p><ul><li>indicative information(<code class="language-plaintext highlighter-rouge">&lt;key: value&gt;</code>에서 <code class="language-plaintext highlighter-rouge">key</code>)을 담당하는 ‘text’ 요소는 20%를 미치지 못함<ul><li>‘text’ 요소가 전체 요소의 40%를 차지함에도 불구하고<li>이 때문에 indicative information의 생성에 ‘text’의 “기여”가 과장될 것<li>과장된 ‘text’의 영향을 줄일 수 있도록 모델링해야 함</ul></ul><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 400 300'%3E%3C/svg%3E" data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/146181795-c25974cc-3004-4714-b4bf-e7056ee78a5c.png" alt="img-description" width="400" height="300" /> <em>indicative information을 담당하는 요소들의 분포, ‘text’ 요소는 제외함, 가장 많은 수를 차지하는 ‘Crossroad’의 수도 ‘text’ 요소에 비하면 수가 적은 편</em></p><hr /><h2 id="4-method">4. Method</h2><p><img data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/146183272-592eb482-ca46-44f1-b43d-ab61f8ad576c.png" alt="img-description" /> <em>논문에서 제안하는 모델의 전체 프레임워크</em></p><h3 id="41-component-detection">4.1. Component Detection</h3><ul><li>교통 표지판의 요소들을 object detector를 통해 탐지해내는 과정<li>single-stage anchor-freed object detection model인 FCOS를 사용<ul><li>물체의 중앙을 기준으로 bounding box를 예측하는 FCOS 알고리즘 특성상 두 물체가 겹쳐서 존재하면 잘 탐지하지 못하는 특성이 있음<li>논문에서는 ‘texts’, ‘symbols’, ‘arrowheads’를 각각 따로따로 탐지하는 detection head를 두어 이를 해결했음</ul>\[L_{DET} = L_{FCOS_T} + L_{FCOS_S} + L_{FCOS_A}\] \[\begin{matrix} \text{where } L_{FCOS_T}, L_{FCOS_S}, \text{ and } L_{FCOS_A} \text{ are the single head losses of}\\ \text{texts, symbols, and arrowheads} \end{matrix}\]</ul><p><img data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/146184817-62c0162d-0717-4e7a-9028-4176bfe2b47b.png" alt="img-description" /> <em>두 요소가 겹쳐서 존재하는 경우</em></p><h3 id="42-relation-reasoning">4.2. Relation Reasoning</h3><ul><li>그래프 구조<ul><li>구성요소(‘texts’, ‘symbols’, ‘arrowheads’)를 그래프의 <strong>node</strong><li>이 요소들의 관계를 그래프의 <strong>edge</strong><li>관계를 일부 node들에 대해서만 정의할 수 없음<ul><li>두 요소간 물리적 거리가 가깝다고 해서 꼭 관계를 형성하지는 않음<li>따라서, 모든 node들이 연결된 그래프 구조로 초기화</ul></ul></ul><p><img data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/146186194-4a99c729-bd02-408d-81d8-6adb77085847.png" alt="img-description" /> <em>관계를 형성하고 있음에도 요소간 거리가 멀리 떨어진 경우가 많음: 때문에 논문에서는 fully connected graph 구조를 사용</em></p><p><strong>[@ Feature generation]</strong></p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 300'%3E%3C/svg%3E" data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/146186905-d30b9b10-0516-4d9d-9809-87233d05657b.png" alt="img-description" width="500" height="300" /></p><ul><li>node feature 및 edge feature를 생성하는 방법<ol><li><strong>RoI feature</strong>: object detector가 만든 bounding box에 RoIAlign을 적용<li><strong>Position Mask</strong>: object detector가 만든 bounding box 위치에 masking한 이미지를 fully connected layer에 연결하여 feature 생성<li><strong>Semantic Encoding</strong>: object detector가 예측한 클래스 정보를 fully connected layer에 연결하여 feature 생성 (클래스 정보로는 edge를 표현할 수 없기 떄문에 edge에 대한 Semantic Encoding은 없다)</ol><li>annotations<ul><li>node feature: $F_N = V_{ds} + V_{ps} + V_{ss}=\text{RoI feature} + \text{Position feature} + \text{Semantic feature} \in R^D$<li>edge feature: $F_E = V_{du} + V_{pu}=\text{RoI feature} + \text{Position feature} \in R^D$<ul><li>$D$는 각 feature 차원 수</ul><li>각 feature가 업데이트 되는 것을 $\prime$을 붙임으로써 표현</ul></ul><p><strong>[@ Graph Attention Network (GAT)]</strong></p><ul><li>GAT는 Graph Network에 Attention 메커니즘을 추가하여 node간 edge에 중요도를 부여하여 Graph 구조 정보를 모델링<li>Attention coefficient<ul><li><em>node $N_i$에 연관된 모든 node 및 edge에 대해 영향력이 있는 정도(중요도)를 계산</em><li>edge $E_{ji}$는 node $N_j$에서 node $N_i$로의 관계를 의미<li>$f_A: R^{2D} \to R^D$: fully connected layer<li>$\parallel$: concatenation operation</ul></ul>\[\alpha_{ji} = {\exp(\sigma(f_A(F_{E_{ji}} \parallel F_{N_i})))\over \sum_{k\in \mathbb N_i} \exp(\sigma(f_A(F_{E_{ki}} \parallel F_{N_i})))}\]<ul><li>node feature의 업데이트<ul><li><em>edge feature에 Attention coefficient를 element-wise 곱을 하고 node feature를 concat하여 fully connected layer에 태움</em><li>$\otimes$: element-wise/hadamard product<li>$\sigma$: activation function, 논문에서는 <code class="language-plaintext highlighter-rouge">leaky ReLU</code>사용<li>$f_N: R^{2D} \to R^D$: fully connectec layer</ul></ul>\[F^\prime_{N_i} = \sigma\left(f_N\left(\left(\sum_{j\in\mathbb N_i}\alpha_{ji} \otimes F_{E_{ji}}\right) \parallel F_{N_i}\right)\right)\]<ul><li>edge feature의 업데이트<ul><li><em>두 노드 $N_i$와 $N_j$에 Attention coefficient를 element-wise 곱을 하고 더한 후 edge feature를 concat하여 fully connected layer에 태움</em><li>$f_E: R^{2D} \to R^D$: fullly connected layer</ul></ul>\[F_{E_{ji}}^\prime = \sigma\left(f_E\left((\alpha_{ji}\otimes F_{N_j} + \alpha_{ji} \otimes F_{N_i}) \parallel F_{E_{ji}}\right)\right)\]<ul><li>학습 과정에서는 ground truth bounding box를 통해 학습하여 GAT가 더 잘 수렴할 수 있게하였음</ul><h3 id="43-sign-classification">4.3. Sign Classification</h3><ul><li>교통 표지판의 관계를 파악하는데 교통 표지판의 유형을 분류하는 것이 도움이 될 수 있음<li>backbone feature map 중 가장 작은 feature($FP_s$)와 GAT를 거친 node feature($F_{N}$)들을 더하여 교통 표지판 유형 예측에 사용</ul>\[\begin{matrix} F_{FPN} = f_{FPN}(Conv(Resize(FP_s))),\\ F_{GAT} = \sum_i f_{GAT}(F_{N_i})),\\ f(F_{FPN}, F_{GAT}) = f_{CLS}(F_{FPN} + F_{GAT}) \end{matrix}\]<p><strong>[@ Loss Function]</strong></p><ul><li><strong>section 4.1, 4.2, 4.3</strong>의 요소들의 loss를 선형 결합함</ul>\[\begin{matrix} L=L_{DET} + \lambda_{REL}L_{REL} + \lambda_{CLS}L_{CLS}\\ \text{where } \lambda_{REL} \text{ and } \lambda_{CLS} \text{ are set to 1.0 for training.} \end{matrix}\]<h3 id="44-semantic-description">4.4. Semantic Description</h3><ul><li>Inference objects<ul><li>detected box를 필터링하는 threshold = $0.2$<li>relationship을 필터링하는 relationship threshold = $0.5$<li>text box에 대해서는 사전학습된 OCR모델로 텍스트를 추출함</ul><li><strong>heuristic method</strong> to generate semantic descriptions<ol><li>relation 예측 결과를 통해 relation trees를 구성<li>symbol에 따라 의미 태그 부여<ul><li>교통 표지판 유형에 따라 symbol에 의미를 부여</ul><li><code class="language-plaintext highlighter-rouge">&lt;key: value&gt;</code> structure 구성<ul><li>대부분 <code class="language-plaintext highlighter-rouge">value</code>의 위치에 명확한 요소들이 위치함 (예. 지명, 지하철역명 등등)<li>교통 표지판 유형에 따라 <code class="language-plaintext highlighter-rouge">key</code>의 위치에 명확한 요소들이 위치하는 경우도 있음 (예. lane information sign에서는 차선 번호(lane 1, lane 2, …)가 <code class="language-plaintext highlighter-rouge">key</code>에 위치함)<li>따라서, 해당 작업도 교통 표지판 유형에 따라 부여</ul></ol></ul><hr /><h2 id="5-experiments">5. Experiments</h2><ul><li>CTSU 데이터셋에 대해 실험 수행<blockquote><p>학습과 검증 데이터셋에 대한 설명이 없는 것으로 봐서 아래 실험 결과는 모두 학습 데이터에 대한 실험 결과인 것으로 보인다.</p></blockquote></ul><h3 id="51-implementation-details">5.1. Implementation Details</h3><ul><li>backbone: ImageNet에 사전학습한 ResNet-50, Deformable Convolutional Network (DCN) 테크닉 사용<li>GAT의 레이어 수는 5개<li>50 epochs<li>learning rate 0.01<ul><li>epoch 35, 45에 각각 0.1씩 감소</ul><li>batch size 8 on 2 GPUs</ul><h3 id="52-ablation-studies">5.2. Ablation Studies</h3><p><img data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/146296164-0661b573-85e0-4b9a-af00-4c4e0ecdaa2f.png" alt="image" /></p><p><strong>[@ Multi-head Detection]</strong></p><ul><li>FCOS 특성 상 겹쳐있는 물체를 포착하지 못하는 문제를 밝힘<ul><li>약 1%정도의 성능 향상</ul><li>Faster R-CNN도 겹쳐있는 물체에 대한 해결책이 될 수 있음<ul><li>사전 정의된 anchor 세팅이 성능에 제한이 되었음<li>약 0.7% 정도의 성능 차이</ul></ul><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 400 400'%3E%3C/svg%3E" data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/146296527-a11d3a54-0a9e-444e-81fd-b0cbed784c4c.png" alt="img-description" width="400" height="400" /> <em>기존 FCOS(a)와 논문에서 제안하는 Multi-head detection 모델(b)과의 차이</em></p><p><strong>[@ Semantic Encoding]</strong></p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 400 400'%3E%3C/svg%3E" data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/146296527-a11d3a54-0a9e-444e-81fd-b0cbed784c4c.png" alt="img-description" width="400" height="400" /> <em>semantic feature를 제외했을 때, arrowhead가 아닌 symbol에 relation이 할당됨(c)</em></p><h3 id="53-comparison-with-the-state-of-the-art">5.3. Comparison with the state-of-the-art</h3><p><img data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/146297804-11154732-2ab9-45c5-8800-41d8dff27db1.png" alt="image" /></p><hr /><p><img data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/146298455-34ca91fb-1c3e-4468-ad19-eedeae45476e.png" alt="image" /></p><hr /><h2 id="6-conclusion">6. Conclusion</h2><ul><li>intelligent transportation을 위한 traffic sign understanding이라는 task를 새롭게 제안<li>bounding boxes, relations, semantic description 레이블을 할당한 CTSU Dataset 제안<li>Component detection, relation reasoning, sign classification, semantic description generation의 multi-task 학습 프레임워크를 제안</ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/paper-review/'>paper-review</a>, <a href='/categories/computer-vision/'>Computer Vision</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/deep-learning/" class="post-tag no-text-decoration" >deep_learning</a> <a href="/tags/ocr/" class="post-tag no-text-decoration" >ocr</a> <a href="/tags/form-understanding/" class="post-tag no-text-decoration" >form_understanding</a> <a href="/tags/traffic-sign-understanding/" class="post-tag no-text-decoration" >traffic_sign_understanding</a> <a href="/tags/ctsu-dataset/" class="post-tag no-text-decoration" >CTSU_Dataset</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=[paper-review] Learning to Understand Traffic Signs - good-riverdeer&url=https://good-riverdeer.github.io/posts/Learning_to_Understand_Traffic_Signs/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=[paper-review] Learning to Understand Traffic Signs - good-riverdeer&u=https://good-riverdeer.github.io/posts/Learning_to_Understand_Traffic_Signs/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=[paper-review] Learning to Understand Traffic Signs - good-riverdeer&url=https://good-riverdeer.github.io/posts/Learning_to_Understand_Traffic_Signs/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/DDPM/">[paper-review] Denoising diffusion probabilistic models</a><li><a href="/posts/SLidR/">[paper-review] Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data</a><li><a href="/posts/TEBNER/">[paper-review] TEBNER: Domain Specific Named Entity Recognition with Type Expanded Boundary-aware Network.</a><li><a href="/posts/EfficientNet/">[paper-review] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a><li><a href="/posts/Vision_Transformer/">[paper-review] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/deep-learning/">deep_learning</a> <a class="post-tag" href="/tags/transformer/">transformer</a> <a class="post-tag" href="/tags/autonomous-driving/">autonomous_driving</a> <a class="post-tag" href="/tags/distillation/">distillation</a> <a class="post-tag" href="/tags/document-understanding/">document_understanding</a> <a class="post-tag" href="/tags/lidar/">lidar</a> <a class="post-tag" href="/tags/multimodal/">multimodal</a> <a class="post-tag" href="/tags/named-entity-recognition/">Named_Entity_Recognition</a> <a class="post-tag" href="/tags/ner/">NER</a> <a class="post-tag" href="/tags/ocr/">ocr</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Mask_TextSpotter/"><div class="card-body"> <span class="timeago small" >Nov 22, 2021<i class="unloaded">2021-11-22T18:20:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[paper-review] Mask TextSpotter v3: Segmentation Proposal Network for Robust Scene Text Spotting</h3><div class="text-muted small"><p> Liao, M., Pang, G., Huang, J., Hassner, T., &amp; Bai, X. (2020). Mask textspotter v3: Segmentation proposal network for robust scene text spotting. In Computer Vision–ECCV 2020: 16th European Conf...</p></div></div></a></div><div class="card"> <a href="/posts/EfficientNet/"><div class="card-body"> <span class="timeago small" >Oct 7, 2021<i class="unloaded">2021-10-07T20:48:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[paper-review] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</h3><div class="text-muted small"><p> Tan, M., &amp; Le, Q. (2019, May). Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning (pp. 6105-6114). PMLR. 개인적인 논문해석을 포함하고 ...</p></div></div></a></div><div class="card"> <a href="/posts/DocFormer/"><div class="card-body"> <span class="timeago small" >Dec 23, 2021<i class="unloaded">2021-12-23T13:20:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[paper-review] DocFormer: End-to-End Transformer for Document Understanding</h3><div class="text-muted small"><p> Appalaraju, S., Jasani, B., Kota, B. U., Xie, Y., &amp; Manmatha, R. (2021). DocFormer: End-to-End Transformer for Document Understanding. arXiv preprint arXiv:2106.11539. 개인적인 논문해석을 포함하고 있으며, 의역 ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Mask_TextSpotter/" class="btn btn-outline-primary" prompt="Older"><p>[paper-review] Mask TextSpotter v3: Segmentation Proposal Network for Robust Scene Text Spotting</p></a> <a href="/posts/DocFormer/" class="btn btn-outline-primary" prompt="Newer"><p>[paper-review] DocFormer: End-to-End Transformer for Document Understanding</p></a></div><div id="disqus_thread" class="pt-2 pb-2"><p class="text-center text-muted small"> Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script type="text/javascript"> var disqus_config = function () { this.page.url = 'https://good-riverdeer.github.io/posts/Learning_to_Understand_Traffic_Signs/'; this.page.identifier = '/posts/Learning_to_Understand_Traffic_Signs/'; }; /* Lazy loading */ var disqus_observer = new IntersectionObserver(function (entries) { if(entries[0].isIntersecting) { (function () { var d = document, s = d.createElement('script'); s.src = 'https://.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); disqus_observer.disconnect(); } }, { threshold: [0] }); disqus_observer.observe(document.querySelector('#disqus_thread')); /* Auto switch theme */ function reloadDisqus() { /* Disqus hasn't been loaded */ if (typeof DISQUS === "undefined") { return; } if (document.readyState == 'complete') { DISQUS.reset({ reload: true, config: disqus_config }); } } const modeToggle = document.querySelector(".mode-toggle"); if (modeToggle !== null) { modeToggle.addEventListener('click', reloadDisqus); window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', reloadDisqus); } </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://github.com/good-riverdeer">riverdeer</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/deep-learning/">deep_learning</a> <a class="post-tag" href="/tags/transformer/">transformer</a> <a class="post-tag" href="/tags/autonomous-driving/">autonomous_driving</a> <a class="post-tag" href="/tags/distillation/">distillation</a> <a class="post-tag" href="/tags/document-understanding/">document_understanding</a> <a class="post-tag" href="/tags/lidar/">lidar</a> <a class="post-tag" href="/tags/multimodal/">multimodal</a> <a class="post-tag" href="/tags/named-entity-recognition/">Named_Entity_Recognition</a> <a class="post-tag" href="/tags/ner/">NER</a> <a class="post-tag" href="/tags/ocr/">ocr</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
