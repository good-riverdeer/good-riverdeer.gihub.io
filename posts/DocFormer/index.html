<!DOCTYPE html><html lang="ko" mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="[paper-review] DocFormer: End-to-End Transformer for Document Understanding" /><meta property="og:locale" content="ko" /><meta name="description" content="Appalaraju, S., Jasani, B., Kota, B. U., Xie, Y., &amp; Manmatha, R. (2021). DocFormer: End-to-End Transformer for Document Understanding. arXiv preprint arXiv:2106.11539." /><meta property="og:description" content="Appalaraju, S., Jasani, B., Kota, B. U., Xie, Y., &amp; Manmatha, R. (2021). DocFormer: End-to-End Transformer for Document Understanding. arXiv preprint arXiv:2106.11539." /><link rel="canonical" href="https://good-riverdeer.github.io/posts/DocFormer/" /><meta property="og:url" content="https://good-riverdeer.github.io/posts/DocFormer/" /><meta property="og:site_name" content="good-riverdeer" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-12-23T13:20:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[paper-review] DocFormer: End-to-End Transformer for Document Understanding" /><meta name="twitter:site" content="@twitter_username" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"description":"Appalaraju, S., Jasani, B., Kota, B. U., Xie, Y., &amp; Manmatha, R. (2021). DocFormer: End-to-End Transformer for Document Understanding. arXiv preprint arXiv:2106.11539.","url":"https://good-riverdeer.github.io/posts/DocFormer/","@type":"BlogPosting","headline":"[paper-review] DocFormer: End-to-End Transformer for Document Understanding","dateModified":"2022-01-17T15:56:29+09:00","datePublished":"2021-12-23T13:20:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://good-riverdeer.github.io/posts/DocFormer/"},"@context":"https://schema.org"}</script><title>[paper-review] DocFormer: End-to-End Transformer for Document Understanding | good-riverdeer</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="good-riverdeer"><meta name="application-name" content="good-riverdeer"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/school.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">good-riverdeer</a></div><div class="site-subtitle font-italic">딥러닝을 공부하는</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/good-riverdeer" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['riverdeer.youn','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>[paper-review] DocFormer: End-to-End Transformer for Document Understanding</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>[paper-review] DocFormer: End-to-End Transformer for Document Understanding</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> riverdeer </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Thu, Dec 23, 2021, 1:20 PM +0900" >Dec 23, 2021<i class="unloaded">2021-12-23T13:20:00+09:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Mon, Jan 17, 2022, 3:56 PM +0900" >Jan 17, 2022<i class="unloaded">2022-01-17T15:56:29+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2899 words">16 min read</span></div></div><div class="post-content"><p><a href="https://arxiv.org/abs/2106.11539">Appalaraju, S., Jasani, B., Kota, B. U., Xie, Y., &amp; Manmatha, R. (2021). DocFormer: End-to-End Transformer for Document Understanding. <em>arXiv preprint arXiv:2106.11539.</em></a></p><p><em>개인적인 논문해석을 포함하고 있으며, 의역 및 오역이 남발할 수 있습니다. 올바르지 못한 내용에 대한 피드백을 환영합니다 :)</em></p><hr /><h2 id="1-introduction">1. Introduction</h2><ul><li>Visual Document Understanding (VDU)<ul><li>PDF 형태 혹은 이미지 형태인 디지털 문서에 대한 이해<ul><li>entity grouping, sequence labeling, document classification</ul><li>문서에서 OCR(Optical Character Recognition)을 수행하는 연구는 많지만 VDU를 위해선 <strong>구조와 레이아웃</strong>을 모두 반영해야 함<li>최근 Transformer 구조를 통해 <strong>text, spatial, image</strong>를 모두 반영하여 이를 해결하려는 연구가 다수 진행<ul><li>각 연구마다 <strong>text, spatial, image</strong>의 세 가지 modality를 결합하는 방식이 각자 다름<li>NLP(Natural Language Processing)에서 그랫듯이 unsupervised 방식으로 사전 학습(pre-training)하고 downstream task에 맞게 미세 조정(fine-tuning)하는 것이 일반적</ul></ul><li><em>Cross-modality feature correlation</em><ul><li>multi-modal 학습은 <strong>텍스트</strong>를 임의의 범위의 <strong>시각적 영역</strong>에 매핑하는 과정<ul><li>“사람”이라는 단어를 설명하는 텍스트 modality와 달리, 이에 해당하는 visual modality는 단순 픽셀 집합에 불과함</ul><li>때문에 modality간의 feature 상관관계를 모델링하는 것이 어려움</ul><li>DocFormer<ul><li>Architecture 특징<ul><li>multi-modal self-attention<li>shared spatial embeddings</ul><li>pre-training with 3 unsupervised multi-modal tasks<ul><li>multi-modal masked language modeling task (MM-MLM)<li>learn-to-reconstruct (LTR)<li>text describes image (TDI)</ul></ul></ul><p><strong>[@ Contributions]</strong></p><ul><li>문서 이미지에서 <strong>text, visual, spatial</strong> features를 결합할 수 있는 새로운 형태의 multi-modal attention layer 제안<li>Multi-modal feature collaboration을 위한 세 가지 unsupervised pre-training task 제안, 이 중 두 가지는 해당 분야에 새로운 방법: MM-MLM, LTR<li>end-to-end로 학습 가능하고 사전 학습된 object detection 모델을 사용하지 않음<li>VDU의 4가지 downstream task에 대해 DocFormer는 state-of-the-art 성능을 달성<li>문서 이미지에서 텍스트를 추출하기 위한 Custom OCR 모델을 사용하지 않음</ul><hr /><h2 id="2-background">2. Background</h2><p><strong>[@ Grid based methods using CNN]</strong></p><ul><li>구조적인 형태가 많은 문서(예: forms, tables, receipts, invoices)<li>invoice(송장) 문서에서 표의 형태를 통해 유형 분류 수행<ul><li>송장번호, 날짜, 공급업체 이름, 주소, …</ul></ul><p><strong>[@ BERT transformer-encoder based methods]</strong></p><ul><li><a href="https://dl.acm.org/doi/abs/10.1145/3394486.3403172">LayoutLM</a><ul><li>BERT 아키텍처를 문서 이미지에 맞게 수정하여 document understanding 수행<ul><li>2D spatial coordinate embeddings<li>1D position<li>text token embeddings</ul><li>visual features and its bounding box coordinates for each word token, obtained using a Faster-RCNN<li>11M 개의 unlabeled page를 통해 사전학습</ul><li><a href="https://dl.acm.org/doi/abs/10.1145/3394486.3403172">LayoutLMv2</a><ul><li>LayoutLM을 향상<ul><li>모델에 visual feature가 입력되는 방식을 개선<li>text token에 더해주는 대신 개별적이고 독립적인 token으로 처리<li>pre-training task를 추가</ul></ul><li><a href="https://openreview.net/forum?id=punMXQEsPr0">BROS</a><ul><li>2D spatial embeddings<li>graph-based classifier<ul><li>text token 사이의 엔티티 상관관계 예측에 사용</ul></ul></ul><p><strong>[@ Multi-modal transformer encoder-decoder based methods]</strong></p><ul><li><a href="https://arxiv.org/abs/2101.11272">Layout-T5</a><ul><li>a question answering task on a database of web article document images</ul><li><a href="https://arxiv.org/abs/2101.11272">TILT</a><ul><li>convolutional features + T5 architecture</ul></ul><hr /><h2 id="3-approach">3. Approach</h2><p><strong>[@ Conceptual Overview]</strong></p><ul><li>Joint Multi-Modal<ul><li>vision feature와 text feature가 하나의 긴 시퀀스로 결합<li><em>cross-modality feature correlation</em>의 측면에서 self-attention의 학습이 어려울 것임</ul><li>Two-Stream Multi-Modal<ul><li>각 modality를 담당하는 별도의 branch를 사용<li>이미지와 텍스트의 결합이 끝에 이르러서야 발생하기 때문에 이상적인 방법이 아님</ul><li>Single-Stream Multi-Modal<ul><li>visual feature도 text token과 동일한 형태로 만들어 서로 더해줌<li>vision과 language feature는 서로 다른 유형의 데이터, 단순하게 더하는 것은 부자연스러운 방법</ul><li>Discrete Multi-Modal (paper’s)<ul><li>visual, spatial feature를 text feature에서 분리하여 반복되는 각각의 transformer layer마다 residual하게 입력<li>text feature에 대한 visual, spatial feature의 영향이 각 transformer layer마다 달라질 것을 기대하였음</ul></ul><p><img data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/147203580-88836e60-adef-44ff-8031-4a73cc1d8aa4.png" alt="img-description" width="500" /></p><h3 id="31-model-architecture">3.1. Model Architecture</h3><p><strong>[@ features extract &amp; processing]</strong></p><ul><li><strong>Visual features</strong><ul><li>입력 이미지 $v\in \mathbb R^{3\times h\times w}$를 ResNet50 CNN으로 feature 추출: $f_{cnn}(\theta, v)$<ul><li>ResNet50의 4번째 블록 feature 사용 $v_{l_4} \in \mathbb R^{c\times h_l \times w_l}$<li>$v_{l_4} = f_{cnn}(\theta, v)$<li>$c=2048, h_l = {h\over 32}, w_l = {w\over 32}$</ul><li>$1\times 1$ convolution을 통해 채널 $c$ 축소<ul><li>transformer encoder의 입력 token의 수인 $d$로 축소<li>$(c, h_l, w_l) \to (d, h_l, w_l)$</ul><li>flatten &amp; linear transformation<ul><li>$(d, h_l, w_l) \to (d, h_l \times w_l) \to (d, N)$<li>$d=768, N=512$</ul></ul></ul>\[\bar V = linear(conv_{1\times 1}(f_{cnn}(\theta, v)))\]<ul><li><strong>Language features</strong><ul><li>OCR로 추출한 텍스트 $t$를 tokenizing<ul><li>word-piece tokenizer 사용<li>$t_{tok} = {[CLS], t_{tok_1}, t_{tok_2}, …, t_{tok_n}}, (n=511)$<li>한 문서에서 나타나는 토큰 수가<ul><li>511보다 큰 경우 나머지는 무시<li>511보다 작은 경우 나머지 공간은 $[PAD]$ 토큰으로 채움, $[PAD]$ 토큰은 self-attention 계산 과정에서 무시</ul></ul><li>trainable embedding layer $W_t$로 사영<ul><li>해당 레이어의 가중치는 LayoutLMv1의 사전학습 가중치를 사용</ul></ul></ul>\[\bar T = W_t(t_{tok})\]<ul><li><strong>Spatial features</strong><ul><li>각 단어 $k$개 대하여 bounding box 좌표값 $b_k=(x_1, y_1, x_2, y_2, x_3, y_3, x_4, y_4)$<li>위 $b_k$만 사용하는 것에 그치지 않고 추가적인 정보를 더 인코딩<ul><li>bounding box의 높이 $h$, 너비 $w$<li>네 모서리와 중점에 대해 word box간의 상대적 거리<ul><li> \[A_{rel}=\left\{A^{k+1}_{num}-A^k_{num}\right\}\]<li>$A\in (x, y); num\in(1, 2, 3, 4, c)$<li>$c$는 중점<li>$k$ index는 top-left $\to$ bottom-right 방향으로 증가<li>즉, $k+1$번째 word box는 $k$번째 word box의 우하향 방향에 위치</ul><li>$P^{abs}$: 1D positional encoding</ul><li>$x$와 $y$, visual feature $\bar V$와 language feature $\bar T$에 대해 각각 따로 임베딩 행렬을 두고 따로 학습하여 계산<ul><li>spatial 영향은 modality 별로 각각 발생할 것이라고 생각</ul></ul></ul>\[\bar V_s = W^x_v(x_1, x_3, w, A^x_{rel}) + W^y_v(y_1, y_3, h, A^y_{rel}) +P^{abs}_v,\] \[\bar T_s = W^x_t(x_1, x_3, w, A^x_{rel}) + W^y_t(y_1, y_3, h, A^y_{rel}) +P^{abs}_t\]<p><strong>[@ Multi-Modal Self-Attention Layer]</strong></p><ul><li>a transformer encoder $f_{enc}$ outputs a multi-modal feature representations $\bar M$ of the same shape as each of the input features ($d=768, N=512$)<ul><li>$\eta$는 transformer의 파라미터</ul></ul>\[\bar M = f_{enc}(\eta, \bar V, \bar V_s, \bar T, \bar T_s)\]<ul><li>$i$번째 입력 토큰에 대한 $l$번째 transformer layer의 multi-modal feature</ul>\[\bar M^l_i = \sum_{j=1}^L {\exp(\alpha_{ij})\over \sum_{j^\prime=1}^n \exp(\alpha_{ij^\prime})}(x^l_j W^{V, l})\]<p><img data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/147220758-50672405-9b41-4c3a-bbc7-cc0826c8e166.png" alt="img_description" /></p>\[\alpha_{ij} = {1\over \sqrt d} (x^l_i W^{Q, l})(x^l_j W^{K, l})^T\]<p>generalization을 위한 $\sqrt d$와 $l$번째 레이어의 것이라는 표기를 제거하여 간단하게 나타내면,</p>\[\alpha_{ij} = (x_i W^{Q})(x_j W^{K})^T\]<p>여기서 visual feature와 text feature에 대해 각각 다른 연산 흐름을 갖는다.</p><ul><li><strong>attention distribution for visual feature</strong><ul><li>$x^v$는 각 토큰의 visual feature<li><code class="language-plaintext highlighter-rouge">query 1D relative attn.</code>, <code class="language-plaintext highlighter-rouge">key 1D relative attn.</code>, <code class="language-plaintext highlighter-rouge">visual spatial attn.</code>을 모두 더해주어 <em>local feature</em>를 포착하는데 힘을 쏟음</ul></ul><p><img data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/147210661-7d7267bd-e9f6-4d83-afcf-69eb690d879e.png" alt="img-description" /></p><p><img data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/147221523-512b5709-f8fe-4420-9146-ff4143252130.png" alt="img-description" /></p><ul><li><strong>attention distribution for text feature</strong><ul><li>위 visual feature에서의 연산과 거의 비슷함</ul></ul>\[\alpha_{ij}^t = (x_i W^Q_t)(x_j W^K_t) + (x_i W^Q_t\alpha_{ij}) + (x_j W^K_t\alpha_{ij}) + (\bar T_s W_s^Q)(\bar T_s W_s^K)\]<p><img data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/147222945-22b1a277-872d-4981-aeb2-a0db5bc491a5.png" alt="img-description" /></p><ul><li>단, 입력 토큰 $x_i$는 이전 transformer layer의 output인 multi-modal feautre (만약 첫 번째 레이어라면 word embedding일 것)<ul><li>모든 transformer layer에 visual feature는 모두 동일하게 입력이 주어지지만, text feature는 transformer layers stack을 따라 흐르기 때문이다. (아래 그림 참조)</ul></ul><p><img data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/147222699-06b190d5-b603-4e06-873f-805fcc67db9a.png" alt="img-description" width="500" /> <em>visual feature는 각 transformer layer에 따로따로 투입, text feature(word embedding)은 transformer layers stack을 타고 흐름</em></p><ul><li>multi-modal feature output<ul><li>$l$번째 transformer layer의 output은 $\bar M_l = \hat V_l + \hat T_l$</ul></ul><blockquote><p>본 논문에서 강조하는 점 중 하나는 Spatial embeddings를 위한 attention 가중치를 공유하고 있다는 것인데, 이는 위 attention 연산 흐름을 나타내는 그림에 spatial Query, Key matrix가 visual과 text 모두 분홍색으로 같은 색으로 칠하는 것으로 강조하고 있고, 아래 실험(4. Experiments)을 통해 이에 대한 효과를 입증하고 있다.</p></blockquote><h3 id="32-pre-training">3.2. Pre-training</h3><p><img data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/147306046-e08f951d-09d5-4a45-8a4a-f26da1cff1e9.png" alt="img-description" /></p><h4 id="multi-modal-masked-language-modeling-mm-mlm"><strong>Multi-Modal Masked Language Modeling (MM-MLM)</strong></h4><ul><li>기존 BERT에서 소개되었던 기존 Masked Language Modeling (MLM) task를 개선<ul><li>텍스트 시퀀스 $t$에서 일부를 마스킹한 corrupted 시퀀스 $\tilde t$<li>multi-modal feature embedding인 $\bar M$을 바탕으로 원래 시퀀스 $t$로 복구하는 task<li>LayoutLMv2와 같은 연구에서는 마스킹한 텍스트에 해당하는 이미지 영역도 같이 마스킹하였으나, 본 연구에서는 이미지 영역에 대한 마스킹을 하지 않음<li>마스킹 비율은 BERT의 것과 동일</ul><li>Cross-entropy loss로 학습</ul><h4 id="learn-to-reconstruct-ltr"><strong>Learn To Reconstruct (LTR)</strong></h4><ul><li>MM-MLM task의 이미지 버전 (image reconstruction)<li>multi-modal feature embedding $\bar M$을 shallow decoder로 투입, 원래 입력 이미지로 복구<ul><li>auto-encoder를 통한 image reconstruction과 비슷함</ul><li>smooth-L1 loss로 학습</ul><h4 id="text-describes-image-tdi"><strong>Text Describes Image (TDI)</strong></h4><ul><li>위 두 사전학습 task(MM-MLM, LTR)가 <em>local features</em>에 집중하였던 것과 다르게 <em>global features</em>에 집중한 task<li>multi-modal feature embedding $\bar M$을 입력으로 단일 linear layer를 통과해 binary classification 수행<ul><li>한 배치 내에서 80%는 올바른 text-image pair, 20%는 잘못된 text-image pair를 구성하도록 하였음<li>단, 잘못된 text-image pair가 구성될 경우 LTR task의 loss는 무시</ul><li>binary cross-entropy로 학습</ul><h4 id="final-pre-training-loss"><strong>final pre-training loss</strong></h4>\[L_pt = \lambda L_{MM-MLM} + \beta L_{LTR} + \gamma L_{TDI}\] \[\lambda=5, \beta=1, \gamma=5\]<hr /><h2 id="4-experiments">4. Experiments</h2><ul><li>개요<ul><li>모든 실험에 걸쳐 training set을 통해 fine-tuning<li>test나 validation set에 대한 결과<li>데이터셋 별로 별도의 특화된 hyper-parameter tuning은 진행하지 않았음</ul><li>Models<ul><li>기존 transformer encoder model에 대한 terminology를 따름<ul><li>12개의 transformer layer가 있는 경우 <em>-base</em> 모델 (768 hidden state and 12 attention heads)<li>24개의 transformer layer가 있는 경우 <em>-large</em> 모델 (1024 hidden state and 16 attention heads)</ul><li>text and spatial features만 사용하는 모델에 대해서도 실험<ul><li>DocFormer의 유연성 강조<li>visual features가 포함됨으로써 더 나아지는 모습 강조</ul></ul></ul><h3 id="41-sequence-labeling-task">4.1. Sequence Labeling task</h3><ul><li>FUNSD dataset<ul><li>form understanding을 위한 데이터셋<ul><li>문서 내 각 요소들의 entity 유형 및 클래스가 레이블링 되어 있음 (아래 그림 참조)</ul><li>149 train / 50 test pages</ul><li>Sequence labeling task<ul><li>각 구성 요소의 클래스를 예측하는 task</ul></ul><p><img data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/147306304-f1070b23-0e50-44e4-9c44-c13b3122c71a.png" alt="img-description" width="500" /> <em>LayoutLMv2가 11M의 데이터로 사전학습한 것에 반해 DocFormer는 5M의 데이터로 학습한 결과임을 강조하고 있음</em></p><p><img data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/147306354-c5d65726-900d-4137-b605-48154e357764.png" alt="img-description" width="500" /> <em>Qualitative result</em></p><h3 id="42-document-classification-task">4.2. Document Classification task</h3><ul><li>RVL-CDIP dataset<ul><li>320,000 train / 40,000 validation / 40,000 test<li>grayscale images<li>16 classes<li>text 및 layout 정보는 Tesseract OCR로 도출</ul></ul><p><img data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/147306785-876ead33-1a5a-4ae3-be15-365f00eb0d7e.png" alt="img-description" width="500" /></p><h3 id="43-entity-extraction-task">4.3. Entity Extraction Task</h3><ul><li>OCR로 추출한 정보들(text, spatial)과 문서 이미지(image)를 조합<li>각 entity의 정보를 예측하는 task<li>CORD dataset<ul><li>영수증 이미지 데이터셋<li>30 fields under 4 categories</ul><li>Kleister-NDA dataset<ul><li>legal NDA documents<li>4개의 fixed labels를 추출하는 데이터셋</ul></ul><p><img data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/147307764-38bf46b9-49d3-4908-9d28-042c4bb7145d.png" alt="img-description" /></p><p><img data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/147307430-8429d217-2852-4fd3-9165-c9d90065247a.png" alt="img-description" width="500" /></p><h3 id="44-more-experiments">4.4. More Experiments</h3><p><strong><em>Shared or Independent Spatial embeddings?</em></strong></p><ul><li>vision and language modalities 간의 sharing spatial embedding의 효과<li>sharing spatial embedding을 통해 <em>feature correlation</em>을 학습할 수 있다고 주장</ul><p><img data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/147308252-8bdb063b-d6d9-4bb2-b41c-15c6b48eb016.png" alt="img-description" width="500" /></p><p><strong><em>Do our pre-training tasks help?</em></strong></p><ul><li>비교적 학습 데이터 수가 적은 데이터셋에서는 사전 학습이 필요한 것으로 보임</ul><p><img data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/147308342-bea71902-bef9-4a28-a121-13934965909d.png" alt="img-description" width="500" /></p><p><strong><em>Does a deeper projection head help?</em></strong></p><ul><li>위 실험 결과들은 모두 downstream task에 단일 linear layer만으로 예측한 결과임<li>($fc\to ReLU \to LayerNorm \to fc$) 형태의 더 깊은 projection head를 사용하는 것의 효과에 대한 실험<li>학습 데이터가 비교적 많은 데이터셋에 대해 효과를 보였음</ul><p><img data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/147308539-689df192-d3d0-4009-8648-963850a14a4a.png" alt="img-description" width="500" /></p><h3 id="45-ablation-study">4.5. Ablation Study</h3><p><img data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/147308592-1072704d-ea88-4c5a-8fa9-e943dbf41c6f.png" alt="img-description" width="500" /> <em>pre-training task의 유무에 따른 결과 비교</em></p><p><img data-proofer-ignore data-src="https://user-images.githubusercontent.com/67779236/147308625-3a97915b-5cde-4c69-9896-3f16481857d5.png" alt="img-description" width="500" /> <em>DocFormer의 구성요소 유무에 따른 결과 비교</em></p><hr /><h2 id="5-conclusion">5. Conclusion</h2><ul><li>DocFormer 제안, 다양한 Visual Document Understanding tasks에 대한 end-to-end trainable transformer based model<li>multi-modal attention 방법론 제안<li>두 가지 새로운 vision-plus-language 사전학습 task 제안<li>실험을 통해 DocFormer가 비교적 규모가 작은 모델임에도 4가지 데이터셋에 대해 SOTA 성능을 달성<li>Future works<ul><li>multi-lingual 환경에 대한 DocFormer 연구<li>info-graphics, maps, web-pages와 같은 다양한 형태의 문서에 대한 연구</ul></ul><hr /><blockquote><p>Microsoft의 LayoutLM를 잇는 Amazon의 document를 위한 transformer 모델이다. LayoutLM과 비교했을 때 사용하는 문서의 요소들이나 방법들이 크게 다르지는 않는 것 같다. 다만 그 형태를 조금씩 바꾸었음에도 11M $\to$ 5M으로 사전학습에 사용한 데이터셋 수를 크게 줄인 것은 흥미로운 부분이다.</p></blockquote></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/paper-review/'>paper-review</a>, <a href='/categories/computer-vision/'>Computer Vision</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/deep-learning/" class="post-tag no-text-decoration" >deep_learning</a> <a href="/tags/document-understanding/" class="post-tag no-text-decoration" >document_understanding</a> <a href="/tags/multimodal/" class="post-tag no-text-decoration" >multimodal</a> <a href="/tags/transformer/" class="post-tag no-text-decoration" >transformer</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=[paper-review] DocFormer: End-to-End Transformer for Document Understanding - good-riverdeer&url=https://good-riverdeer.github.io/posts/DocFormer/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=[paper-review] DocFormer: End-to-End Transformer for Document Understanding - good-riverdeer&u=https://good-riverdeer.github.io/posts/DocFormer/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=[paper-review] DocFormer: End-to-End Transformer for Document Understanding - good-riverdeer&url=https://good-riverdeer.github.io/posts/DocFormer/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/DDPM/">[paper-review] Denoising diffusion probabilistic models</a><li><a href="/posts/SLidR/">[paper-review] Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data</a><li><a href="/posts/TEBNER/">[paper-review] TEBNER: Domain Specific Named Entity Recognition with Type Expanded Boundary-aware Network.</a><li><a href="/posts/EfficientNet/">[paper-review] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a><li><a href="/posts/Vision_Transformer/">[paper-review] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/deep-learning/">deep_learning</a> <a class="post-tag" href="/tags/transformer/">transformer</a> <a class="post-tag" href="/tags/autonomous-driving/">autonomous_driving</a> <a class="post-tag" href="/tags/distillation/">distillation</a> <a class="post-tag" href="/tags/document-understanding/">document_understanding</a> <a class="post-tag" href="/tags/lidar/">lidar</a> <a class="post-tag" href="/tags/multimodal/">multimodal</a> <a class="post-tag" href="/tags/named-entity-recognition/">Named_Entity_Recognition</a> <a class="post-tag" href="/tags/ner/">NER</a> <a class="post-tag" href="/tags/ocr/">ocr</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Donut/"><div class="card-body"> <span class="timeago small" >Dec 27, 2021<i class="unloaded">2021-12-27T12:20:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[paper-review] Donut: Document Understanding Transformer without OCR</h3><div class="text-muted small"><p> Kim, G., Hong, T., Yim, M., Park, J., Yim, J., Hwang, W., … &amp; Park, S. (2021). Donut: Document Understanding Transformer without OCR. arXiv preprint arXiv:2111.15664. 개인적인 논문해석을 포함하고 있으며, 의역 및...</p></div></div></a></div><div class="card"> <a href="/posts/Vision_Transformer/"><div class="card-body"> <span class="timeago small" >Aug 10, 2021<i class="unloaded">2021-08-10T12:20:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[paper-review] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</h3><div class="text-muted small"><p> Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., … &amp; Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv...</p></div></div></a></div><div class="card"> <a href="/posts/Swin_Transformer/"><div class="card-body"> <span class="timeago small" >Aug 27, 2021<i class="unloaded">2021-08-27T12:20:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[paper-review] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</h3><div class="text-muted small"><p> Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., … &amp; Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030. 개인적인 논문해석을 ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Learning_to_Understand_Traffic_Signs/" class="btn btn-outline-primary" prompt="Older"><p>[paper-review] Learning to Understand Traffic Signs</p></a> <a href="/posts/Donut/" class="btn btn-outline-primary" prompt="Newer"><p>[paper-review] Donut: Document Understanding Transformer without OCR</p></a></div><div id="disqus_thread" class="pt-2 pb-2"><p class="text-center text-muted small"> Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script type="text/javascript"> var disqus_config = function () { this.page.url = 'https://good-riverdeer.github.io/posts/DocFormer/'; this.page.identifier = '/posts/DocFormer/'; }; /* Lazy loading */ var disqus_observer = new IntersectionObserver(function (entries) { if(entries[0].isIntersecting) { (function () { var d = document, s = d.createElement('script'); s.src = 'https://.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); disqus_observer.disconnect(); } }, { threshold: [0] }); disqus_observer.observe(document.querySelector('#disqus_thread')); /* Auto switch theme */ function reloadDisqus() { /* Disqus hasn't been loaded */ if (typeof DISQUS === "undefined") { return; } if (document.readyState == 'complete') { DISQUS.reset({ reload: true, config: disqus_config }); } } const modeToggle = document.querySelector(".mode-toggle"); if (modeToggle !== null) { modeToggle.addEventListener('click', reloadDisqus); window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', reloadDisqus); } </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://github.com/good-riverdeer">riverdeer</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/deep-learning/">deep_learning</a> <a class="post-tag" href="/tags/transformer/">transformer</a> <a class="post-tag" href="/tags/autonomous-driving/">autonomous_driving</a> <a class="post-tag" href="/tags/distillation/">distillation</a> <a class="post-tag" href="/tags/document-understanding/">document_understanding</a> <a class="post-tag" href="/tags/lidar/">lidar</a> <a class="post-tag" href="/tags/multimodal/">multimodal</a> <a class="post-tag" href="/tags/named-entity-recognition/">Named_Entity_Recognition</a> <a class="post-tag" href="/tags/ner/">NER</a> <a class="post-tag" href="/tags/ocr/">ocr</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
