<!DOCTYPE html><html lang="ko" mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="[paper-review] Mask TextSpotter v3: Segmentation Proposal Network for Robust Scene Text Spotting" /><meta property="og:locale" content="ko" /><meta name="description" content="Liao, M., Pang, G., Huang, J., Hassner, T., &amp; Bai, X. (2020). Mask textspotter v3: Segmentation proposal network for robust scene text spotting. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XI 16 (pp. 706-722). Springer International Publishing." /><meta property="og:description" content="Liao, M., Pang, G., Huang, J., Hassner, T., &amp; Bai, X. (2020). Mask textspotter v3: Segmentation proposal network for robust scene text spotting. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XI 16 (pp. 706-722). Springer International Publishing." /><link rel="canonical" href="https://good-riverdeer.github.io/posts/Mask_TextSpotter/" /><meta property="og:url" content="https://good-riverdeer.github.io/posts/Mask_TextSpotter/" /><meta property="og:site_name" content="good-riverdeer" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-11-22T18:20:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[paper-review] Mask TextSpotter v3: Segmentation Proposal Network for Robust Scene Text Spotting" /><meta name="twitter:site" content="@twitter_username" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"description":"Liao, M., Pang, G., Huang, J., Hassner, T., &amp; Bai, X. (2020). Mask textspotter v3: Segmentation proposal network for robust scene text spotting. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XI 16 (pp. 706-722). Springer International Publishing.","url":"https://good-riverdeer.github.io/posts/Mask_TextSpotter/","@type":"BlogPosting","headline":"[paper-review] Mask TextSpotter v3: Segmentation Proposal Network for Robust Scene Text Spotting","dateModified":"2022-01-17T15:56:29+09:00","datePublished":"2021-11-22T18:20:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://good-riverdeer.github.io/posts/Mask_TextSpotter/"},"@context":"https://schema.org"}</script><title>[paper-review] Mask TextSpotter v3: Segmentation Proposal Network for Robust Scene Text Spotting | good-riverdeer</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="good-riverdeer"><meta name="application-name" content="good-riverdeer"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/school.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">good-riverdeer</a></div><div class="site-subtitle font-italic">딥러닝을 공부하는</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/good-riverdeer" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['riverdeer.youn','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>[paper-review] Mask TextSpotter v3: Segmentation Proposal Network for Robust Scene Text Spotting</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>[paper-review] Mask TextSpotter v3: Segmentation Proposal Network for Robust Scene Text Spotting</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> riverdeer </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Mon, Nov 22, 2021, 6:20 PM +0900" >Nov 22, 2021<i class="unloaded">2021-11-22T18:20:00+09:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Mon, Jan 17, 2022, 3:56 PM +0900" >Jan 17, 2022<i class="unloaded">2022-01-17T15:56:29+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2646 words">14 min read</span></div></div><div class="post-content"><p><a href="https://link.springer.com/chapter/10.1007%2F978-3-030-58621-8_41">Liao, M., Pang, G., Huang, J., Hassner, T., &amp; Bai, X. (2020). Mask textspotter v3: Segmentation proposal network for robust scene text spotting. In <em>Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XI 16 (pp. 706-722).</em> Springer International Publishing.</a></p><p><em>개인적인 논문해석을 포함하고 있으며, 의역 및 오역이 남발할 수 있습니다. 올바르지 못한 내용에 대한 피드백을 환영합니다 :)</em></p><hr /><h2 id="1-introduction">1. Introduction</h2><p><strong>[@ Scene text spotter]</strong></p><ul><li>최근 scene text spotting task에는 end-to-end 학습 방식의 딥러닝이 많이 적용되고 있음<li>좋은 scene text spotting 아래 세 가지 능력을 갖추어야 함<ul><li><strong><em>Rotation robustness</em></strong>: 텍스트가 이미지 축에 잘 정렬되어 있지 않았을 때에 강건함<li><strong><em>Aspect ratio robustness</em></strong>: non-Latin scripts에는 주로 word 단위보다는 긴 텍스트 라인으로 텍스트 인스턴스가 구성되어 있는데, 이처럼 다양한 텍스트 인스턴스 종횡비에 강건함<li><strong><em>Shape robustness</em></strong>: Logo같은 텍스트에 주로 나타나는 일반적이지 않은 모양의 텍스트에 강건함</ul></ul><p><strong>[@ Mask TextSpotter series]</strong></p><ul><li>Region Proposal Network(RPN)의 한계<ol><li>manually pre-designed anchors를 사용하므로 극단적인 종횡비를 가지는 텍스트 인스턴스를 포착하기 쉽지 않음<li>RPN이 생성하는 axis-aligned rectangular proposals는 그 box안에 인접한 다른 텍스트 인스턴스들이 함께 포함되는 경우가 많음</ol></ul><p><img data-proofer-ignore data-src="https://images.velog.io/images/riverdeer/post/b150f82b-a6ef-450c-85f2-16172a94593c/image.png" style="margin:50px 0 10px 0" /> <em>anchor-box, <a href="https://herbwood.tistory.com/10">출처</a></em></p><p><img data-proofer-ignore data-src="https://images.velog.io/images/riverdeer/post/548a99bb-cb6e-4cfe-966e-3757c9cd9e23/image.png" style="margin:50px 0 10px 0" /> <em>2번 한계의 예시</em> <br /></p><ul><li>선행연구, <strong>Mask TextSpotter v1</strong>, <strong>Mask TextSpotter v2</strong>에서는 Region Proposal Network (RPN)을 통해 RoI feature를 추출하고 이 RPN이 제안한 proposal box들에 detection과 recognition을 수행<ul><li><strong><em>rotation robustness</em></strong>, <strong><em>shape robustness</em></strong>를 갖출 수 있는 RPN을 제안<li>But, <strong><em>aspect ratio robustness</em></strong>까지 모두 갖추지는 못했음</ul></ul><p><strong>[@ Segmentation Proposal Network (SPN)]</strong></p><ul><li>Segmentation Proposal Network을 통해 정확한 polygonal 형태의 proposal 표현을 할 수 있음<li>더 나아가 정확한 형태의 proposal 표현을 통해 <strong>hard RoI masking</strong> 방법을 적용, 인접한 텍스트 인스턴스나 배경 노이즈의 간섭을 억제할 수 있음</ul><p><strong>[@ Contributions]</strong></p><ul><li><strong>Segmentation Proposal Network(SPN)</strong><ul><li>극단적인 종횡비나 특이한 형태를 가진 텍스트 인스턴스를 정확하게 포착할 수 있는 SPN를 제안</ul><li><strong>hard RoI masking</strong><ul><li>SPN이 생성해낸 proposal에 적용하여 배경 픽셀이나 인접한 다른 텍스트 인스턴스가 유발할 수 있는 노이즈를 제거</ul><li><strong>Mask TextSpotter v3</strong><ul><li>rotation, aspect ratio, shape에 모두 robust한 text spotter 모델<li>다양한 벤치마크에 높은 성능을 보임</ul></ul><hr /><h2 id="2-related-work">2. Related Work</h2><p><strong>[@ Two-stage scene text spotting]</strong></p><ul><li><a href="https://ieeexplore.ieee.org/abstract/document/6460871?casa_token=mxjYSGOuaL8AAAAA:9-FiAX0KMCXYeu8DNSUVzYCINCdh3cz3OjLE3zaL_b7kx4F4W50KaWmCLMcbNwHwyLCzVqSBFVI">Wang et al.</a> tried to detect and classify characters with CNNs.<li><a href="https://link.springer.com/article/10.1007%2Fs11263-015-0823-z">Jaderberg et al.</a> proposed a scene text spotting method<ul><li>proposal generation module<li><strong>a random forest classifier</strong> to filter proposals<li>a CNN-based regression module for refining the proposals<li>a CNN-based word classifier for recognition</ul><li><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewPaper/14202">TextBoxes</a> and <a href="https://ieeexplore.ieee.org/abstract/document/8334248?casa_token=JkAfo7MVaU8AAAAA:SkR9jLyHXZIocPQ9ghfYenqQMpCdaO5xf9whHHm7992FIlYXE6ZX3dT-LuKEehbeYcSbLQOuxZs">TextBoxes++</a> <strong>combined</strong> thier proposed scene text detector <strong>with CRNN</strong><li><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Zhan_GA-DAN_Geometry-Aware_Domain_Adaptation_Network_for_Scene_Text_Detection_and_ICCV_2019_paper.html">Zhan et al.</a> proposed to apply <strong>multi-modal spatial learning</strong> into the scene text detection and recognition system.</ul><p><strong>[@ End-to-end trainable scene text spotting]</strong></p><ul><li><a href="https://openaccess.thecvf.com/content_ECCV_2018/html/Pengyuan_Lyu_Mask_TextSpotter_An_ECCV_2018_paper.html">Mask TextSpotter v1</a> is <strong>the first end-to-end</strong> trainable arbitrary-shape scene text spotter<ul><li>consisting of a detection module based on <strong>Mask R-CNN</strong> and character segmentation module for recognition.</ul><li><a href="https://openaccess.thecvf.com/content_ECCV_2018/html/Pengyuan_Lyu_Mask_TextSpotter_An_ECCV_2018_paper.html">Mask TextSpotter v2</a> extends <a href="https://openaccess.thecvf.com/content_ECCV_2018/html/Pengyuan_Lyu_Mask_TextSpotter_An_ECCV_2018_paper.html">Mask TextSpotter v1</a> by applying a <strong>spatial attention module</strong> for recognition<ul><li>spatial attention module: character 수준의 공간적 왜곡을 바로 잡아줄 수 있는 모듈</ul><li><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Qin_Towards_Unconstrained_End-to-End_Text_Spotting_ICCV_2019_paper.html">Qin et al.</a> also combine a <strong>Mask R-CNN detector</strong> and <strong>an attention-based recognizer</strong> to deal with arbitrary-shape text instances<ul><li><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Qin_Towards_Unconstrained_End-to-End_Text_Spotting_ICCV_2019_paper.html">Qin et al.</a>의 연구에선 mask map을 recognition에 성능향상을 위해 RoI feature에 대해 RoI masking을 수행<li>하지만, mask map을 생성하는 데 RPN을 사용하기 때문에 proposals을 생성하는 데 부정확한 결과를 만들어낼 수 있음 (Introduction에서 밝힌 RPN의 단점)</ul><li><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Xing_Convolutional_Character_Networks_ICCV_2019_paper.html">Xing et al.</a> propose to <strong>simultaneously detect/recognize</strong> the characters and the text instances, using the text instance detection results to group the characters.<li><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Feng_TextDragon_An_End-to-End_Framework_for_Arbitrary_Shaped_Text_Spotting_ICCV_2019_paper.html">TextDragon</a> detects and recognizes text instances <strong>by grouping and decoding</strong> a series of local regions along with <strong>their centerline</strong></ul><p><strong>[@ Segmentation-based scene text detectors]</strong></p><ul><li><a href="https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Multi-Oriented_Text_Detection_CVPR_2016_paper.html">Zhang et al.</a> <strong>first use FCN</strong> to obtain the salient map of the text region<ul><li>then estimate the text line hypotheses by combining the salient map and character components.<li>Finally, another FCN predicts the centroid of each character to remove the false hypotheses.</ul><li><a href="https://arxiv.org/abs/1603.09423">He et al.</a> propose Cascaded Convolutional Text Networks (CCTN) for text center lines and text regions.<li><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Shape_Robust_Text_Detection_With_Progressive_Scale_Expansion_Network_CVPR_2019_paper.html">PSENet</a> adopts a progressive scale expansion algorithm to get the bounding boxes from multi-scale segmentation maps.<li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/6812">DB</a> proposes a differentiable binarization module for a segmentation network.<li>본 논문에서는 기존 Segmentation-based scene text detector에 비해 다양한 단서와 추가적인 모듈을 결합하여 detection task를 수행함<ul><li>proposal generation에 segmentation network을 사용한다는 점을 강조할 수 있음</ul></ul><hr /><h2 id="3-methodology">3. Methodology</h2><p><img data-proofer-ignore data-src="https://images.velog.io/images/riverdeer/post/305c9289-3803-46cc-904a-6b7ea8d7f8d0/image.png" alt="" /></p><p>Mask TextSpotter v3 consists of …</p><ul><li>a ResNet-50 backbone, a Segmentation Proposal Network (SPN) for proposal generation<li>a Fast R-CNN module for refining proposals<li>a text instance segmentation module for accurate detection<li>a character segmentation module and a spatial attentional module for recognition</ul><p>추가적으로 Mask TextSpotter v3는 RoI feature의 형태를 다각형, polygonal 형태로 생성하기 때문에 정확한 detection을 할 수 있고 recognition의 성능에도 좋은 영향을 줄 수 있음</p><h3 id="31-segmentation-proposal-network">3.1. Segmentation proposal network</h3><ul><li>U-Net의 형태를 사용, 다양한 크기의 다양한 feature를 사용<li>SPN의 output $F$는 위 feature들을 결합하여 ${H\over 4} \times{W\over 4}$ 크기로 생성됨<ul><li>$H, W$는 각각 입력 이미지의 높이, 너비</ul><li>$F$를 통해 Segmentation을 수행하여 최종적으로 $1\times H \times W$ 크기의 predict segmentation map $S$를 생성함</ul><p align="center"> <img data-proofer-ignore data-src="https://images.velog.io/images/riverdeer/post/ddac7eff-8b8b-4cdb-884e-810d896de4ac/image.png" style="margin:50px 0 10px 0" /> <i>최종 Segmentation 수행 모듈의 구조</i></p><p><strong>[@ Segmentation label generation]</strong> Segmentation 성능 향상을 위해 text instance들의 크기를 축소시킴으로써 인접한 text instance들을 분리하려는 테크닉이 일반적임</p><ul><li><em>Vatti clipping algorithm</em><ul><li>$d$ pixel 만큼 텍스트 영역을 축소시키는 테크닉<li>the offset pixel $d=A(1-r^2)/L$<ul><li>$A$는 텍스트 인스턴스 polygon의 면적<li>$L$은 텍스트 인스턴스 polygon의 둘레<li>$r$ is the shrink ratio</ul></ul></ul><p><img data-proofer-ignore data-src="https://images.velog.io/images/riverdeer/post/2ab00cc8-ecc7-4725-a030-24efe777bf07/image.png" alt="" /></p><p><strong>[@ Proposal generation]</strong></p><ul><li>먼저 Segmentation map $S$를 이진화하여 binary map $B$를 계산</ul><p>\(B_{i,j} = \begin{cases} 1 &amp; \mathrm{if} \space S_{i,j} \ge t,\\ 0 &amp; \mathrm{otherwise.} \end{cases} \\ \mathrm{Here,} \space t=0.5\)</p><ul><li>이후 _Vatti clipping algorithm_을 원복</ul>\[\begin{matrix} \hat d = \hat A \times \hat r / \hat L\\ \mathrm {Here,} \space \hat r = 3.0 \end{matrix}\]<h3 id="32-hard-roi-masking">3.2. Hard RoI masking</h3><ul><li>직사각형의 binary map $B$에서 각 text instance RoI feature와 크기가 동일한 polygon mask $M$을 생성 \(M=\begin{cases} 1, &amp; \mathrm{if \space in \space the \space polygon \space region}\\ 0, &amp; \mathrm{else} \end{cases}\)<li>RoI feature $R=R_0 * M$, $*$는 element-wise multiplication<li>hard RoI masking을 통해 배경 영역이나 인접한 다른 텍스트 인스턴스들의 방해를 억제할 수 있음<li>결과적으로 detection과 recognition 모두에 성능 향상을 도모할 수 있음</ul><h3 id="33-detection-and-recognition">3.3. Detection and recognition</h3><ul><li>text detection and recognition의 설계는 Mask TextSpotter v2의 것과 동일<ul><li>Mask TextSpotter v2가 당시 최고의 detection, recognition 모델임<li>RPN-based scene text spotter와 (본 논문에서 제안하는) SPN-based scene text spotter의 공정한 비교를 위함</ul><li>hard RoI masking을 거친 masked RoI features는 Fast R-CNN의 입력으로 주어지고 localization을 가다듬고 character segmentation module과 spatial attentional module로 recognition을 수행</ul><h3 id="34-optimization">3.4. Optimization</h3><p>\(L = L_s + \alpha_1L_{rcnn}+\alpha_2L_{mask}\)</p><ul><li>$L_{rcnn}$ is defined in Fast R-CNN<li>$L_{mask}$ is defined in Mask TestSpotter v2, consisting of a <strong>text instance segmentation loss</strong>, <strong>a chracter segmentation loss</strong>, and <strong>a spatial attentional decoder loss</strong>.<li>$L_s$ indicates the SPN loss<ul><li>SPN loss엔 Dice loss를 사용 \(I=\sum(S*G); \space U=\sum S + \sum G; \space L_s=1-{2.0\times I\over U}\)<li>$S$ is the segmentation map, $G$ is the target map, $*$ represents element-wise multiplication.</ul><li>$\alpha_1=\alpha_2=1.0$</ul><hr /><h2 id="4-experiments">4. Experiments</h2><h3 id="41-datasets">4.1. Datasets</h3><ul><li><strong><em>SynthText</em></strong><ul><li>800K 텍스트 이미지를 포함한 합성 데이터셋<li>annotations for word/character bounding boxes and text sequences.</ul><li><strong><em>Rotated ICDAR 2013 dataset (RoIC13)</em></strong><ul><li><strong><em>ICDAR 2013</em></strong> 데이터셋에서 $15^\circ, 30^\circ, 45^\circ, 60^\circ, 75^\circ, 90^\circ$를 회전시켜 직접 제작<li><strong><em>ICDAR 2013</em></strong>의 텍스트 인스턴스들이 모두 수평적으로 정렬되어 있기 때문에 이 특성을 이용해 텍스트의 회전 방향에 대한 강건함(<em>rotation robustness</em>)을 테스트 할 수 있음</ul><li><strong><em>MSRA-TD500</em></strong><ul><li>영어와 중국어로 구성된 multi-language scene text detection benchmark<li>많은 수의 텍스트 인스턴스가 극단적인 종횡비로 구성됨<li>recognition annotations가 포함되지 않음</ul><li><strong><em>Total-Text</em></strong><ul><li>다양한 형태의 텍스트 인스턴스, 가로세로 방향의 인스턴스, 곡선 형태의 텍스트들이 포함<li>polygonal bounding box와 transcription annotations 포함</ul><li><strong><em>ICDAR 2015 (IC15)</em></strong><ul><li>quadrilateral bounding boxes로 레이블 구성<li>대부분의 이미지가 저해상도이고 작은 텍스트 인스턴스를 포함</ul></ul><h3 id="42-implementation-details">4.2. Implementation details</h3><p><strong>[@ Mask TextSpotter v2]</strong></p><ul><li>공정한 비교를 위해 같은 학습 데이터와 Data augmentation 과정을 거침<li>한 가지 차이점<ul><li>SPN이 더 극단적인 형태의 text instance에도 강건하기 때문에 rotation 각도 범위를 $[-30^\circ, 30^\circ]$에서 $[-90^\circ, 90^\circ]$로 확장</ul></ul><p><strong>[@ hyper-parameters &amp; training details]</strong></p><ul><li>optimizer: SGD with a weight decay of 0.001 and momentum of 0.9<li><strong><em>SynthText</em></strong>로 사전학습 수행 후 데이터셋을 조합하여 미세조정 수행<ul><li><strong><em>SynthText: ICDAR 2013: ICDAR 2015: SCUT: Total-Text $= 2:2:2:1:1$</em></strong><li>학습에 사용되는 batch를 위 비율로 구성, 즉 batch를 8로 구성<li>pre-training<ul><li>learning rate는 0.01로 시작<li>100K, 200K iteration에서 각각 $1/10$씩 감소</ul><li>fine-tuning<ul><li>학습 시와 동일한 환경을 사용<li>initial learning rate만 0.001로 시작</ul><li>pre-training, fine-tuning 모두 250K번째의 가중치를 사용했음</ul></ul><h3 id="43-rotation-robustness">4.3. Rotation robustness</h3><p>자체적으로 구축한 <strong><em>RoIC13</em></strong>에 테스트 수행</p><p><strong>[@ Detection task]</strong></p><p><img data-proofer-ignore data-src="https://images.velog.io/images/riverdeer/post/794cc67a-9e9a-48a2-9910-7979c4478a15/image.png" alt="" /></p><p><strong>[@ End-to-end recognition task]</strong></p><p><img data-proofer-ignore data-src="https://images.velog.io/images/riverdeer/post/bc6d8c66-fba0-4ab4-8397-8dd7829b4911/image.png" alt="" /></p><blockquote><ul><li>Evaluation protocol of <strong>IC15</strong> (<a href="https://rrc.cvc.uab.es/?ch=4&amp;com=tasks">출처</a>)<ul><li>ground truth bounding box와 50% 이상 겹치고 text 내용이 일치할 경우 true positive<li>일부 작은 텍스트에 대해 <strong>“do not care”</strong>의 레이블이 되어있는 경우가 있음<ul><li>ground truth bounding box와 50% 이상 겹치는 경우, 혹은 찾아내지 못하는 경우에도 evaluation에 포함되지 않는다.</ul></ul></ul></blockquote><h3 id="44-aspect-ratio-robustness">4.4. Aspect ratio robustness</h3><ul><li>극단적인 종횡비의 text instance가 많이 출현하는 <strong><em>MSRA-TD500</em></strong> 데이터셋에 대해 평가 진행<li>recognition annotation이 없기 때문에 detection task에 대한 평가만 수행</ul><p><img data-proofer-ignore data-src="https://images.velog.io/images/riverdeer/post/12f3885a-994b-485f-9025-896f716082e7/image.png" alt="" /></p><p><img data-proofer-ignore data-src="https://images.velog.io/images/riverdeer/post/2f896d1c-1180-44bd-bf99-187ee3579376/image.png" alt="" /></p><h3 id="45-shape-robustness">4.5. Shape robustness</h3><ul><li>horizontal, oriented, and curved 형태의 다양한 형태가 많이 포함된 <strong><em>Total-Text</em></strong>에 대해 평가 진행</ul><p><img data-proofer-ignore data-src="https://images.velog.io/images/riverdeer/post/c4972dec-aebf-4906-b389-7b3aadbb9af5/image.png" alt="" /></p><p><img data-proofer-ignore data-src="https://images.velog.io/images/riverdeer/post/d0b4dc33-bdd9-4553-b3fb-c5c1ebc74952/image.png" alt="" /></p><h3 id="46-small-text-instance-robustness">4.6. Small text instance robustness</h3><ul><li>TextDragon이 <code class="language-plaintext highlighter-rouge">Strong</code>, <code class="language-plaintext highlighter-rouge">Weak</code> task에서는 가장 좋은 성능을 보였지만 일반적인 경우라고 볼 수 있는 <code class="language-plaintext highlighter-rouge">Generic</code> task에서 가장 좋은 성능을 큰 차이로 보임<ul><li><code class="language-plaintext highlighter-rouge">Strong</code>: <code class="language-plaintext highlighter-rouge">Weak</code>: <code class="language-plaintext highlighter-rouge">Generic</code> = text word의 종류 수 $100: 1000+: 90k$</ul></ul><p><img data-proofer-ignore data-src="https://images.velog.io/images/riverdeer/post/8f0dfea7-48bd-49a8-a440-ac15a3713040/image.png" alt="" /></p><h3 id="47-ablation-study">4.7. Ablation study</h3><ul><li>options 1<ul><li>direct: segmentation/binary map을 곧바로 사용<li>indirect: segmentation/binary map을 추가적인 레이어를 더해 후처리를 가하여 사용</ul><li>options 2<ul><li>soft: soft probability map(값의 범위가 $[0,1]$)을 사용<li>hard: mask map의 값이 0과 1로만 구성된 것을 사용</ul><li>논문에서 제안하는 hard RoI masking 방법이 의미가 있음</ul><p><img data-proofer-ignore data-src="https://images.velog.io/images/riverdeer/post/84ae6000-bbd3-44bd-976f-55fe64eb48a9/image.png" alt="" /></p><hr /><h2 id="5-conclusion">5. Conclusion</h2><ul><li>end-to-end 학습 모델 Mask TextSpotter v3를 제안<ul><li>SPN을 도입하여 정확한 텍스트 영역 폴리곤을 생성할 수 있음</ul><li>다양한 데이터셋에 대한 검증 수행<ul><li><strong><em>Rotated ICDAR 2013</em></strong> 데이터셋에 rotation robustness 검증<li><strong><em>MSRA-TD500</em></strong> 데이터셋에 aspect ratio robustness 검증<li><strong><em>Total-Text</em></strong> 데이터셋에 shape robustness 검증<li><strong><em>IC15</em></strong> 데이터셋에 작은 텍스트 인스턴스에 대한 강건함도 검증</ul></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/paper-review/'>paper-review</a>, <a href='/categories/computer-vision/'>Computer Vision</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/deep-learning/" class="post-tag no-text-decoration" >deep_learning</a> <a href="/tags/ocr/" class="post-tag no-text-decoration" >ocr</a> <a href="/tags/scene-text/" class="post-tag no-text-decoration" >scene_text</a> <a href="/tags/segmentation/" class="post-tag no-text-decoration" >segmentation</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=[paper-review] Mask TextSpotter v3: Segmentation Proposal Network for Robust Scene Text Spotting - good-riverdeer&url=https://good-riverdeer.github.io/posts/Mask_TextSpotter/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=[paper-review] Mask TextSpotter v3: Segmentation Proposal Network for Robust Scene Text Spotting - good-riverdeer&u=https://good-riverdeer.github.io/posts/Mask_TextSpotter/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=[paper-review] Mask TextSpotter v3: Segmentation Proposal Network for Robust Scene Text Spotting - good-riverdeer&url=https://good-riverdeer.github.io/posts/Mask_TextSpotter/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/DDPM/">[paper-review] Denoising diffusion probabilistic models</a><li><a href="/posts/SLidR/">[paper-review] Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data</a><li><a href="/posts/TEBNER/">[paper-review] TEBNER: Domain Specific Named Entity Recognition with Type Expanded Boundary-aware Network.</a><li><a href="/posts/EfficientNet/">[paper-review] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a><li><a href="/posts/Vision_Transformer/">[paper-review] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/deep-learning/">deep_learning</a> <a class="post-tag" href="/tags/transformer/">transformer</a> <a class="post-tag" href="/tags/autonomous-driving/">autonomous_driving</a> <a class="post-tag" href="/tags/distillation/">distillation</a> <a class="post-tag" href="/tags/document-understanding/">document_understanding</a> <a class="post-tag" href="/tags/lidar/">lidar</a> <a class="post-tag" href="/tags/multimodal/">multimodal</a> <a class="post-tag" href="/tags/named-entity-recognition/">Named_Entity_Recognition</a> <a class="post-tag" href="/tags/ner/">NER</a> <a class="post-tag" href="/tags/ocr/">ocr</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Learning_to_Understand_Traffic_Signs/"><div class="card-body"> <span class="timeago small" >Dec 15, 2021<i class="unloaded">2021-12-15T12:10:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[paper-review] Learning to Understand Traffic Signs</h3><div class="text-muted small"><p> Guo, Y., Feng, W., Yin, F., Xue, T., Mei, S., &amp; Liu, C. L. (2021, October). Learning to Understand Traffic Signs. In Proceedings of the 29th ACM International Conference on Multimedia (pp. 2076...</p></div></div></a></div><div class="card"> <a href="/posts/EfficientNet/"><div class="card-body"> <span class="timeago small" >Oct 7, 2021<i class="unloaded">2021-10-07T20:48:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[paper-review] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</h3><div class="text-muted small"><p> Tan, M., &amp; Le, Q. (2019, May). Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning (pp. 6105-6114). PMLR. 개인적인 논문해석을 포함하고 ...</p></div></div></a></div><div class="card"> <a href="/posts/DocFormer/"><div class="card-body"> <span class="timeago small" >Dec 23, 2021<i class="unloaded">2021-12-23T13:20:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[paper-review] DocFormer: End-to-End Transformer for Document Understanding</h3><div class="text-muted small"><p> Appalaraju, S., Jasani, B., Kota, B. U., Xie, Y., &amp; Manmatha, R. (2021). DocFormer: End-to-End Transformer for Document Understanding. arXiv preprint arXiv:2106.11539. 개인적인 논문해석을 포함하고 있으며, 의역 ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/EfficientNet/" class="btn btn-outline-primary" prompt="Older"><p>[paper-review] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</p></a> <a href="/posts/Learning_to_Understand_Traffic_Signs/" class="btn btn-outline-primary" prompt="Newer"><p>[paper-review] Learning to Understand Traffic Signs</p></a></div><div id="disqus_thread" class="pt-2 pb-2"><p class="text-center text-muted small"> Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script type="text/javascript"> var disqus_config = function () { this.page.url = 'https://good-riverdeer.github.io/posts/Mask_TextSpotter/'; this.page.identifier = '/posts/Mask_TextSpotter/'; }; /* Lazy loading */ var disqus_observer = new IntersectionObserver(function (entries) { if(entries[0].isIntersecting) { (function () { var d = document, s = d.createElement('script'); s.src = 'https://.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); disqus_observer.disconnect(); } }, { threshold: [0] }); disqus_observer.observe(document.querySelector('#disqus_thread')); /* Auto switch theme */ function reloadDisqus() { /* Disqus hasn't been loaded */ if (typeof DISQUS === "undefined") { return; } if (document.readyState == 'complete') { DISQUS.reset({ reload: true, config: disqus_config }); } } const modeToggle = document.querySelector(".mode-toggle"); if (modeToggle !== null) { modeToggle.addEventListener('click', reloadDisqus); window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', reloadDisqus); } </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://github.com/good-riverdeer">riverdeer</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/deep-learning/">deep_learning</a> <a class="post-tag" href="/tags/transformer/">transformer</a> <a class="post-tag" href="/tags/autonomous-driving/">autonomous_driving</a> <a class="post-tag" href="/tags/distillation/">distillation</a> <a class="post-tag" href="/tags/document-understanding/">document_understanding</a> <a class="post-tag" href="/tags/lidar/">lidar</a> <a class="post-tag" href="/tags/multimodal/">multimodal</a> <a class="post-tag" href="/tags/named-entity-recognition/">Named_Entity_Recognition</a> <a class="post-tag" href="/tags/ner/">NER</a> <a class="post-tag" href="/tags/ocr/">ocr</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
