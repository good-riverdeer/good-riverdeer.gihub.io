<!DOCTYPE html><html lang="ko" mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="[paper-review] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks" /><meta property="og:locale" content="ko" /><meta name="description" content="Tan, M., &amp; Le, Q. (2019, May). Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning (pp. 6105-6114). PMLR." /><meta property="og:description" content="Tan, M., &amp; Le, Q. (2019, May). Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning (pp. 6105-6114). PMLR." /><link rel="canonical" href="https://good-riverdeer.github.io/posts/EfficientNet/" /><meta property="og:url" content="https://good-riverdeer.github.io/posts/EfficientNet/" /><meta property="og:site_name" content="good-riverdeer" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-10-07T20:48:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[paper-review] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks" /><meta name="twitter:site" content="@twitter_username" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"description":"Tan, M., &amp; Le, Q. (2019, May). Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning (pp. 6105-6114). PMLR.","url":"https://good-riverdeer.github.io/posts/EfficientNet/","@type":"BlogPosting","headline":"[paper-review] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks","dateModified":"2022-01-17T20:48:22+09:00","datePublished":"2021-10-07T20:48:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://good-riverdeer.github.io/posts/EfficientNet/"},"@context":"https://schema.org"}</script><title>[paper-review] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks | good-riverdeer</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="good-riverdeer"><meta name="application-name" content="good-riverdeer"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/school.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">good-riverdeer</a></div><div class="site-subtitle font-italic">딥러닝을 공부하는</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/good-riverdeer" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['riverdeer.youn','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>[paper-review] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>[paper-review] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> riverdeer </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Thu, Oct 7, 2021, 8:48 PM +0900" >Oct 7, 2021<i class="unloaded">2021-10-07T20:48:00+09:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Mon, Jan 17, 2022, 8:48 PM +0900" >Jan 17, 2022<i class="unloaded">2022-01-17T20:48:22+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3458 words">19 min read</span></div></div><div class="post-content"><p><a href="https://arxiv.org/abs/1905.11946">Tan, M., &amp; Le, Q. (2019, May). Efficientnet: Rethinking model scaling for convolutional neural networks. <em>In International Conference on Machine Learning</em> (pp. 6105-6114). PMLR.</a></p><p><em>개인적인 논문해석을 포함하고 있으며, 의역 및 오역이 남발할 수 있습니다. 올바르지 못한 내용에 대한 피드백을 환영합니다 :)</em></p><hr /><h2 id="1-introduction">1. Introduction</h2><p><strong>[@ Scaling up ConvNets]</strong></p><ul><li>ConvNets의 모델 크기를 늘리는 것은 정확도를 향상시키기 위한 가장 보편적인 방법<ul><li>ResNet이 대표적이며, ResNet-18에서 ResNet-200으로 더 많은 레이어를 사용함으로써 모델의 크기를 늘릴 수 있다.</ul><li>하지만 ConvNets의 Scaling에 대한 연구는 잘 이루어지지 않았다.<ul><li>이전에 모델의 <strong>depth, width, resolution</strong>을 확장하여 모델 성능을 높이려는 시도가 있었지만 이 세 가지 측면 중에서 한 가지 측면에만 모델을 확장하는 것이 일반적이었다.<li>세 가지 측면 모두에 대한 확장도 있었긴 했지만 사람이 임의의 값으로 지정했었다.<li><strong>depth</strong>: 모델 구성에서 레이어 수를 늘리는 것, ex) from ResNet-18 to ResNet-200<li><strong>width</strong>: ConvNet 구성에서 convolution layer의 채널 수를 늘리는 것<li><strong>resolution</strong>: 입력 이미지의 해상도를 키우는 것</ul></ul><p><img data-proofer-ignore data-src="https://images.velog.io/images/riverdeer/post/db2b4f5c-e203-4a2f-a22c-9faefa15febd/image.png" alt="" /></p><p><strong>[@ Rethinking Model Scaling]</strong></p><ul><li>본 논문에서는 ConvNets의 크기를 확장하는 프로세스를 다시 생각해보고 연구했다.<ul><li>정확도(accuracy)와 효율성(efficiency)를 모두 달성할 수 있는 원칙적인 방법에 대한 연구<li><strong>depth, width,resolution</strong> 모든 측면에서 균형을 맞추는 것이 중요</ul><li>기존의 임의의 비율로 <strong>depth, width, resolution</strong>을 조정하던 것과 다르게 일정하게 고정된 scaling coefficient로 모델의 크기를 조정하게 된다.<ul><li>$2^N$배 더 많은 컴퓨팅 자원을 사용할 수 있다면 <strong>depth, width, resolution</strong>을 각각 $\alpha^N, \beta^N, \gamma^N$배 하기만 하면 된다.<li>$\alpha, \beta, \gamma$는 간단한 grid search를 통해 찾을 수 있는 constant coefficient이다.</ul><li>직관적으로도 이 scaling 방법은 합리적인 방법이다.<ul><li>입력 이미지 해상도(<strong>resolution</strong>)를 늘리면 더 많은 레이어를 쌓아 receptive field를 늘리고(<strong>depth</strong>), 더 많은 채널을 계산에 포함해 세분화된 패턴을 잘 포착해야 한다(<strong>width</strong>).</ul></ul><hr /><h2 id="2-related-work">2. Related Work</h2><p><strong>[@ ConvNet Accuracy]</strong></p><ul><li>점차 ConvNets는 정확해지고는 있지만 그 크기도 커져가고 있다.<ul><li>2014년 ImageNet competition winner <strong>GoogleNet</strong>이 6.8M의 파라미터 수를 가지고 top-1 accuracy 74.8%<li>2017년 ImageNet competition winner <strong>SENet</strong>은 145M의 파라미터 수를 가지고 top-1 accuracy 82.7%<li>논문 발표 당시 <strong>GPipe</strong>는 557M의 파라미터 수로 84.3%의 top-1 accuracy</ul><li>더 높은 정확도가 우리에게는 필요한데 현재 하드웨어의 메모리 한계에 도달했다.</ul><p><strong>[@ ConvNet Efficiency]</strong></p><ul><li>Neural Architecture Search (NAS)가 효율적인 mobile-size의 ConvNets를 설계하는 데 널리 사용되고 있다.<ul><li><strong>depth, width, convolution kernel types and sizes</strong> 등등의 광범위하게 조정하여 최적의 ConvNets을 설계할 수 있다.<li>다만, 훨씬 더 큰 사이즈를 가진 모델들에 대해서는 많은 계산 비용이 필요하기 때문에, 이 방법을 적용하기 어렵다.</ul><li>본 논문에서, 거대한 ConvNets에 의 모델 효율성을 극대화하면서 정확도를 향상시킬 수 있는 방법에 대해 연구한다.</ul><p><strong>[@ Model Scaling]</strong></p><ul><li>depth 확장<ul><li>대표적인 scaling 방법으로 레이어의 수를 조정하여 크기를 조정한다.<li>ResNet은 ResNet-18 부터 ResNet-200까지 모델의 크기를 조정할 수 있다. (<em><a href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">He et al., 2016</a></em>)</ul><li>width 확장<ul><li>WideResNet(<em><a href="https://arxiv.org/abs/1605.07146">Zagoruyko &amp; Komodakis, 2016</a></em>) 이나 MobileNets(<em><a href="https://arxiv.org/abs/1704.04861">Howard et al., 2017</a></em>)은 모델의 크기를 Conv Layer 채널 수로 조정한 대표적 예시</ul><li>depth와 width가 모두 중요하다라고 생각한 연구<ul><li>(<em><a href="http://proceedings.mlr.press/v70/raghu17a.html">Raghu et al., 2017</a>; <a href="https://arxiv.org/abs/1806.10909">Lin &amp; Jegelka, 2018</a>; <a href="https://arxiv.org/abs/1703.02065">Sharir &amp; Shashua, 2018</a>; <a href="https://proceedings.neurips.cc/paper/2017/file/32cbf687880eb1674a07bf717761dd3a-Paper.pdf">Lu et al., 2018</a></em>)<li>여전히 ConvNet의 효율, 성능을 모두 향상 시킬 수 있는 방법에 대한 논의가 필요하다.</ul></ul><hr /><h2 id="3-compound-model-scaling">3. Compound Model Scaling</h2><h3 id="31-problem-formulation">3.1. Problem Formulation</h3><p><strong>[@ Annotations]</strong></p><ul><li>ConvNet Layer $i$를 함수 $Y_i=\mathcal F_i(X_i)$로 나타낼 수 있다. ($X_i=\left &lt; H_i, W_i, C_i\right &gt;$)<li>ConvNet $\mathcal N$은 레이어들이 합성된 리스트의 형태로 표현할 수 있다: $\mathcal N = \mathcal F_k \bigodot…\bigodot\mathcal F_2\bigodot\mathcal F_1(X_1)$<ul><li>사실 ConvNet layer들이 모여 하나의 stage로 구성되고, 이 stage들의 모여 하나의 ConvNet을 이루며 이 stage 구조가 반복되는 경우가 많다.<ul><li>예를 들어, ResNet은 각 stage가 여러 layer로 구성된 5 stages가 존재<li>모든 layer들은 같은 convolution type을 갖는다.<li>모든 stage들은 같은 아키텍처를 갖는다.</ul></ul><li><p>따라서 아래와 같이 ConvNet을 정의할 수 있다.</p><p>\(\mathcal N = \bigodot_{i=1...s}\mathcal F_i^{L_i}(X_{\left &lt; H_i, W_i, C_i\right &gt;})\)</p><ul><li>$i$번째 stage에서 $\mathcal F_i$ layer가 $L_i$번 반복 -&gt; $\mathcal F_i^{L_i}$</ul></ul><p><strong>[@ 최적화문제 정의]</strong> Model scaling은 baseline 아키텍처에서 ConvNet layer의 디자인을 변경하지 않은 채로 모델의 <strong><em>length</em></strong>($L_i$), <strong><em>width</em></strong>($C_i$), <strong><em>resolution</em></strong>($H_i, W_i$)을 수정하는 것이 일반적이다. 각 레이어 $i$에 대해 이 변수들($L_i, C_i, H_i, W_i$)을 최적으로 찾는 것만으로도 엄청난 경우의 수를 갖게 된다. 즉, 우리의 목적함수는 아래와 같이 정의할 수 있게 된다.</p><p>\(\begin {aligned} \max_{d, w, r} &amp;&amp;&amp; Accuracy(\mathcal N(d, w, r)) \\ s.t. &amp;&amp;&amp; N(d, w, r) = \bigodot_{i=1...s}\mathcal {\hat{F_i}}^{d*\hat{L_i}}(X_{\left &lt; r \cdot \hat{H_i}, r\cdot \hat{W_i}, w \cdot \hat{C_i} \right &gt;}) \\ &amp;&amp;&amp; \mathrm{Memory}(\mathcal N) \le \mathrm{target\_memory} \\ &amp;&amp;&amp; \mathrm{FLOPS}(\mathcal N) \le \mathrm{target\_flops} \end {aligned}\) \(\mathrm{where} \space \mathcal {\hat{F_i}}, \hat{L_i}, \hat{H_i}, \hat{W_i}, \hat{C_i} \space \text{are predefined parameters in baseline network}\)</p><p><br /></p><h3 id="32-scaling-dimensions">3.2. Scaling Dimensions</h3><p>위 최적화에 있어 주요 문제점은 변수 $d, w, r$이 서로서로 영향이 있으며 각각 변수들을 조정함에 있어 서로 다른 컴퓨팅 자원의 한계 하에서 조정해야 한다는 점이다.</p><p><strong>[@ Depth]</strong></p><ul><li>ConvNets를 조정하는 데 가장 보편적인 방법<li><p>그 근거는 모델을 깊게 쌓을수록 더 풍부하고 복잡한 특징들을 포착하여 새로운 task에도 잘 일반화될 수 있을 것이라는 직관적인 추론이다.</p><li><strong>But.</strong> 신경망을 깊게 구성할수록 vanishing gradient 문제는 피할 수 없다. (<em>ResNet-1000은 ResNet-101보다 훨씬 더 많은 레이어로 구성했음에도 비슷한 정확도를 보인다.</em>)</ul><p><strong>[@ Width]</strong></p><ul><li>ConvNets의 width를 조정하는 방법은 주로 작은 사이즈의 모델을 크게할 때 사용된다.<li><p>더 넓은 신경망을 구성할수록 fine-grained features를 잘 포착할 수 있으며 학습이 더 쉬워진다는 이전 연구도 있었다. (<a href="https://arxiv.org/abs/1605.07146"><em>Zhagoruyko &amp; Komodakis, 2016</em></a>)</p><li><strong>But.</strong> 얇고 넓은 모델들은 higher level features를 포착하는 데 어려움을 가지는 경향이 있다.<li>본 논문에서의 실험(<strong><em>Figure 3 (left)</em></strong>)에서도 모델이 넓어질수록 금새 모델의 정확도가 포화됨을 볼 수 있다.</ul><p><strong>[@ Resolution]</strong></p><ul><li>입력 이미지의 해상도가 높은 해상도를 가질수록 ConvNets는 더 fine-grained patterns를 포착할 수 있다.<li>초기 ConvNets는 $224\times 224$의 해상도로 출발했으나 최근에는 $299\times 299, 331\times 331$ 크기의 이미지를 사용하는 경향이 있다.<li><p>Object detection과 같은 task에서는 $600 \times 600$의 해상도를 높은 해상도를 사용한다.</p><li><strong>But.</strong> 본 논문에서의 실험 결과(<strong><em>figure 3 (right)</em></strong>)를 보면 해상도가 점점 커질수록 정확도가 늘어나는 비율은 점차 줄어든다.</ul><p><strong style="font-size: 20px; color: red;">[Observation 1]</strong><strong style="font-size: 20px;"> - 신경망의 크기를 <i>width, depth, resolution</i> 중 한 가지 측면에서 조정하는 것은 정확도를 향상시킬 수 있지만 모델이 커질수록 그 효과는 줄어든다.</strong></p><p><img data-proofer-ignore data-src="https://images.velog.io/images/riverdeer/post/ac67f7c3-fef8-416f-bc22-e74587e7417f/image.png" alt="" /></p><p><br /></p><h3 id="33-compound-scaling">3.3. Compound Scaling</h3><p>앞선 실험을 통해 <strong>depth, width, resolution</strong> 각각의 측면에서 크기를 조정하는 것이 독립적이지 않다는 점을 알 수 있었다. 직관적으로도 높은 해상도의 이미지는 더 깊은 모델을 사용하는 것이 올바르다고 느껴진다. 따라서 본 논문에서는 서로 다른 <strong>depth, width, resolution</strong> 측면의 스케일링 정도를 조정하고 각각이 균등하게 조정되야 함을 주장하고 있다.</p><p><strong><em>Figure 4</em></strong>는 이러한 직관적인 추론을 증명하는 결과이다. $d$와 $r$을 조정하며 서로 다른 네 가지의 환경에서 $w$를 점차 늘려가며 실험한 결과, $d=1.0, r=1.0$일 때 즉, $w$만 차별적으로 조정할 때 정확도가 포화되는 속도가 가장 빨랐다.</p><p><strong style="font-size: 20px; color: red;">[Observation 2]</strong><strong style="font-size: 20px;"> - 정확도와 효율성을 모두 잡아내기 위해선, <i>width, depth, resolution</i>을 균형있게 조정하는 것이 중요하다.</strong></p><p><img data-proofer-ignore data-src="https://images.velog.io/images/riverdeer/post/e258ee71-bb04-4dc5-939f-35aa6065196f/image.png" alt="" /></p><p>사실 일부 선행 연구에서 <strong>depth, width, resolution</strong>을 균형있게 조정하는 연구를 진행했었지만 이 수치를 모두 수동으로 조정했었다. (<em><a href="https://openaccess.thecvf.com/content_cvpr_2018/html/Zoph_Learning_Transferable_Architectures_CVPR_2018_paper.html">Zoph et al., 2018</a>, <a href="https://ojs.aaai.org/index.php/AAAI/article/view/4405">Real et al., 2019</a></em>)</p><p><strong>[@ Compound Scaling Method]</strong> 본 논문에서는 사용자가 자신의 하드웨어 스펙에 맞게 크기 조정 정도를 <em>compound coefficient</em> $\boldsymbol\phi$로 조정할 수 있는 방법을 제안한다.</p>\[\begin {aligned} \mathrm{depth:} &amp;&amp;&amp; d=\alpha^\boldsymbol\phi \\ \mathrm{width:} &amp;&amp;&amp; w = \beta^\boldsymbol\phi \\ \mathrm{resolution:} &amp;&amp;&amp; r = \gamma^\boldsymbol\phi \\ \mathrm{s.t.} &amp;&amp;&amp; \alpha\cdot\beta^2\cdot\gamma^2 \approx 2 \\ &amp;&amp;&amp; \alpha \ge 1, \beta \ge 1, \gamma \ge 1 \end {aligned}\]<blockquote><h4 id="-제약조건-alphacdotbeta2cdotgamma2-approx-2-에-대하여">@ 제약조건 $\alpha\cdot\beta^2\cdot\gamma^2 \approx 2$ 에 대하여.</h4><ul><li>ConvNets의 <strong>depth, width, resolution</strong>를 조정함에 따라 연산의 <strong>FLOPS</strong>는 각각 $d, w^2, r^2$으로 비례한다.<ul><li>즉, <strong>depth</strong>를 두 배 늘리면 <strong>FLOPS</strong>가 두 배 늘어나지만<li><strong>width, resolution</strong>을 각각 두 배 늘리면 <strong>FLOPS</strong>는 각각 네 배씩 증가한다.</ul><li>ConvNet 중심의 모델들은 대부분의 계산 비용이 ConvNets에서 발생하기 때문에 총 모델의 <strong>FLOPS</strong>는 $\alpha\cdot\beta^2\cdot\gamma^2$에 근접하게 늘어난다.<li>본 논문에서는 $\alpha\cdot\beta^2\cdot\gamma^2$의 값을 $2$에 근접하게 제약조건을 걸었기 때문에 총 <strong>FLOPS</strong>는 $2^\boldsymbol\phi$만큼 증가한다고 볼 수 있다.</ul></blockquote><hr /><h2 id="4-efficientnet-architecture">4. EfficientNet Architecture</h2><p>Model Scaling은 baseline 모델의 각 레이어 연산을 수정하지 않기 때문에 좋은 baseline 모델을 갖추는 것이 중요하다. 현존하는 ConvNets 아키텍처들에도 본 논문에서 제안하는 scaling 방법을 적용할 것이지만 그 효과를 더 잘 보여주기 위해 <strong>EfficientNet</strong>이라는 새로운 baseline 모델을 제안한다.</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Tan_MnasNet_Platform-Aware_Neural_Architecture_Search_for_Mobile_CVPR_2019_paper">Tan et al., 2019</a>에서의 연구에서 처럼 <strong>Multi-objective neural architecture search (AutoML)</strong> 방법을 사용해 <strong>정확도</strong>와 <strong>FLOPS</strong>를 모두 최적화하는 모델 구조, <strong>EfficientNet-B0</strong>를 찾았다.</p><ul><li><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Tan_MnasNet_Platform-Aware_Neural_Architecture_Search_for_Mobile_CVPR_2019_paper">Tan et al., 2019</a> 에서의 연구와 같은 search space<li>optimization goal: $ACC(m) \times [FLOPS(m)/T]^w$<ul><li>$ACC(m), FLOPS(m)$: 모델 $m$에 대한 정확도와 FLOPS<li>$T$: target FLOPS<li>$w$: 정확도와 FLOPS의 trade-off를 조정하는 hyperparameter 논문에서는 $-0.07$ 사용</ul></ul><p><strong>[@ Compound Scaling Method의 사용]</strong> 이 <strong>EfficientNet-B0</strong>를 baseline으로 compound scaling method를 사용</p><ul><li><strong>Step 1.</strong> $\boldsymbol\phi=1$로 고정하고, 두 배의 컴퓨팅 자원이 사용 가능함을 가정하고 $\alpha, \beta, \gamma$를 작은 grid search를 통해 최적값 $\alpha=1.2, \beta=1.1, \gamma=1.15$를 얻을 수 있었다.<li><strong>Step 2.</strong> $\boldsymbol\phi$를 점차 늘려가며 차례로 <strong>EfficientNet-B1 ~ B7</strong>을 획득한다.<ul><li>모델의 몸집을 키운 다음에 최적의 $\alpha, \beta, \gamma$ 값을 찾는 것이 훨씬 더 나은 성능을 보장하겠지만 이 방법은 너무 계산비용이 크다.<li><em>앞선 실험을 통해 $\alpha, \beta, \gamma$의 최적 조합이 있음을 밝혀내고 이를 작은 모델에서 최적 조합을 찾고 이를 점차 몸집을 늘려 성능이 뛰어난 모델을 만들었다는 점이 본 연구의 주요 contribution이라 볼 수 있을 것이다.</em></ul></ul><hr /><h2 id="5-experiments">5. Experiments</h2><h3 id="51-scaling-up-mobilenets-and-resnets">5.1. Scaling Up MobileNets and ResNets</h3><p><strong><em>Table 3</em></strong>에서 MobileNets, ResNets 모두에서 논문에서 제안하는 <strong>Compound Scale Method</strong>가 FLOPS는 비슷하게 유지하면서 성능을 크게 향상시킴을 확인할 수 있다.</p><p><img data-proofer-ignore data-src="https://images.velog.io/images/riverdeer/post/e56e8f03-8ae7-4152-b90f-0362cff68392/image.png" alt="" /></p><h3 id="52-imagenet-results-for-efficientnet">5.2. ImageNet Results for EfficientNet</h3><p><strong><em>Table 2</em></strong>는 각 <strong>EfficientNet</strong>이 비슷한 성능을 보이는 모델들이 계산 비용 면에서는 크게 앞서는 모습을 보여준다.</p><p><img data-proofer-ignore data-src="https://images.velog.io/images/riverdeer/post/e012d0ab-65a9-4c9a-a2b9-a3de41325cbc/image.png" alt="" /></p><h3 id="53-transfer-learning-results-for-efficientnet">5.3. Transfer Learning Results for EfficientNet</h3><p>전이학습에 사용되어도 파라미터 수를 현저하게 줄일 수 있다.</p><p><img data-proofer-ignore data-src="https://images.velog.io/images/riverdeer/post/b34fa8da-d803-4573-94f4-3d42d9c5f49a/image.png" alt="" /></p><hr /><h2 id="6-discussion">6. Discussion</h2><p><strong>[@ Compound Scaling Method 빼기 EfficientNet Architecture]</strong> EfficientNet에서 논문에서 제안하는 Compound Scaling Method의 효과를 덜어내보았다. <strong>depth, width, resolution</strong> 한 가지 측면에서만 scaling을 진행하는 것보다 Compound Scaling Method가 확실히 더 나은 성능을 보임을 알 수 있다. <img data-proofer-ignore data-src="https://images.velog.io/images/riverdeer/post/7b5b6db1-a950-4339-aa04-fee0d7fe9399/image.png" alt="" /></p><p><strong>[@ Class Activation Map]</strong> 위 실험에서 사용된 모델들의 예측에 집중하는 영역을 살펴보면 Compound Scaling Method를 사용했을 때 물체의 디테일에 잘 집중하는 모습을 볼 수 있다.</p><p><img data-proofer-ignore data-src="https://images.velog.io/images/riverdeer/post/688b4f30-94ab-452d-ad3b-46058ef1f691/image.png" alt="" /></p><hr /><h2 id="7-conclusion">7. Conclusion</h2><ul><li>중요하면서도 이전의 많은 연구들이 간과하고 있던 ConvNets을 몸집을 늘릴 때 <strong>depth, width, resolution</strong>을 균형있게 늘려야 한다는 점을 체계적으로 밝혀냈다.<li>이를 위한 <strong>Compound Scaling Method</strong>를 제안하였으며 간단하고 효과적으로 baseline ConvNets의 몸집을 키울 수 있었다. 이 과정에서 보다 원칙적인 방법으로 모델의 효율성까지 유지할 수 있었다.<li>EfficientNet이라는 mobile-size의 baseline 모델을 제시했으며, 이 모델을 기반으로 <strong>Compound Scaling Method</strong>를 사용해 모델을 효과적으로 더 나은 성능을 위해 사이즈를 늘릴 수 있음을 보였다.<li>결과적으로 <strong>EfficientNet+Compound Scaling Method</strong>로 계산 비용을 최소화하며 최고 수준의 성능을 달성할 수 있었다.</ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/paper-review/'>paper-review</a>, <a href='/categories/computer-vision/'>Computer Vision</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/deep-learning/" class="post-tag no-text-decoration" >deep_learning</a> <a href="/tags/cnn/" class="post-tag no-text-decoration" >CNN</a> <a href="/tags/convolution/" class="post-tag no-text-decoration" >convolution</a> <a href="/tags/efficientnet/" class="post-tag no-text-decoration" >efficientnet</a> <a href="/tags/transfer-learning/" class="post-tag no-text-decoration" >transfer_learning</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=[paper-review] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks - good-riverdeer&url=https://good-riverdeer.github.io/posts/EfficientNet/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=[paper-review] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks - good-riverdeer&u=https://good-riverdeer.github.io/posts/EfficientNet/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=[paper-review] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks - good-riverdeer&url=https://good-riverdeer.github.io/posts/EfficientNet/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/DDPM/">[paper-review] Denoising diffusion probabilistic models</a><li><a href="/posts/SLidR/">[paper-review] Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data</a><li><a href="/posts/TEBNER/">[paper-review] TEBNER: Domain Specific Named Entity Recognition with Type Expanded Boundary-aware Network.</a><li><a href="/posts/EfficientNet/">[paper-review] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a><li><a href="/posts/Vision_Transformer/">[paper-review] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/deep-learning/">deep_learning</a> <a class="post-tag" href="/tags/transformer/">transformer</a> <a class="post-tag" href="/tags/autonomous-driving/">autonomous_driving</a> <a class="post-tag" href="/tags/distillation/">distillation</a> <a class="post-tag" href="/tags/document-understanding/">document_understanding</a> <a class="post-tag" href="/tags/lidar/">lidar</a> <a class="post-tag" href="/tags/multimodal/">multimodal</a> <a class="post-tag" href="/tags/named-entity-recognition/">Named_Entity_Recognition</a> <a class="post-tag" href="/tags/ner/">NER</a> <a class="post-tag" href="/tags/ocr/">ocr</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Mask_TextSpotter/"><div class="card-body"> <span class="timeago small" >Nov 22, 2021<i class="unloaded">2021-11-22T18:20:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[paper-review] Mask TextSpotter v3: Segmentation Proposal Network for Robust Scene Text Spotting</h3><div class="text-muted small"><p> Liao, M., Pang, G., Huang, J., Hassner, T., &amp; Bai, X. (2020). Mask textspotter v3: Segmentation proposal network for robust scene text spotting. In Computer Vision–ECCV 2020: 16th European Conf...</p></div></div></a></div><div class="card"> <a href="/posts/Learning_to_Understand_Traffic_Signs/"><div class="card-body"> <span class="timeago small" >Dec 15, 2021<i class="unloaded">2021-12-15T12:10:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[paper-review] Learning to Understand Traffic Signs</h3><div class="text-muted small"><p> Guo, Y., Feng, W., Yin, F., Xue, T., Mei, S., &amp; Liu, C. L. (2021, October). Learning to Understand Traffic Signs. In Proceedings of the 29th ACM International Conference on Multimedia (pp. 2076...</p></div></div></a></div><div class="card"> <a href="/posts/DocFormer/"><div class="card-body"> <span class="timeago small" >Dec 23, 2021<i class="unloaded">2021-12-23T13:20:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[paper-review] DocFormer: End-to-End Transformer for Document Understanding</h3><div class="text-muted small"><p> Appalaraju, S., Jasani, B., Kota, B. U., Xie, Y., &amp; Manmatha, R. (2021). DocFormer: End-to-End Transformer for Document Understanding. arXiv preprint arXiv:2106.11539. 개인적인 논문해석을 포함하고 있으며, 의역 ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/ArcFace_project/" class="btn btn-outline-primary" prompt="Older"><p>[project-review] ArcFace를 활용한 한국인 안면 인식</p></a> <a href="/posts/Mask_TextSpotter/" class="btn btn-outline-primary" prompt="Newer"><p>[paper-review] Mask TextSpotter v3: Segmentation Proposal Network for Robust Scene Text Spotting</p></a></div><div id="disqus_thread" class="pt-2 pb-2"><p class="text-center text-muted small"> Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script type="text/javascript"> var disqus_config = function () { this.page.url = 'https://good-riverdeer.github.io/posts/EfficientNet/'; this.page.identifier = '/posts/EfficientNet/'; }; /* Lazy loading */ var disqus_observer = new IntersectionObserver(function (entries) { if(entries[0].isIntersecting) { (function () { var d = document, s = d.createElement('script'); s.src = 'https://.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); disqus_observer.disconnect(); } }, { threshold: [0] }); disqus_observer.observe(document.querySelector('#disqus_thread')); /* Auto switch theme */ function reloadDisqus() { /* Disqus hasn't been loaded */ if (typeof DISQUS === "undefined") { return; } if (document.readyState == 'complete') { DISQUS.reset({ reload: true, config: disqus_config }); } } const modeToggle = document.querySelector(".mode-toggle"); if (modeToggle !== null) { modeToggle.addEventListener('click', reloadDisqus); window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', reloadDisqus); } </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://github.com/good-riverdeer">riverdeer</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/deep-learning/">deep_learning</a> <a class="post-tag" href="/tags/transformer/">transformer</a> <a class="post-tag" href="/tags/autonomous-driving/">autonomous_driving</a> <a class="post-tag" href="/tags/distillation/">distillation</a> <a class="post-tag" href="/tags/document-understanding/">document_understanding</a> <a class="post-tag" href="/tags/lidar/">lidar</a> <a class="post-tag" href="/tags/multimodal/">multimodal</a> <a class="post-tag" href="/tags/named-entity-recognition/">Named_Entity_Recognition</a> <a class="post-tag" href="/tags/ner/">NER</a> <a class="post-tag" href="/tags/ocr/">ocr</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
