<!DOCTYPE html><html lang="ko" mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="[paper-review] Denoising diffusion probabilistic models" /><meta property="og:locale" content="ko" /><meta name="description" content="Ho, J., Jain, A., &amp; Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in neural information processing systems, 33, 6840-6851. official implementation pytorch implementation" /><meta property="og:description" content="Ho, J., Jain, A., &amp; Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in neural information processing systems, 33, 6840-6851. official implementation pytorch implementation" /><link rel="canonical" href="https://good-riverdeer.github.io/posts/DDPM/" /><meta property="og:url" content="https://good-riverdeer.github.io/posts/DDPM/" /><meta property="og:site_name" content="good-riverdeer" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-07-19T16:30:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[paper-review] Denoising diffusion probabilistic models" /><meta name="twitter:site" content="@twitter_username" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"description":"Ho, J., Jain, A., &amp; Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in neural information processing systems, 33, 6840-6851. official implementation pytorch implementation","url":"https://good-riverdeer.github.io/posts/DDPM/","@type":"BlogPosting","headline":"[paper-review] Denoising diffusion probabilistic models","dateModified":"2023-08-02T17:17:38+09:00","datePublished":"2023-07-19T16:30:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://good-riverdeer.github.io/posts/DDPM/"},"@context":"https://schema.org"}</script><title>[paper-review] Denoising diffusion probabilistic models | good-riverdeer</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="good-riverdeer"><meta name="application-name" content="good-riverdeer"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/school.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">good-riverdeer</a></div><div class="site-subtitle font-italic">딥러닝을 공부하는</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/good-riverdeer" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['riverdeer.youn','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>[paper-review] Denoising diffusion probabilistic models</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>[paper-review] Denoising diffusion probabilistic models</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> riverdeer </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Wed, Jul 19, 2023, 4:30 PM +0900" >Jul 19<i class="unloaded">2023-07-19T16:30:00+09:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Wed, Aug 2, 2023, 5:17 PM +0900" >Aug 2<i class="unloaded">2023-08-02T17:17:38+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3994 words">22 min read</span></div></div><div class="post-content"><ul><li><a href="https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html">Ho, J., Jain, A., &amp; Abbeel, P. (2020). Denoising diffusion probabilistic models. <em>Advances in neural information processing systems, 33</em>, 6840-6851.</a><li><a href="https://github.com/hojonathanho/diffusion">official implementation</a><li><a href="https://github.com/lucidrains/denoising-diffusion-pytorch">pytorch implementation</a></ul><p><em>개인적인 논문해석을 포함하고 있으며, 의역 및 오역이 남발할 수 있습니다. :)</em></p><p><br /></p><hr /><p><br /></p><h2 id="1-introduction">1. Introduction</h2><p>Deep generative models, 심층 신경망을 활용한 생성 모델들은 여러 다양한 모달리티에서 널리 쓰이며 높은 품질의 데이터 샘플들을 생성하는 데 활용되어왔습니다. GAN(Generative adversarial networks)과 VAE(variational autoencoder)가 대표적입니다.</p><p>본 논문에서는 diffusion probabilistic model, 즉 diffusion model을 생성 모델로써 제안합니다. 저자들은 diffusion model을 <strong>“똑같은 샘플을 생성하기 위해 variational inference를 통해 일정 시간 파라미터가 학습되는 Markov chain”</strong>으로 정의합니다.</p><p><img data-proofer-ignore data-src="../../assets/img/DDPM/fig1.png" alt="image-description" /></p><p>위 이미지에서 \(q(\mathbf{x}_t \mid \mathbf{x}_{t-1})\) 과정을 <em>diffusion process</em>. 반대로, $p_{\theta}(\mathbf{x}_{t-1} \mid \mathbf{x_t})$ 과정을 <em>diffusion process</em>를 역으로 되돌리며 학습하는 과정인 <em>reverse process</em>라고 이해할 수 있습니다. 이와 같은 <em>reverse process</em>를 학습함으로써 노이즈가 쌓여있는 $\mathbf{x}_T$로부터 원본 데이터 $\mathbf{x}_0$를 복원할 수 있습니다.</p><p><br /></p><hr /><p><br /></p><h2 id="2-background">2. Background</h2><p>지금까지의 latent variable을 통해 생성해냈던 여타 다른 생성모델들과 이 diffusion model을 다르게 만드는 점이 바로 <em>diffusion process</em>입니다. latent variable을 활용하는 대표적 모델 VAE는 latent를 encoding하는 encoder와 이를 decoding하는 decoder를 모두 학습하는 반면, diffusion model은 <em>forward/diffusion process</em>는 고정된채로 이를 풀어내는 <em>reverse process</em>만을 학습합니다.</p><h3 id="forwarddiffusion-process">forward/diffusion process</h3><p>위에서 언급한 것처럼 고정된 Markov chain을 여러번 거치는 방식으로 수행됩니다.</p>\[\begin{align} q(\mathbf{x}_t|\mathbf{x}_{t-1}) &amp;= \mathcal{N}(\mathbf{x}_t ; \sqrt{1-\beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I}), \\ q(\mathbf{x}_{1:T} | \mathbf{x}_0) &amp;= \prod^T_{t=1} q(\mathbf{x}_t|\mathbf{x}_{t-1}) \end{align}\]<p>첫 입력, 원본 이미지 \(\mathbf{x}_0\)를 총 $T$번의 Markov chain을 거침으로써 이미지에 노이즈를 부여하게 됩니다. 이 때, $t$번의 step을 차근차근 밟아나가면서 $\mathbf{x}_0$에서 $\mathbf{x}_t$를 만들어 낼 수 있겠지만, 한 번에 이를 만들어 낼 수 있습니다.</p><p>차근차근 밟아나가며 \(\mathbf{x}_t\)를 생성하는 것은 memory는 물론 computing resource도 많이 소모하는 방법입니다. 저자들은 어차피 stochastic gradient를 사용하기 때문에 $t$에 대한 기댓값을 한 번에 계산하는 식으로 구현할 수 있다고 말합니다.</p>\[\begin{align} q(\mathbf{x}_t | \mathbf{x}_0) &amp;= \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\mathbf{\alpha}}_t} \mathbf{x}_0, (1-\bar\alpha_t)\mathbf{I}), \\ \alpha_t &amp;= 1-\beta_t, \bar\alpha_t = \prod^t_{s=1}\alpha_s \end{align}\]<h3 id="reverse-process">Reverse Process</h3><p>노이즈를 총 \(T\)번 부여한 뒤, 이 결과 \(\mathbf{x}_T\)를 원복하려는 생성 태스크에선 diffusion process의 역 계산인 $q(\mathbf{x}_{t-1} \mid \mathbf{x}_t)$를 타겟으로 학습하고자 하는 solution입니다. 따라서, 이 분포를 근사할 수 있는 모델 parameter, $\theta$를 학습하는 것이 diffusion model의 주요 idea입니다.</p>\[\begin{align} p_{\theta}(\mathbf{x}_{0:T}) &amp;= p(\mathbf{x}_T) \prod^T_{t=1} p_{\theta}(\mathbf{x}_{t-1} | \mathbf{x}_t), \\ p_{\theta}(\mathbf{x}_{t-1} | \mathbf{x}_t) &amp;= \mathcal{N}(\mathbf{x}_{t-1}; \mu_{\theta}(\mathbf{x}_t, t), \Sigma_{\theta}(\mathbf{x}_t, t)) \end{align}\]<p>위 식에서, 각 $t$ step에서의 평균 $\mu_{\theta}$와 표준편차 $\Sigma_{\theta}$는 학습되어야 하는 parameter들 입니다. 다만, 여기의 $\Sigma_{\theta}$는 학습 가능한 parameter로 두고 학습할 수 있겠지만 저자들의 경험상 상수로 두고 학습해도 무방하다고 합니다. 이 단계들의 시작점인 $p(\mathbf{x}_T)=\mathcal{N}(\mathbf{x}_T; \mathbf{0}, \mathbf{I})$는 충분히 Gaussian Noise가 층층히 쌓인 결과이기 때문에 표준정규분포의 형태로 정의할 수 있습니다.</p><p><br /></p><hr /><p><br /></p><h2 id="3-diffusion-models-and-denoising-autoencoders">3. Diffusion models and denoising autoencoders</h2><p>이제 본 논문의 주요 기여점, 위에서 언급한 idea를 어떻게 하나의 loss function으로 정리할 수 있었는지 살펴보겠습니다.</p>\[\mathbb{E}_q[\underbrace{D_{KL}(q(\mathbf{x}_T|\mathbf{x}_0) || p(\mathbf{x}_T))}_{L_T} + \sum_{t&gt;1}\underbrace{D_{KL}(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) || p_{\theta}(\mathbf{x}_{t-1} | \mathbf{x}_t))}_{L_{t-1}} \underbrace{-\log p_{\theta}(\mathbf{x}_0 | \mathbf{x}_1)}_{L_0}]\]<p>위 최종 loss의 정의는 각 step $t$의 변화에 따라 loss term이 어떻게 구성되어 있는지 알 수 있습니다. 전반적인 loss의 구성은 각 step에 따라 <em>forward process</em> $q$분포와 <em>reverse process</em> $p$분포의 차이를 KL Divergence의 형태로 정의해 두 분포의 차이를 줄이는 컨셉으로 이루어져 있습니다.</p><h3 id="forward-process-and-loss">Forward process and Loss</h3><p>여기에서 \(\mathbf{x}_T\)는 항상 Gaussian distribution을 근사하게 됩니다 ($p(\mathbf{x}_T) = \mathcal{N}(\mathbf{x}_T; \mathbf{0}, \mathbf{I})$). 따라서 $q(\mathbf{x}_T \mid \mathbf{x}_0)$는 사실상 표준정규분포로 빠르게 수렴할 것입니다. 때문에 학습 간 무시해도 되는 loss term입니다.</p><p>$L_T$의 의미를 다시 생각해보면, 아래 두 분포의 차이를 의미하는 것을 알 수 있습니다.</p><ul><li>\(p(\mathbf{x}_T)\): 노이즈가 쌓인 $\mathbf{x}_T$의 분포<li>\(q(\mathbf{x}_T \mid \mathbf{x}_0)\): $\mathbf{x}_0$가 주어졌을 때, $\mathbf{x}_T$로 diffusion process가 진행된 후의 분포</ul><p>따라서 두 분포는 모두 자연스럽게 랜덤한 정규분포일 것이고, 그 차이는 아주 미미할 것입니다.</p><h3 id="reverse-process-and-loss">Reverse process and Loss</h3><p>해당 loss term이 DDPM의 핵심 loss term이라고 할 수 있습니다. 그 의미를 생각해보면, $p$와 $q$의 reverse process 및 forward process 분포 차이를 줄이는 것으로 볼 수 있죠.</p><p>허나 갑자기 \(L_{t-1}\)에 타겟으로 제공되어야 할 \(q(\mathbf{x}_{t-1} \mid \mathbf{x}_t)\) 대신 \(q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)\)가 등장해 상당히 어색합니다. 이는 단일 조건이 주어진 \(q(\mathbf{x}_{t-1} \mid \mathbf{x}_t)\) 분포는 구할 수 없지만, 하나의 조건이 더 주어진 $q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)$ 분포는 Bayes’ rule에 의해 정리되고 추적이 가능하기 때문입니다.</p>\[\begin{align} q(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{x}_0) &amp;= \mathcal{N}(\mathbf{x}_{t-1}; \tilde\mu(\mathbf{x}_t, \mathbf{x}_0), \tilde\beta_t\mathbf{I}) \\ &amp;= q(\mathbf{x}_t | \mathbf{x}_{t-1}, \mathbf{x}_0) \frac{q(\mathbf{x}_{t-1} | \mathbf{x}_0)}{q(\mathbf{x}_t | \mathbf{x}_0)} \\ &amp;= \cdots \\ &amp;\propto \exp \left( -\frac{1}{2} \left( (\frac{\alpha_t}{\beta_t}+\frac{1}{1-\bar\alpha_{t-1}})\mathbf{x}^2_{t-1} - (\frac{2\sqrt{\alpha_t}}{\beta_t}+\frac{2\sqrt{\bar\alpha_{t-1}}}{1-\bar\alpha_{t-1}}\mathbf{x}_0)\mathbf{x}_{t-1} + C(\mathbf{x}_t, \mathbf{x}_0) \right) \right), \\ &amp; \text{where } C(\mathbf{x}_t, \mathbf{x}_0) \text{ is some function not involving } \mathbf{x}_{t-1}. \end{align}\]<p>자세한 수식은 <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#reverse-diffusion-process">참조 포스팅</a>에서 확인해주시기 바랍니다. 위 수식에서 \(\mathbf{x}_{t-1}\)에 대한 <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">Gaussian density function의 정의</a>에 의해 \(\mathbf{x}_{t-1}\)에 대한 Gaussian 분포의 평균과 분산을 도출할 수 있게 됩니다.</p>\[\begin{align} \tilde\beta_t &amp;= \frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t} \cdot \beta_t \\ \tilde\mu_t(\mathbf{x}_t, \mathbf{x}_0) &amp;= \frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})}{1-\bar\alpha_t}\mathbf{x}_t + \frac{\sqrt{\bar\alpha_{t-1}}\beta_t}{1-\bar\alpha_t}\mathbf{x}_0 \end{align}\]<p>다시, <a href="https://lilianweng.github.io/posts/2018-08-12-vae/#reparameterization-trick">reparameterization trick</a>에 의해 \(\mathbf{x}_0=\frac{1}{\sqrt{\bar\alpha_t}}(\mathbf{x}_t - \sqrt{1-\bar\alpha_t}\epsilon_t)\) 로 표현할 수 있고, 이를 위 식에 대입해</p>\[\begin{align} \tilde\mu_t = \frac{1}{\sqrt{\alpha_t}}(\mathbf{x}_t - \frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon_t), \\ \text{where } \epsilon_t, \epsilon_{t-1}, ... \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \end{align}\]<p>위와 같이 평균을 도출할 수 있습니다.</p><p>이제 다 왔습니다. 주어진 입력 \(\mathbf{x}_t\)의 분포를 알아냈으니 우리의 모델이 이 \(\mathbf{x}_t\)를 모방할 수 있게 차이를 줄이는 방향으로 학습할 수 있습니다. 위에서 KL Divergence로 표현된 $L_{t-1}$를 아래와 같이 쓸 수 있는데요.</p>\[\begin{align} L_{t-1} = \mathbb{E}_q \left[ \frac{1}{2\sigma^2_t} \lVert \color{red}{\tilde\mu_t(\mathbf{x}_t, \mathbf{x}_0)} - \color{cyan}{\mu_\theta(\mathbf{x}_t, t)} \rVert ^2 \right] + C, \\ \text{where } C \text{ is a constant that does not depend on } \theta. \end{align}\]<p>우리가 학습하고자 하는 분포가 \(p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1} ; \mu_\theta(\mathbf{x}_t, t), \Sigma_\theta(\mathbf{x}_t, t))\) 임을 생각해보면 학습 단계에서 입력으로 주어지는 \(\mathbf{x}_t\)로부터 $\tilde\mu_t$를 추적할 수 있습니다. 따라서, $\mu_\theta$를 위 수식을 통해 $\tilde\mu_t$와의 차이를 줄이는 방향으로 학습할 수 있게됩니다.</p><p>이 수식을 우리가 알고있는 정보들을 조합해 학습에 간단한 형태로 바꾸어보겠습니다.</p>\[\begin{align} L_t &amp;= \mathbb{E}_{\mathbf{x}_0, \epsilon} \left[ \frac{1}{2\lVert\Sigma_\theta\rVert_2^2} \lVert \color{red}{\frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon_t \right) } - \color{cyan}{\frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon_\theta(\mathbf{x}_t, t) \right)} \rVert^2 \right] \\ &amp;= \mathbb{E}_{\mathbf{x}_0, \epsilon} \left[ \frac{(1-\alpha_t)^2}{2\alpha_t(1-\bar\alpha_t)\lVert\Sigma_\theta\rVert_2^2} \lVert \epsilon_t - \epsilon_\theta(\mathbf{x}_t, t) \rVert^2 \right] \\ &amp;= \mathbb{E}_{\mathbf{x}_0, \epsilon} \left[ \frac{(1-\alpha_t)^2}{2\alpha_t(1-\bar\alpha_t)\lVert\Sigma_\theta\rVert_2^2} \lVert \epsilon_t - \epsilon_\theta(\sqrt{\bar\alpha_t}\mathbf{x}_0 + \sqrt{1-\bar\alpha_t}\epsilon_t, t) \rVert^2 \right] \end{align}\]<p>마지막으로 논문에서는 경험적인 결과로 계수를 제외해도 모델의 수렴에 상관없다고 언급합니다.</p>\[L_t^{\text{simple}} = \mathbb{E}_{t\sim[1,T], \mathbf{x}_0, \epsilon_t} \left[ \lVert \epsilon_t - \epsilon_\theta(\sqrt{\bar\alpha_t}\mathbf{x}_0 + \sqrt{1-\bar\alpha_t}\epsilon_t, t) \rVert \right]\]<p>최종적으로 정리된 학습 알고리즘을 논문에서와 같이 아래로 정리할 수 있습니다.</p><p><img data-proofer-ignore data-src="../../assets/img/DDPM/fig2.png" alt="image-description" /></p><p><br /></p><hr /><p><br /></p><h2 id="appendix-code-implementation">Appendix. Code Implementation</h2><p>코드 구현의 리뷰는 아래 repo를 참고 했습니다.</p><ul><li><a href="https://github.com/lucidrains/denoising-diffusion-pytorch">pytorch implementation</a></ul><h3 id="1-training-overview">1) Training Overview</h3><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">denoising_diffusion_pytorch</span> <span class="kn">import</span> <span class="n">Unet</span><span class="p">,</span> <span class="n">GaussianDiffusion</span><span class="p">,</span> <span class="n">Trainer</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Unet</span><span class="p">(</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">dim_mults</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
    <span class="n">flash_attn</span> <span class="o">=</span> <span class="bp">True</span>
<span class="p">)</span>

<span class="n">diffusion</span> <span class="o">=</span> <span class="n">GaussianDiffusion</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">image_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">timesteps</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>           <span class="c1"># number of steps
</span>    <span class="n">sampling_timesteps</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>  <span class="c1"># number of sampling timesteps (using ddim for faster inference [see citation for ddim paper])
</span>    <span class="n">objective</span> <span class="o">=</span> <span class="s">'pred_noise'</span>
<span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">diffusion</span><span class="p">,</span>
    <span class="s">'path/to/your/images'</span><span class="p">,</span>
    <span class="n">train_batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">train_lr</span> <span class="o">=</span> <span class="mf">8e-5</span><span class="p">,</span>
    <span class="n">train_num_steps</span> <span class="o">=</span> <span class="mi">700000</span><span class="p">,</span>         <span class="c1"># total training steps
</span>    <span class="n">gradient_accumulate_every</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>    <span class="c1"># gradient accumulation steps
</span>    <span class="n">ema_decay</span> <span class="o">=</span> <span class="mf">0.995</span><span class="p">,</span>                <span class="c1"># exponential moving average decay
</span>    <span class="n">amp</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>                       <span class="c1"># turn on mixed precision
</span>    <span class="n">calculate_fid</span> <span class="o">=</span> <span class="bp">True</span>              <span class="c1"># whether to calculate fid during training
</span><span class="p">)</span>

<span class="n">trainer</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
</pre></table></code></div></div><h3 id="2-gaussian-diffusion---training">2) Gaussian Diffusion - Training</h3><h4 id="2-1-initialize">2-1) initialize</h4><p>먼저 생성자에서 논문에서 정의한 loss term과 sampling의 계산을 위한 여러 사전정보들을 초기화합니다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="c1"># denoising_diffusion_pytorch/denoising_diffusion_pytorch.py
# L421
</span><span class="k">def</span> <span class="nf">linear_beta_schedule</span><span class="p">(</span><span class="n">timesteps</span><span class="p">):</span>
    <span class="s">"""
    linear schedule, proposed in original ddpm paper
    """</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="mi">1000</span> <span class="o">/</span> <span class="n">timesteps</span>
    <span class="n">beta_start</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="mf">0.0001</span>
    <span class="n">beta_end</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="mf">0.02</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">beta_start</span><span class="p">,</span> <span class="n">beta_end</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">float64</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
</pre><td class="rouge-code"><pre><span class="c1"># denoising_diffusion_pytorch/denoising_diffusion_pytorch.py
# L457
</span><span class="k">class</span> <span class="nc">GaussianDiffusion</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="p">...</span>
        <span class="c1"># L489
</span>        <span class="k">if</span> <span class="n">beta_schedule</span> <span class="o">==</span> <span class="s">'linear'</span><span class="p">:</span>
            <span class="n">beta_schedule_fn</span> <span class="o">=</span> <span class="n">linear_beta_schedule</span>
        <span class="k">elif</span> <span class="n">beta_schedule</span> <span class="o">==</span> <span class="s">'cosine'</span><span class="p">:</span>
            <span class="n">beta_schedule_fn</span> <span class="o">=</span> <span class="n">cosine_beta_schedule</span>
        <span class="k">elif</span> <span class="n">beta_schedule</span> <span class="o">==</span> <span class="s">'sigmoid'</span><span class="p">:</span>
            <span class="n">beta_schedule_fn</span> <span class="o">=</span> <span class="n">sigmoid_beta_schedule</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s">'unknown beta schedule </span><span class="si">{</span><span class="n">beta_schedule</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>

        <span class="n">betas</span> <span class="o">=</span> <span class="n">beta_schedule_fn</span><span class="p">(</span><span class="n">timesteps</span><span class="p">,</span> <span class="o">**</span><span class="n">schedule_fn_kwargs</span><span class="p">)</span>

        <span class="n">alphas</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">betas</span>
        <span class="n">alphas_cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cumprod</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">alphas_cumprod_prev</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">pad</span><span class="p">(</span><span class="n">alphas_cumprod</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">value</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">)</span>

        <span class="n">timesteps</span><span class="p">,</span> <span class="o">=</span> <span class="n">betas</span><span class="p">.</span><span class="n">shape</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_timesteps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span>

        <span class="c1"># sampling related parameters
</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">sampling_timesteps</span> <span class="o">=</span> <span class="n">default</span><span class="p">(</span><span class="n">sampling_timesteps</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">)</span> <span class="c1"># default num sampling timesteps to number of timesteps at training
</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="p">.</span><span class="n">sampling_timesteps</span> <span class="o">&lt;=</span> <span class="n">timesteps</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">is_ddim_sampling</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">sampling_timesteps</span> <span class="o">&lt;</span> <span class="n">timesteps</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ddim_sampling_eta</span> <span class="o">=</span> <span class="n">ddim_sampling_eta</span>

        <span class="c1"># helper function to register buffer from float64 to float32
</span>
        <span class="n">register_buffer</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">name</span><span class="p">,</span> <span class="n">val</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">val</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">))</span>

        <span class="n">register_buffer</span><span class="p">(</span><span class="s">'betas'</span><span class="p">,</span> <span class="n">betas</span><span class="p">)</span>
        <span class="n">register_buffer</span><span class="p">(</span><span class="s">'alphas_cumprod'</span><span class="p">,</span> <span class="n">alphas_cumprod</span><span class="p">)</span>
        <span class="n">register_buffer</span><span class="p">(</span><span class="s">'alphas_cumprod_prev'</span><span class="p">,</span> <span class="n">alphas_cumprod_prev</span><span class="p">)</span>

        <span class="c1"># calculations for diffusion q(x_t | x_{t-1}) and others
</span>
        <span class="n">register_buffer</span><span class="p">(</span><span class="s">'sqrt_alphas_cumprod'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">alphas_cumprod</span><span class="p">))</span>
        <span class="n">register_buffer</span><span class="p">(</span><span class="s">'sqrt_one_minus_alphas_cumprod'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">alphas_cumprod</span><span class="p">))</span>
        <span class="n">register_buffer</span><span class="p">(</span><span class="s">'log_one_minus_alphas_cumprod'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">alphas_cumprod</span><span class="p">))</span>
        <span class="n">register_buffer</span><span class="p">(</span><span class="s">'sqrt_recip_alphas_cumprod'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">alphas_cumprod</span><span class="p">))</span>
        <span class="n">register_buffer</span><span class="p">(</span><span class="s">'sqrt_recipm1_alphas_cumprod'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">alphas_cumprod</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>

        <span class="c1"># calculations for posterior q(x_{t-1} | x_t, x_0)
</span>
        <span class="n">posterior_variance</span> <span class="o">=</span> <span class="n">betas</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">alphas_cumprod_prev</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">alphas_cumprod</span><span class="p">)</span>

        <span class="c1"># above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)
</span>
        <span class="n">register_buffer</span><span class="p">(</span><span class="s">'posterior_variance'</span><span class="p">,</span> <span class="n">posterior_variance</span><span class="p">)</span>

        <span class="c1"># below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain
</span>
        <span class="n">register_buffer</span><span class="p">(</span><span class="s">'posterior_log_variance_clipped'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">posterior_variance</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span> <span class="o">=</span><span class="mf">1e-20</span><span class="p">)))</span>
        <span class="n">register_buffer</span><span class="p">(</span><span class="s">'posterior_mean_coef1'</span><span class="p">,</span> <span class="n">betas</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">alphas_cumprod_prev</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">alphas_cumprod</span><span class="p">))</span>
        <span class="n">register_buffer</span><span class="p">(</span><span class="s">'posterior_mean_coef2'</span><span class="p">,</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">alphas_cumprod_prev</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">alphas</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">alphas_cumprod</span><span class="p">))</span>
</pre></table></code></div></div><ul><li><code class="language-plaintext highlighter-rouge">betas</code> = $\beta_t$<ul><li><code class="language-plaintext highlighter-rouge">alphas</code> = $\alpha_t = 1 - \beta_t$</ul><li><code class="language-plaintext highlighter-rouge">alphas_cumprod</code> = $\bar\alpha_t = \prod_{i=1}^t \alpha_i$<li><p><code class="language-plaintext highlighter-rouge">alphas_cumprod_prev</code> = $\bar\alpha_{t-1} = \prod_{i=1}^{t-1} \alpha_i$</p><li><code class="language-plaintext highlighter-rouge">posterior_variance</code> = $\tilde\beta_t = \frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}\cdot\beta_t$<li>posterior_mean<ul><li><code class="language-plaintext highlighter-rouge">posterior_mean_coef1</code> = $\frac{\sqrt{\bar\alpha_{t-1}\beta_t}}{1-\bar\alpha_t}$<li><code class="language-plaintext highlighter-rouge">posterior_mean_coef2</code> = $\frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})}{1-\bar\alpha_t}$</ul></ul><p>정의는 <a href="./#reverse-process-and-loss">본문</a>에서 참고</p><h4 id="2-2-forward-function">2-2) forward function</h4><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="c1"># L457
</span><span class="k">class</span> <span class="nc">GaussianDiffusion</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="p">...</span>
    <span class="c1"># L796
</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">img_size</span><span class="p">,</span> <span class="o">=</span> <span class="o">*</span><span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">img</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">image_size</span>
        <span class="k">assert</span> <span class="n">h</span> <span class="o">==</span> <span class="n">img_size</span> <span class="ow">and</span> <span class="n">w</span> <span class="o">==</span> <span class="n">img_size</span><span class="p">,</span> <span class="sa">f</span><span class="s">'height and width of image must be </span><span class="si">{</span><span class="n">img_size</span><span class="si">}</span><span class="s">'</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_timesteps</span><span class="p">,</span> <span class="p">(</span><span class="n">b</span><span class="p">,),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">).</span><span class="nb">long</span><span class="p">()</span>

        <span class="n">img</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">p_losses</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></table></code></div></div><p>diffusion의 학습을 시작하면 <code class="language-plaintext highlighter-rouge">GaussianDiffusion</code> 인스턴스의 <code class="language-plaintext highlighter-rouge">forward</code> method가 실행됩니다. 여기에서 diffusion process의 time step $t$를 샘플링하게 됩니다. 이 경우엔 <code class="language-plaintext highlighter-rouge">self.num_timesteps=100</code>로 설정했고, batch_size는 <code class="language-plaintext highlighter-rouge">32</code>로, 총 32개의 [0, 99] 범위의 랜덤한 $t$를 생성할 것입니다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="n">t</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span> <span class="mi">6</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="mi">76</span><span class="p">,</span> <span class="mi">86</span><span class="p">,</span> <span class="mi">47</span><span class="p">,</span> <span class="mi">77</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> 
            <span class="mi">11</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">59</span><span class="p">,</span> <span class="mi">59</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">46</span><span class="p">,</span> <span class="mi">67</span><span class="p">,</span> <span class="mi">55</span><span class="p">,</span> 
            <span class="mi">4</span><span class="p">,</span> <span class="mi">71</span><span class="p">,</span> <span class="mi">43</span><span class="p">,</span> <span class="mi">76</span><span class="p">,</span> <span class="mi">88</span><span class="p">,</span> <span class="mi">62</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> 
            <span class="mi">93</span><span class="p">,</span> <span class="mi">29</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">82</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">46</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">63</span><span class="p">],</span>
            <span class="n">device</span><span class="o">=</span><span class="s">'cuda:0'</span><span class="p">)</span>
</pre></table></code></div></div><h4 id="2-3-method---p_losses">2-3) method - p_losses</h4><p>이후 발전된 형태의 수식 구현코드들이 여럿 포함되어 있지만, 이를 제외하고 원조 DDPM의 구현코드(<code class="language-plaintext highlighter-rouge">self.objective == 'pred_noise'</code>)만 표현하면 아래와 같습니다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
</pre><td class="rouge-code"><pre><span class="c1"># L457
</span><span class="k">class</span> <span class="nc">GaussianDiffusion</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="p">...</span>
    <span class="c1"># L741
</span>    <span class="k">def</span> <span class="nf">q_sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_start</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">default</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x_start</span><span class="p">))</span>

        <span class="k">return</span> <span class="p">(</span>
            <span class="n">extract</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">sqrt_alphas_cumprod</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_start</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_start</span> <span class="o">+</span>
            <span class="n">extract</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">sqrt_one_minus_alphas_cumprod</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_start</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">noise</span>
        <span class="p">)</span>

    <span class="p">...</span>

    <span class="c1"># L749
</span>    <span class="k">def</span> <span class="nf">p_losses</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_start</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">offset_noise_strength</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x_start</span><span class="p">.</span><span class="n">shape</span>

        <span class="n">noise</span> <span class="o">=</span> <span class="n">default</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x_start</span><span class="p">))</span>

        <span class="c1"># noise sample
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">q_sample</span><span class="p">(</span><span class="n">x_start</span> <span class="o">=</span> <span class="n">x_start</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise</span> <span class="o">=</span> <span class="n">noise</span><span class="p">)</span>

        <span class="c1"># predict and take gradient step
</span>        <span class="n">model_out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

        <span class="n">target</span> <span class="o">=</span> <span class="n">noise</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">model_out</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span> <span class="o">=</span> <span class="s">'none'</span><span class="p">)</span>

    <span class="p">...</span>
</pre></table></code></div></div><p>먼저, <code class="language-plaintext highlighter-rouge">noise</code>와 <code class="language-plaintext highlighter-rouge">x_start</code>($\mathbf{x}_0$)를 통해 $t$ 시점에서의 posterior를 <code class="language-plaintext highlighter-rouge">self.q_sample</code> method를 통해 샘플링합니다.</p><p>잘 살펴보면 <code class="language-plaintext highlighter-rouge">self.q_sample</code> method의 정의는 <a href="./#reverse-process-and-loss">본문</a>의 <em>Algorithm 1</em>의 다섯번째 줄 $\epsilon_\theta(\sqrt{\bar\alpha_t}\mathbf{x}_0 + \sqrt{1-\bar\alpha_t}\epsilon, t)$의 수식과 같다는 것을 알 수 있습니다.</p><p>마지막으로 샘플링한 것을 <code class="language-plaintext highlighter-rouge">noise</code>($\mathbf{\epsilon}$)와의 MSE(Mean Squared Error)를 구하는 것을 loss로 정의할 수 있습니다.</p><h3 id="3-generating-overview">3) Generating Overview</h3><p>생성 과정은 아래의 예시와 같이 <code class="language-plaintext highlighter-rouge">self.sample</code> method를 수행하며 진행됩니다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="n">sampled_images</span> <span class="o">=</span> <span class="n">diffusion</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">sampled_images</span><span class="p">.</span><span class="n">shape</span> <span class="c1"># (4, 3, 32, 32)
</span></pre></table></code></div></div><h3 id="4-gaussian-diffusion---generating">4) Gaussian Diffusion - Generating</h3><h4 id="4-1-method---sample">4-1) method - sample</h4><p>발전된 형태인 DDIM의 구현(<code class="language-plaintext highlighter-rouge">self.ddim_sample</code>)을 제외하고, 구현코드를 보면 $T$ (<code class="language-plaintext highlighter-rouge">self.num_timesteps</code>)부터 0까지 $t$(<code class="language-plaintext highlighter-rouge">t</code>)를 차례차례 밟아가며 <code class="language-plaintext highlighter-rouge">self.p_sample</code> method를 수행하는 구조를 띄고 있습니다.</p><p>따라서 <code class="language-plaintext highlighter-rouge">self.p_sample</code> 메서드의 인자 <code class="language-plaintext highlighter-rouge">t</code>는 99부터 시작해 0까지 총 100($=T$)번 실행되며, 100번 이미지를 생성해낼 겁니다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
</pre><td class="rouge-code"><pre><span class="c1"># L457
</span><span class="k">class</span> <span class="nc">GaussianDiffusion</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="p">...</span>
    <span class="c1"># L654
</span>    <span class="k">def</span> <span class="nf">p_sample_loop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">return_all_timesteps</span> <span class="o">=</span> <span class="bp">False</span><span class="p">):</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">device</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">device</span>

        <span class="n">img</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">)</span>
        <span class="n">imgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">img</span><span class="p">]</span>

        <span class="n">x_start</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_timesteps</span><span class="p">)),</span> <span class="n">desc</span> <span class="o">=</span> <span class="s">'sampling loop time step'</span><span class="p">,</span> <span class="n">total</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_timesteps</span><span class="p">):</span>
            <span class="n">self_cond</span> <span class="o">=</span> <span class="n">x_start</span> <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">self_condition</span> <span class="k">else</span> <span class="bp">None</span>
            <span class="n">img</span><span class="p">,</span> <span class="n">x_start</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">p_sample</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">self_cond</span><span class="p">)</span>
            <span class="n">imgs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>

        <span class="n">ret</span> <span class="o">=</span> <span class="n">img</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">return_all_timesteps</span> <span class="k">else</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">imgs</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">ret</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">unnormalize</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ret</span>

    <span class="p">...</span>

    <span class="c1"># L715
</span>    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">return_all_timesteps</span> <span class="o">=</span> <span class="bp">False</span><span class="p">):</span>
        <span class="n">image_size</span><span class="p">,</span> <span class="n">channels</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">image_size</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">channels</span>
        <span class="n">sample_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">p_sample_loop</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">.</span><span class="n">is_ddim_sampling</span> <span class="k">else</span> <span class="bp">self</span><span class="p">.</span><span class="n">ddim_sample</span>
        <span class="k">return</span> <span class="n">sample_fn</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">image_size</span><span class="p">,</span> <span class="n">image_size</span><span class="p">),</span> <span class="n">return_all_timesteps</span> <span class="o">=</span> <span class="n">return_all_timesteps</span><span class="p">)</span>
</pre></table></code></div></div><p>이 과정 역시 <a href="./#reverse-process-and-loss">본문</a>의 <em>Algorithm 2</em>의 전체적인 프로세스 정의와 같음을 확인할 수 있었습니다.</p><p>그렇다면 <code class="language-plaintext highlighter-rouge">self.p_sample</code> method의 정의를 살펴보겠습니다.</p><h4 id="4-2-method---p_sample">4-2) method - p_sample</h4><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
</pre><td class="rouge-code"><pre><span class="c1"># L35
</span><span class="n">ModelPrediction</span> <span class="o">=</span>  <span class="n">namedtuple</span><span class="p">(</span><span class="s">'ModelPrediction'</span><span class="p">,</span> <span class="p">[</span><span class="s">'pred_noise'</span><span class="p">,</span> <span class="s">'pred_x_start'</span><span class="p">])</span>

<span class="p">...</span>

<span class="c1"># L457
</span><span class="k">class</span> <span class="nc">GaussianDiffusion</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="p">...</span>

    <span class="c1"># L600
</span>    <span class="k">def</span> <span class="nf">q_posterior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_start</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">posterior_mean</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">extract</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">posterior_mean_coef1</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_start</span> <span class="o">+</span>
            <span class="n">extract</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">posterior_mean_coef2</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_t</span>
        <span class="p">)</span>
        <span class="n">posterior_variance</span> <span class="o">=</span> <span class="n">extract</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">posterior_variance</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">posterior_log_variance_clipped</span> <span class="o">=</span> <span class="n">extract</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">posterior_log_variance_clipped</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">posterior_mean</span><span class="p">,</span> <span class="n">posterior_variance</span><span class="p">,</span> <span class="n">posterior_log_variance_clipped</span>

    <span class="p">...</span>

    <span class="c1"># L609
</span>    <span class="k">def</span> <span class="nf">model_predictions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_self_cond</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">clip_x_start</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">rederive_pred_noise</span> <span class="o">=</span> <span class="bp">False</span><span class="p">):</span>
        <span class="n">model_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_self_cond</span><span class="p">)</span>
        <span class="n">maybe_clip</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">,</span> <span class="nb">min</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="nb">max</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">)</span> <span class="k">if</span> <span class="n">clip_x_start</span> <span class="k">else</span> <span class="n">identity</span>

        <span class="n">pred_noise</span> <span class="o">=</span> <span class="n">model_output</span>
        <span class="n">x_start</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">predict_start_from_noise</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">pred_noise</span><span class="p">)</span>
        <span class="n">x_start</span> <span class="o">=</span> <span class="n">maybe_clip</span><span class="p">(</span><span class="n">x_start</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">clip_x_start</span> <span class="ow">and</span> <span class="n">rederive_pred_noise</span><span class="p">:</span>
            <span class="n">pred_noise</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">predict_noise_from_start</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_start</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">ModelPrediction</span><span class="p">(</span><span class="n">pred_noise</span><span class="p">,</span> <span class="n">x_start</span><span class="p">)</span>
    

    <span class="p">...</span>
    
    <span class="c1"># L634
</span>    <span class="k">def</span> <span class="nf">p_mean_variance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_self_cond</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">clip_denoised</span> <span class="o">=</span> <span class="bp">True</span><span class="p">):</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model_predictions</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_self_cond</span><span class="p">)</span>
        <span class="n">x_start</span> <span class="o">=</span> <span class="n">preds</span><span class="p">.</span><span class="n">pred_x_start</span>

        <span class="k">if</span> <span class="n">clip_denoised</span><span class="p">:</span>
            <span class="n">x_start</span><span class="p">.</span><span class="n">clamp_</span><span class="p">(</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>

        <span class="n">model_mean</span><span class="p">,</span> <span class="n">posterior_variance</span><span class="p">,</span> <span class="n">posterior_log_variance</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">q_posterior</span><span class="p">(</span><span class="n">x_start</span> <span class="o">=</span> <span class="n">x_start</span><span class="p">,</span> <span class="n">x_t</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">model_mean</span><span class="p">,</span> <span class="n">posterior_variance</span><span class="p">,</span> <span class="n">posterior_log_variance</span><span class="p">,</span> <span class="n">x_start</span>

    <span class="p">...</span>

    <span class="c1"># L645
</span>    <span class="k">def</span> <span class="nf">p_sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">x_self_cond</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
        <span class="n">b</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span><span class="p">,</span> <span class="n">device</span> <span class="o">=</span> <span class="o">*</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">device</span>
        <span class="n">batched_times</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">full</span><span class="p">((</span><span class="n">b</span><span class="p">,),</span> <span class="n">t</span><span class="p">,</span> <span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>
        <span class="n">model_mean</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">model_log_variance</span><span class="p">,</span> <span class="n">x_start</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">p_mean_variance</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">batched_times</span><span class="p">,</span> <span class="n">x_self_cond</span> <span class="o">=</span> <span class="n">x_self_cond</span><span class="p">,</span> <span class="n">clip_denoised</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">if</span> <span class="n">t</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mf">0.</span> <span class="c1"># no noise if t == 0
</span>        <span class="n">pred_img</span> <span class="o">=</span> <span class="n">model_mean</span> <span class="o">+</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">model_log_variance</span><span class="p">).</span><span class="n">exp</span><span class="p">()</span> <span class="o">*</span> <span class="n">noise</span>
        <span class="k">return</span> <span class="n">pred_img</span><span class="p">,</span> <span class="n">x_start</span>
</pre></table></code></div></div><p>먼저, <code class="language-plaintext highlighter-rouge">self.p_sample</code> method의 마지막 줄에서 <code class="language-plaintext highlighter-rouge">x_start</code> 변수는 역시 <a href="./#reverse-process-and-loss">본문</a>의 <em>Algorithm 2</em>의 의 것과 동일한 형태로 구현되어 있습니다. <code class="language-plaintext highlighter-rouge">x_start</code> 변수를 추적하다보면 아래의 <code class="language-plaintext highlighter-rouge">self.predict_start_from_noise</code> method에서 도출되는 것을 확인할 수 있는데요, 이 계산식이 <a href="./#reverse-process-and-loss">본문</a>의 <em>Algorithm 2</em>의 네 번째 줄의 것과 동일한 것을 확인할 수 있었습니다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span text-data=" Python "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="c1"># L457
</span><span class="k">class</span> <span class="nc">GaussianDiffusion</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="p">...</span>
    <span class="c1"># L576
</span>    <span class="k">def</span> <span class="nf">predict_start_from_noise</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">extract</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">sqrt_recip_alphas_cumprod</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_t</span> <span class="o">-</span>
            <span class="n">extract</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">sqrt_recipm1_alphas_cumprod</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">noise</span>
        <span class="p">)</span>
</pre></table></code></div></div><p>그렇다면 실제 생성될 이미지인 <code class="language-plaintext highlighter-rouge">pred_img</code> 변수는 어떻게 구성될까요?</p><p><code class="language-plaintext highlighter-rouge">pred_img</code>를 구성하는 <code class="language-plaintext highlighter-rouge">model_mean</code>과 <code class="language-plaintext highlighter-rouge">model_log_variance</code> 변수를 추적하다보면, 두 변수 모두 <code class="language-plaintext highlighter-rouge">self.q_posterior</code> method에서 도출되는 것을 확인할 수 있습니다.</p><p>그 계산 결과는 diffusion model의 reverse process를 지속적으로 학습하며 추적이 가능해진 <code class="language-plaintext highlighter-rouge">posterior</code> 분포를 구성하는 과정으로 보여집니다. 다시 말해, 본문에서 언급한 아래의 평균과 분산으로 분포를 구성해 이미지를 생성함을 알 수 있죠.</p><ul><li><code class="language-plaintext highlighter-rouge">posterior_variance</code> = $\tilde\beta_t = \frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}\cdot\beta_t$<li>posterior_mean<ul><li><code class="language-plaintext highlighter-rouge">posterior_mean_coef1</code> = $\frac{\sqrt{\bar\alpha_{t-1}\beta_t}}{1-\bar\alpha_t}$<li><code class="language-plaintext highlighter-rouge">posterior_mean_coef2</code> = $\frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})}{1-\bar\alpha_t}$</ul></ul>\[\begin{align} \tilde\beta_t &amp;= \frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t} \cdot \beta_t \\ \tilde\mu_t(\mathbf{x}_t, \mathbf{x}_0) &amp;= \frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})}{1-\bar\alpha_t}\mathbf{x}_t + \frac{\sqrt{\bar\alpha_{t-1}}\beta_t}{1-\bar\alpha_t}\mathbf{x}_0 \end{align}\]<p><br /></p><hr /><p><br /></p><blockquote><p>References</p><ul><li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models">https://lilianweng.github.io/posts/2021-07-11-diffusion-models</a><li><a href="https://happy-jihye.github.io/diffusion/diffusion-1/">https://happy-jihye.github.io/diffusion/diffusion-1/</a><li><a href="https://process-mining.tistory.com/188">https://process-mining.tistory.com/188</a><li><a href="https://github.com/lucidrains/denoising-diffusion-pytorch">pytorch implementation</a></ul></blockquote></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/paper-review/'>paper-review</a>, <a href='/categories/computer-vision/'>Computer Vision</a>, <a href='/categories/diffusion/'>Diffusion</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/deep-learning/" class="post-tag no-text-decoration" >deep_learning</a> <a href="/tags/diffusion/" class="post-tag no-text-decoration" >diffusion</a> <a href="/tags/generative-model/" class="post-tag no-text-decoration" >generative_model</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=[paper-review] Denoising diffusion probabilistic models - good-riverdeer&url=https://good-riverdeer.github.io/posts/DDPM/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=[paper-review] Denoising diffusion probabilistic models - good-riverdeer&u=https://good-riverdeer.github.io/posts/DDPM/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=[paper-review] Denoising diffusion probabilistic models - good-riverdeer&url=https://good-riverdeer.github.io/posts/DDPM/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/DDPM/">[paper-review] Denoising diffusion probabilistic models</a><li><a href="/posts/SLidR/">[paper-review] Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data</a><li><a href="/posts/TEBNER/">[paper-review] TEBNER: Domain Specific Named Entity Recognition with Type Expanded Boundary-aware Network.</a><li><a href="/posts/EfficientNet/">[paper-review] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a><li><a href="/posts/Vision_Transformer/">[paper-review] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/deep-learning/">deep_learning</a> <a class="post-tag" href="/tags/transformer/">transformer</a> <a class="post-tag" href="/tags/autonomous-driving/">autonomous_driving</a> <a class="post-tag" href="/tags/distillation/">distillation</a> <a class="post-tag" href="/tags/document-understanding/">document_understanding</a> <a class="post-tag" href="/tags/lidar/">lidar</a> <a class="post-tag" href="/tags/multimodal/">multimodal</a> <a class="post-tag" href="/tags/named-entity-recognition/">Named_Entity_Recognition</a> <a class="post-tag" href="/tags/ner/">NER</a> <a class="post-tag" href="/tags/ocr/">ocr</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/EfficientNet/"><div class="card-body"> <span class="timeago small" >Oct 7, 2021<i class="unloaded">2021-10-07T20:48:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[paper-review] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</h3><div class="text-muted small"><p> Tan, M., &amp; Le, Q. (2019, May). Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning (pp. 6105-6114). PMLR. 개인적인 논문해석을 포함하고 ...</p></div></div></a></div><div class="card"> <a href="/posts/Mask_TextSpotter/"><div class="card-body"> <span class="timeago small" >Nov 22, 2021<i class="unloaded">2021-11-22T18:20:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[paper-review] Mask TextSpotter v3: Segmentation Proposal Network for Robust Scene Text Spotting</h3><div class="text-muted small"><p> Liao, M., Pang, G., Huang, J., Hassner, T., &amp; Bai, X. (2020). Mask textspotter v3: Segmentation proposal network for robust scene text spotting. In Computer Vision–ECCV 2020: 16th European Conf...</p></div></div></a></div><div class="card"> <a href="/posts/Learning_to_Understand_Traffic_Signs/"><div class="card-body"> <span class="timeago small" >Dec 15, 2021<i class="unloaded">2021-12-15T12:10:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[paper-review] Learning to Understand Traffic Signs</h3><div class="text-muted small"><p> Guo, Y., Feng, W., Yin, F., Xue, T., Mei, S., &amp; Liu, C. L. (2021, October). Learning to Understand Traffic Signs. In Proceedings of the 29th ACM International Conference on Multimedia (pp. 2076...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/SLidR/" class="btn btn-outline-primary" prompt="Older"><p>[paper-review] Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data</p></a> <a href="/posts/ScribbleKitti/" class="btn btn-outline-primary" prompt="Newer"><p>[paper-review] Scribble-Supervised LiDAR Semantic Segmentation</p></a></div><div id="disqus_thread" class="pt-2 pb-2"><p class="text-center text-muted small"> Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script type="text/javascript"> var disqus_config = function () { this.page.url = 'https://good-riverdeer.github.io/posts/DDPM/'; this.page.identifier = '/posts/DDPM/'; }; /* Lazy loading */ var disqus_observer = new IntersectionObserver(function (entries) { if(entries[0].isIntersecting) { (function () { var d = document, s = d.createElement('script'); s.src = 'https://.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); disqus_observer.disconnect(); } }, { threshold: [0] }); disqus_observer.observe(document.querySelector('#disqus_thread')); /* Auto switch theme */ function reloadDisqus() { /* Disqus hasn't been loaded */ if (typeof DISQUS === "undefined") { return; } if (document.readyState == 'complete') { DISQUS.reset({ reload: true, config: disqus_config }); } } const modeToggle = document.querySelector(".mode-toggle"); if (modeToggle !== null) { modeToggle.addEventListener('click', reloadDisqus); window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', reloadDisqus); } </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://github.com/good-riverdeer">riverdeer</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/deep-learning/">deep_learning</a> <a class="post-tag" href="/tags/transformer/">transformer</a> <a class="post-tag" href="/tags/autonomous-driving/">autonomous_driving</a> <a class="post-tag" href="/tags/distillation/">distillation</a> <a class="post-tag" href="/tags/document-understanding/">document_understanding</a> <a class="post-tag" href="/tags/lidar/">lidar</a> <a class="post-tag" href="/tags/multimodal/">multimodal</a> <a class="post-tag" href="/tags/named-entity-recognition/">Named_Entity_Recognition</a> <a class="post-tag" href="/tags/ner/">NER</a> <a class="post-tag" href="/tags/ocr/">ocr</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
