<!DOCTYPE html><html lang="ko" mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="[paper-review] Scribble-Supervised LiDAR Semantic Segmentation" /><meta property="og:locale" content="ko" /><meta name="description" content="Unal, O., Dai, D., &amp; Van Gool, L. (2022). Scribble-supervised lidar semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 2697-2707). official implementation" /><meta property="og:description" content="Unal, O., Dai, D., &amp; Van Gool, L. (2022). Scribble-supervised lidar semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 2697-2707). official implementation" /><link rel="canonical" href="https://good-riverdeer.github.io/posts/ScribbleKitti/" /><meta property="og:url" content="https://good-riverdeer.github.io/posts/ScribbleKitti/" /><meta property="og:site_name" content="good-riverdeer" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-08-07T16:30:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[paper-review] Scribble-Supervised LiDAR Semantic Segmentation" /><meta name="twitter:site" content="@twitter_username" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"description":"Unal, O., Dai, D., &amp; Van Gool, L. (2022). Scribble-supervised lidar semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 2697-2707). official implementation","url":"https://good-riverdeer.github.io/posts/ScribbleKitti/","@type":"BlogPosting","headline":"[paper-review] Scribble-Supervised LiDAR Semantic Segmentation","dateModified":"2023-08-07T16:30:00+09:00","datePublished":"2023-08-07T16:30:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://good-riverdeer.github.io/posts/ScribbleKitti/"},"@context":"https://schema.org"}</script><title>[paper-review] Scribble-Supervised LiDAR Semantic Segmentation | good-riverdeer</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="good-riverdeer"><meta name="application-name" content="good-riverdeer"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/school.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">good-riverdeer</a></div><div class="site-subtitle font-italic">딥러닝을 공부하는</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/good-riverdeer" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['riverdeer.youn','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>[paper-review] Scribble-Supervised LiDAR Semantic Segmentation</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>[paper-review] Scribble-Supervised LiDAR Semantic Segmentation</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> riverdeer </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Mon, Aug 7, 2023, 4:30 PM +0900" >Aug 7<i class="unloaded">2023-08-07T16:30:00+09:00</i> </span></div><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="4196 words">23 min read</span></div></div><div class="post-content"><ul><li><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Unal_Scribble-Supervised_LiDAR_Semantic_Segmentation_CVPR_2022_paper.html">Unal, O., Dai, D., &amp; Van Gool, L. (2022). Scribble-supervised lidar semantic segmentation. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 2697-2707).</a><li><a href="https://github.com/ouenal/scribblekitti">official implementation</a></ul><p><em>개인적인 논문해석을 포함하고 있으며, 의역 및 오역이 남발할 수 있습니다. :)</em></p><p><br /></p><hr /><p><br /></p><h2 id="1-introduction">1. Introduction</h2><p>자율주행 및 LiDAR 데이터에 대한 연구가 활발해지고 있으나 3D point cloud에 semantic segmentation label을 할당하는 작업은 매우 비용이 많이 소모됩니다. 저자들은 weak annotation 방법 중 하나인, <strong><em>scribbles</em></strong>가 2D semantic segmentation 작업에도 효과적임을 입증했으며 이를 LiDAR 3D point cloud semantic segmentation에 사용할 것을 제안합니다.</p><p>이들은 3D semantic segmentation의 benchmark 데이터셋으로 많이 사용되는 <strong>SemanticKITTI</strong> 데이터셋 전체 labeled point 수의 8.06%에만 label을 할당해 <strong>ScribbleKITTI</strong>라는 이름의 benchmark 데이터셋을 공개합니다.</p><p><img data-proofer-ignore data-src="../../assets/img/ScribbleKitti/fig1.png" alt="image-description" /></p><p>또한 이 <strong><em>scribbles</em></strong>의 효과적인 학습을 위해 <strong>1) unlabeled points에 대하여 teacher 모델과 student 모델 간의 consistency loss</strong>를 도입하고 <strong>2) outdoor LiDAR 데이터에 적합한 self-training scheme</strong>을 도입하며 <strong>3) pyramid local semantic-context descriptor를 통한 augmentation으로 pseudo-label의 퀄리티를 향상시키는 방법</strong>을 사용합니다.</p><p>결과적으로는 기존 모든 point들에 label이 할당되어 있는 경우보다 8%만의 label을 활용해 상대적으로 95.7%에 해당하는 성능을 달성했습니다.</p><p><br /></p><hr /><p><br /></p><h2 id="4-scribble-supervised-lidar-segmentation">4. Scribble-Supervised LiDAR Segmentation</h2><h3 id="problem-definition">Problem definition</h3><p>논문에선 전체 LiDAR point cloud 집합을 $P = \set{p \mid p=(x, y, z, I) \in \mathbb{R}^4}$로 정의합니다. 여기서 $x, y, z$는 각 point의 위치좌표이고 $I$는 intensity를 나타냅니다.</p><p>이 집합 중 소수의 labeled points, $S \subseteq P$에 대한 loss는 기존 fully supervised task에서 주로 사용하는 $H$ (typically cross-entropy)를 사용합니다. 이를 수식으로 표현하면 아래와 같죠.</p>\[\min_\theta \sum^F_{f=1}\sum^{\lvert P_f \rvert}_{i=1} \mathbb{1}(p) H(\hat y_{f,i\mid\theta}, y_{f,i})\]<p>$\hat y_{f,i\mid\theta}$는 총 point cloud 프레임 $F$ 중 $f$번째 프레임 내 $i$번째 point $p_{f, i} \in P_f$에 대한 모델의 예측 값입니다. 이 모델은 $\theta$를 파라미터로 같습니다. 반대로, $y_{f,i}$는 ground truth label이겠죠.</p><p>위에서 제시한 baseline loss는 전체 데이터 point의 약 8%에 해당하는 point에 대한 학습이기 때문에 <strong>성능저하는 필연적</strong>이고 <strong>빈번하게 출현하지 않는 class에 대한 신뢰도는 제한된 supervision 정보로 인해 당연하게 떨어질 것</strong>입니다.</p><p>이후부턴 본 논문에서 제시하는 위와 같은 문제를 어떠한 방식으로 해결하는지를 제시하고 있습니다.</p><p><br /></p><hr /><p><br /></p><h3 id="4-1-partial-consistency-loss-with-mean-teacher">4-1 Partial Consistency Loss with Mean Teacher</h3><h4 id="mean-teacher">Mean Teacher</h4><p>논문에서는 unlabeled points에서 활용할 수 있는 weak supervision을 효과적으로 적용하기 위해 <strong>Mean teacher</strong> framework을 도입합니다.</p><p>mean teacher framework란? 각각 student (parametrized by $\theta$), teacher (parametrized by $\theta^{\text{EMA}}$)로 정의된 두 모델로써 구성됩니다. 이 때 teacher 모델의 파라미터는 student 모델의 것에 영향 받습니다. 학습 초기에는 거의 teacher 모델 본인의 파라미터로 모델 파라미터가 형성되지만, 점차 student 모델의 파라미터로 대체되죠. 이를 Exponential Moving Average (EMA)라고 말합니다. 말로 이해하려 했을 때 살짝 어렵지만, 수식으로 쉽게 이해할 수 있습니다. 학습이 진행되는 step을 $t$, teacher모델 파라미터가 student 모델의 것으로 대체되는 속도를 나타내는 smoothing coefficient를 $\alpha$라고 했을 때, 아래와 같이 나타냅니다.</p>\[\theta^{\text{EMA}}_t = \alpha \theta^{\text{EMA}}_{t-1} + (1-\alpha) \theta_t\]<p>저자들은 이를 통해 teacher model의 예측이 student 모델을 위한 weak supervision의 형태로 작용할 수 있다고 말하고 있습니다.</p><h4 id="partial-consistency-loss">Partial Consistency loss</h4><p>여기에 더불어 unlabeled points 집합 $U$에 대한 teacher model의 예측값, student model의 예측값 사이의 consistency loss를 도입합니다. 다른 consistency loss를 도입한 연구들과 다르게 unlabeled points에 대해서만 적용한다는 점을 강조하고 있습니다.</p><p>이를 통해 labeled points $S$엔 teacher model의 불확실성이 배제된 깔끔한 supervision을 얻어낼 수 있겠고, 반면 unlabeled points엔 더 정확한 예측을 수행할 수 있는 teacher model의 예측 값을 student model이 근사하는 방향으로 학습하게 될 것 입니다.</p>\[\min_\theta \sum^F_{f=1}\sum^{\lvert P_f \rvert}_{i=1} G_{i,f} = \begin{cases} H(\hat{y}_{f,i\mid\theta}, y_{f,i}) &amp; \text{if } p_{f,i} \in S \\ \log(\hat{y}_{f,i\mid\theta}) \hat{y}_{f,i\mid\theta^{EMA}} &amp; \text{if } p_{f,i} \in U \end{cases}\]<p>하지만 여전히 unlabeled points에 대한 supervision은 전적으로 teacher model의 성능에 제한될 수 있다는 점, 학습 도중 model의 softmax prediction만으로 labeling을 수행하는 <strong>soft pseudo-labeling</strong>만으로는 최대 확률 클래스 이외의 다른 클래스들에도 약간의 supervision이 섞이는 문제점을 지적할 수 있습니다.</p><p><br /></p><hr /><p><br /></p><h3 id="4-2-class-range-balanced-self-training-crb-st">4-2 Class-range-balanced Self-training (CRB-ST)</h3><p>4-1에서 언급한 것처럼 softmax prediction을 통해 pseudo-labeling을 수행했을 때 발생할 수 있는 불확실성을 배제하고 unlabeled points에 대한 확실한 예측 정보를 직접적으로 활용해야 합니다.</p><p>이를 위해 저자들은 pseudo-labeling을 통해 labeled data의 수를 늘립니다. 우선 모델의 예측 값을 통해 pseudo-labeled points 집합 $L$을 정의하고, 이 집합과 기존 scribble labeled points 집합 $S$와 함께 모델을 다시 학습하는데요.</p><p>기존 scribble-supervised semantic segmentation 연구들에서 이런식의 pseudo-labels 기반의 self-training이 효과적임을 입증했었습니다. 하지만 자율주행 차량의 연구를 위해 수집된 해당 segmentation class들은 필연적으로 특정 클래스에 적은 수의 샘플들이 수집될 수 밖에 없습니다 (long tailed 특징).</p><p>해당 문제를 해결하기 위해 이전 연구에선 전체 class distribution에 따라 pseudo-labeling에 대한 threshold를 class 별로 다르게 설정하는 방식으로 접근했습니다. 이 방법이 2D segmentation에 효과적인 결과를 가져다 주었지만, <strong>3D LiDAR 데이터에는 또다른 문제</strong>가 존재함을 논문에서 지적하고 있습니다.</p><p>그 문제는 바로 LiDAR 센서의 특성으로 인해 <strong>특정 공간에 따라 포인트의 밀도가 다르게 분포한다는 점</strong>입니다. 이러한 특성을 가진 LiDAR 센서 데이터에 대해 전역적인 class balance를 고려하게 된다면, 센서와 가까운 영역, 즉 LiDAR 센서와 가까운 영역에 편향될 수 있습니다.</p><p>예를 들어, 일반적으로 차량 포인트들은 LiDAR 센서와 가까운 것에 반해, 수풀 포인트들은 LiDAR 센서와 거리가 멀리 떨어져 있다고 가정해봅시다. 이때 포인트의 class balance는 차량 클래스에 높은 분포를 가지고 반대로 수풀 클래스는 낮은 분포를 가질 것입니다. 왜냐하면 LiDAR 센서와의 거리가 멀면 멀수록 포집되는 포인트의 수가 적을 테니 말이죠. 이로 인해 모든 unlabeled point가 할당될 확률은 차량에 높게 할당될 것입니다. 분명, 센서와 거리가 멀리 떨어진 unlabeled point들은 마땅히 수풀 클래스에 할당될 확률이 높아야 하지만요.</p><p>이러한 점을 해결하기 위해 논문에서는 class balance를 전역적으로 살펴보는 대신, 구역을 나누어 class balance를 살펴봐야 한다고 말합니다. 즉, LiDAR 센서와의 거리가 먼 포인트들끼리 클래스 분포를 살펴봐야 하는 것이죠.</p><p>이를 기존 class balance pseudo-labeling 방법에서 확장해 class-range-balanced (CRB) pseudo-labeling이라고 명명하고 있습니다.</p><p><img data-proofer-ignore data-src="../../assets/img/ScribbleKitti/fig3.png" alt="image-description" /> <em>아래 이미지처럼 센서와 거리가 멀리 떨어진 포인트들 간의 클래스 분포를 통해 pseudo-label을 할당하기 때문에, 수풀(green-colored) 포인트가 할당될 수 있습니다.</em></p><p>위에서 서술한 내용을 알고리즘으로써 정리하면 다음과 같습니다.</p><p><img data-proofer-ignore data-src="../../assets/img/ScribbleKitti/fig4.png" alt="image-description" /> <em>위 알고리즘의 결과물로 각 cylindrical 구역 $r$에 대하여 각 클래스 $c$에 클래스가 할당될 임계값 $k^{(c,r)}$ 집합이 도출됩니다. 이후의 pseudo-labeling 과정에서 이 임계값을 사용할 것입니다.</em></p><p>여기서 또 주의해야할 점 한 가지는 알고리즘 14번째 line에 위치한 $\beta$입니다. 논문에서는 $\beta$를 두어 CRB 방법으로 pseudo-label을 할당할지, 그대로 unlabeled label로 남겨둘지 확률을 부여했습니다. 만약 $\beta=0.5$라면, 전체 unlabeled point 중 절반은 CRB 방법으로 pseudo-label이 부여되고 나머지 절반은 그대로 unlabeled point로 남겨질 겁니다.</p><p>이와 같은 방법을 채용한 구체적인 의도는 논문에서 언급하지 않고 있는 것 같아 넘어가도록 하겠습니다.</p><p>CRB를 적용한 최종 목적함수는 아래와 같습니다.</p>\[\min_{\theta, \hat{y}} \sum_{f=1}^F \sum_{i=1}^{\lvert P_f \rvert} \left[ G_{i,f} - \sum_{c=1}^C \sum_{r=1}^R F_{i,f,c,r} \right]\] \[G_{i,f} = \begin{cases} H(\hat{y}_{f,i\mid\theta}, y_{f,i}) &amp; \text{if } p_{f,i} \in S \cup L \\ \log(\hat{y}_{f,i\mid\theta}) \hat{y}_{f,i\mid\theta^{EMA}} &amp; \text{if } p_{f,i} \in U \backslash L \end{cases}\] \[F_{i,f,c,r} = \begin{cases} (\log(\hat{y}^{(c)}_{f,i\mid \theta^{\text{EMA}}}) + k^{(c,r)}) \hat{y}^{(c)}_{f,i}, &amp; \text{if } r=\lfloor \lVert (p_{x,y})_{f,i} \rVert / B \rfloor \\ 0, &amp; \text{otherwise} \end{cases}\]<p>여기서, negative log-threshold를 nonlinear integer optimization으로 다루기 위해, pseudo-label을 아래와 같이 정의합니다.</p>\[\hat{y}^{(c)*}_{f,i} = \begin{cases} 1, &amp; \text{if } c = \text{argmax }\hat{y}_{f,i \mid \theta^{\text{EMA}}}, \hat{y}_{f,i \mid \theta} \gt \exp(-k^{(c,r)}) \\ &amp; \text{with } r=\lfloor \lVert (p_{x,y})_{f,i} \rVert / B \rfloor \\ 0, &amp; \text{otherwise} \end{cases}\]<p>레이블이 할당되는 각 조건에 대해 살펴볼까요??</p><p>첫 번째 $\text{argmax }\hat{y}_{f,i\mid \theta^{\text{EMA}}}$는 teacher model의 예측 클래스가 $c$일 때를 의미합니다.</p><p>두 번째로 $\hat{y}_{f,i \mid \theta} \gt \exp\left(-k^{(c,r)}\right)$는 student model의 예측 probability가 CRB 과정에서 도출해낸 임계값보다 큰 경우를 의미합니다. 보통 데이터의 분포를 살펴보니 구역 $r$에선 클래스 $c$가 할당되려면 이 정도 probability는 갖춰야 한다는 걸 나타내는 거죠.</p><p><br /></p><hr /><p><br /></p><h3 id="4-3-pyramid-local-semantic-context-pls">4-3 Pyramid Local Semantic-context (PLS)</h3><p>self-training은 pseudo-label의 퀄리티에 성능이 크게 좌우됩니다. 더 좋은 퀄리티의 pseudo-label을 확보하기 위해서 scribble points에서 최대한의 feature를 포착하기 위한 방법이 필요합니다.</p><p>논문에서는 아래 두 가지 3D 공간에서의 semantic classes들의 분포에 대한 고찰을 이야기합니다.</p><ol><li>spatial smoothness constraint - 비슷한 공간 내의 포인트들은 같은 클래스를 가지고 있을 확률이 높습니다.<li>semantic pattern constraint - semantic class들은 서로서로 물리적 연관성을 가지고 있습니다. 예를 들어 차량은 노면이나 주차공간 등의 지면 위에 위치하는 경우가 대부분이고, 보행자 클래스 역시 도보나 건물 등의 주변에 위치해 있는 경우가 많습니다.</ol><p>위 특성을 반영하기 위해 저자들은 <strong>multiple size of bins in cylindrical 좌표</strong>를 채택합니다. 해당 좌표 feature의 특징은 cylinder3D에서 제안되었던 cylindrical 좌표 도입과 함께 <strong>1) 다양한 스케일의 좌표계 사용</strong>이 있습니다. 또한 해당 좌표계에서 histogram을 계산해 <strong>2) 가장 비율이 높은 class 정보를 학습 feature로 사용</strong>한다는 점이 있습니다.</p><p><img data-proofer-ignore data-src="../../assets/img/ScribbleKitti/fig2.png" alt="image-description" /> <em>좌측 그림에서와 같이 녹색 cylindrical bins에선 비교적 지역적인 feature를, 적색 cylindrical bins에선 비교적 전역적인 feature를 포착할 수 있을 것입니다.</em></p><p>논문에서 제안하는 Pyramid Local Semantic-context, PLS는 아래의 수식과 같이 정의합니다.</p>\[\text{PLS} = \left[ \mathbf{h}^1_i/\max(\mathbf{h}^1_i) , ... , \mathbf{h}^s_i / \max(\mathbf{h}^s_i) \right] \in \mathbb{R}^{sC}\]<p>여기서 각 $\mathbf{h}^s_i$는 각 cylindrical bin $b_i$에 대한 scribble label의 클래스 별 히스토그램을 나타냅니다.</p>\[\begin{aligned} \mathbf{h}_i &amp;= \left[ h^{(1)}_i, ..., h^{(C)}_i \right] \in \mathbb{R}^C \\ h^{(c)}_i &amp;= \text{#} \set{y_j = c \forall j \mid p_j \in b_i} \end{aligned}\]<p>위 PLS의 의미를 곱씹어보면 각 cylindrical 영역에 포인트가 위치하는 분포라고 해석할 수 있겠습니다.</p><p>위와 같이 구축해낸 PLS feature를 기존 global geometry 정보($x, y, z$, intensity)와 함께 구성되어 $P_{avg} = \set{ p \mid p=(x,y,z,I,PLS) \in \mathbb{R}^{4+sC}}$를 형성하고, 바로 이 것이 모델 입력으로 사용됩니다.</p><p>하지만, 학습이 완료된 후 pseudo-label을 inference에 사용할 수 없습니다. 그 이유는 scribble label 정보를 통해 위 PLS 정보를 확보할 수 없기 때문이죠.</p><p>따라서 논문에서도 학습 과정의 마지막인 <strong>distillation</strong> 과정에선 이 PLS feature를 학습에서 제외하고 있습니다.</p><p><br /></p><hr /><p><br /></p><h2 id="5-experiments">5. Experiments</h2><ul><li>약어 정리<ul><li>SS - scribble-supervised<li>FS - fully supervised<li>SS/FS - fully supervised 성능 대비 scribble-supervised 성능 비율</ul></ul><h3 id="5-1-results">5-1 Results</h3><p><img data-proofer-ignore data-src="../../assets/img/ScribbleKitti/fig5.png" alt="image-description" /> <em>세 가지 baseline 모델 (Cylinder3D, MinkowskiNet, SPVCNN) 모두 제안하는 방법을 사용했을 때 성능이 향상될 수 있음을 확인할 수 있습니다.</em></p><h3 id="5-2-ablation-studies">5-2 Ablation Studies</h3><h4 id="effects-of-network-components">Effects of Network Components</h4><p>여기에선 논문에서 제안하는 framework의 각 요소가 성능에 미치는 영향을 실험합니다. train set에서의 성능과 validation set에서의 성능을 함께 보여주는데, 논문에선 train set의 성능을 통해 pseudo-label의 퀄리티를 짐작할 수 있을 것이라고 말하고 있습니다.</p><p><img data-proofer-ignore data-src="../../assets/img/ScribbleKitti/fig6.png" alt="image-description" /></p><ul><li>2번째 train mIoU와 4번째 train mIoU를 비교했을 때,<ul><li>PLS 방법 사용 여부에 따라 8% 가량 크게 향상됨을 확인할 수 있습니다.<li>이를 통해 논문에서 제안하는 PLS가 pseudo-label을 잘 포착해낼 수 있음을 짐작할 수 있습니다.</ul><li>2번째 valid 성능과 3번째 valid 성능을 비교했을 때,<ul><li>CRB의 도입에 따라 약 2% 가량 향상됨을 확인할 수 있습니다.</ul></ul><h4 id="pseudo-label-filtering-for-self-training">Pseudo-label Filtering for Self-training</h4><p>CRB pseudo-labeling 모듈의 기여를 파악하기 위해 여러 pseudo-labeling 방법을 비교합니다.</p><ul><li>naive sampling<li>threshold-based sampling<li>class-balanced (CB) sampling<li>DARS<li>class-range-balanced (CRB) sampling</ul><p><img data-proofer-ignore data-src="../../assets/img/ScribbleKitti/fig7.png" alt="image-description" /></p><p>labeling 정확도를 통해 outdoor LiDAR 데이터의 long-tailed 특성을 극복하기 위한 CB와 DARS 방식이 pseudo-label의 퀄리티를 크게 커버하는 모습을 확인할 수 있습니다. 더불어 validation set에 대한 성능 향상도 threshold-based 방식에 비해 크게 향상시킴을 볼 수 있습니다.</p><p>하지만 저자들은 이와 같은 성능 향상폭이 2D semantic segmentation에서 CB나 DARS 방식이 보여주었던 것에 비해 기대에 못 미친다고 다시 한 번 지적하고 있습니다. 여기에서 저자들은 CRB 방식의 도입에 영감을 얻은 것으로 보이죠.</p><p>CRB 방식을 적용했을 때 labeling 정확도는 앞선 두 방식에 비해 개선되지 않았지만, validation set에 대한 성능이 향상됨을 확인할 수 있습니다.</p><h4 id="consistency-loss-within-mean-teacher">Consistency-loss within Mean Teacher</h4><p>여기에서는 consistency loss의 적용 범위에 대해 실험합니다. <strong>1) consistency loss를 전체 모든 point에 대해 적용</strong>하는 것과 <strong>2) unlabeled points에 대해 적용</strong>하는 것을 비교합니다.</p><p><img data-proofer-ignore data-src="../../assets/img/ScribbleKitti/fig7.png" alt="image-description" /></p><p>표의 결과는 두 가지로 나뉠 수 있습니다.</p><ul><li>최초 scribble-label에 대한 성능<li>CRB를 통해 pseudo-label을 할당한 후에 대한 성능</ul><p>최초 scribble-label에 대한 성능에선 성능 차이가 크지 않습니다. 그 이유에 대해 저자들은 scribble-label이 전체 포인트 중 8%에만 해당하기 때문에 대부분의 unlabeled 포인트들로 구성되어 있음을 이야기합니다. 때문에 1) 전체 포인트에 대한 loss 적용과 2) unlabeled points에 대한 loss 적용의 두 가지 케이스의 loss 구성이 비슷해질 것입니다.</p><p>하지만 CRB로 pseudo-labeling을 진행한 후에는 이야기가 달라집니다. mIoU 기준 0.9% 가량 나아지는 모습을 확인할 수 있습니다. 이를 바탕으로 labeled points에도 consistency loss를 적용할 경우 teacher model의 불확실성을 부여해버릴 수 있다고 생각해볼 수 있겠습니다.</p><p>본문의 4.1절에서 언급한 것처럼 Partial consistensy loss의 적용이 labeled points에 teacher model의 불확실성을 배제하기 위함임을 이야기하고 있죠.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/paper-review/'>paper-review</a>, <a href='/categories/3d-computer-vision/'>3D Computer Vision</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/deep-learning/" class="post-tag no-text-decoration" >deep_learning</a> <a href="/tags/autonomous-driving/" class="post-tag no-text-decoration" >autonomous_driving</a> <a href="/tags/distillation/" class="post-tag no-text-decoration" >distillation</a> <a href="/tags/lidar/" class="post-tag no-text-decoration" >lidar</a> <a href="/tags/point-cloud/" class="post-tag no-text-decoration" >point_cloud</a> <a href="/tags/scribble-kitti/" class="post-tag no-text-decoration" >scribble_kitti</a> <a href="/tags/3d-semantic-segmentation/" class="post-tag no-text-decoration" >3d_semantic_segmentation</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=[paper-review] Scribble-Supervised LiDAR Semantic Segmentation - good-riverdeer&url=https://good-riverdeer.github.io/posts/ScribbleKitti/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=[paper-review] Scribble-Supervised LiDAR Semantic Segmentation - good-riverdeer&u=https://good-riverdeer.github.io/posts/ScribbleKitti/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=[paper-review] Scribble-Supervised LiDAR Semantic Segmentation - good-riverdeer&url=https://good-riverdeer.github.io/posts/ScribbleKitti/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/DDPM/">[paper-review] Denoising diffusion probabilistic models</a><li><a href="/posts/SLidR/">[paper-review] Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data</a><li><a href="/posts/TEBNER/">[paper-review] TEBNER: Domain Specific Named Entity Recognition with Type Expanded Boundary-aware Network.</a><li><a href="/posts/EfficientNet/">[paper-review] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a><li><a href="/posts/Vision_Transformer/">[paper-review] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/deep-learning/">deep_learning</a> <a class="post-tag" href="/tags/transformer/">transformer</a> <a class="post-tag" href="/tags/autonomous-driving/">autonomous_driving</a> <a class="post-tag" href="/tags/distillation/">distillation</a> <a class="post-tag" href="/tags/document-understanding/">document_understanding</a> <a class="post-tag" href="/tags/lidar/">lidar</a> <a class="post-tag" href="/tags/multimodal/">multimodal</a> <a class="post-tag" href="/tags/named-entity-recognition/">Named_Entity_Recognition</a> <a class="post-tag" href="/tags/ner/">NER</a> <a class="post-tag" href="/tags/ocr/">ocr</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/SLidR/"><div class="card-body"> <span class="timeago small" >Feb 20<i class="unloaded">2023-02-20T17:30:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[paper-review] Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data</h3><div class="text-muted small"><p> Sautier, C., Puy, G., Gidaris, S., Boulch, A., Bursuc, A., &amp; Marlet, R. (2022). Image-to-lidar self-supervised distillation for autonomous driving data. In Proceedings of the IEEE/CVF Conferenc...</p></div></div></a></div><div class="card"> <a href="/posts/ArcFace_project/"><div class="card-body"> <span class="timeago small" >Sep 30, 2021<i class="unloaded">2021-09-30T12:20:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[project-review] ArcFace를 활용한 한국인 안면 인식</h3><div class="text-muted small"><p> 개요 이번 학기 학부 졸업 프로젝트 과제의 주제로 현재 COVID-19의 확산과 무인화 경향에 힘입어 “딥러닝 기반 얼굴인식을 활용한 본인인증 시스템”을 개발해보았다. 이번 포스팅에서는 위와 같은 프로젝트 수행 과정에서 ArcFace를 활용한 얼굴인식 모델을 구현해보는 과정을 간략하게 소개한다. 1. ArcFace를 활용한 안면인식 모델 개발 ...</p></div></div></a></div><div class="card"> <a href="/posts/EfficientNet/"><div class="card-body"> <span class="timeago small" >Oct 7, 2021<i class="unloaded">2021-10-07T20:48:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[paper-review] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</h3><div class="text-muted small"><p> Tan, M., &amp; Le, Q. (2019, May). Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning (pp. 6105-6114). PMLR. 개인적인 논문해석을 포함하고 ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/DDPM/" class="btn btn-outline-primary" prompt="Older"><p>[paper-review] Denoising diffusion probabilistic models</p></a> <span class="btn btn-outline-primary disabled" prompt="Newer"><p>-</p></span></div><div id="disqus_thread" class="pt-2 pb-2"><p class="text-center text-muted small"> Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script type="text/javascript"> var disqus_config = function () { this.page.url = 'https://good-riverdeer.github.io/posts/ScribbleKitti/'; this.page.identifier = '/posts/ScribbleKitti/'; }; /* Lazy loading */ var disqus_observer = new IntersectionObserver(function (entries) { if(entries[0].isIntersecting) { (function () { var d = document, s = d.createElement('script'); s.src = 'https://.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); disqus_observer.disconnect(); } }, { threshold: [0] }); disqus_observer.observe(document.querySelector('#disqus_thread')); /* Auto switch theme */ function reloadDisqus() { /* Disqus hasn't been loaded */ if (typeof DISQUS === "undefined") { return; } if (document.readyState == 'complete') { DISQUS.reset({ reload: true, config: disqus_config }); } } const modeToggle = document.querySelector(".mode-toggle"); if (modeToggle !== null) { modeToggle.addEventListener('click', reloadDisqus); window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', reloadDisqus); } </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://github.com/good-riverdeer">riverdeer</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/deep-learning/">deep_learning</a> <a class="post-tag" href="/tags/transformer/">transformer</a> <a class="post-tag" href="/tags/autonomous-driving/">autonomous_driving</a> <a class="post-tag" href="/tags/distillation/">distillation</a> <a class="post-tag" href="/tags/document-understanding/">document_understanding</a> <a class="post-tag" href="/tags/lidar/">lidar</a> <a class="post-tag" href="/tags/multimodal/">multimodal</a> <a class="post-tag" href="/tags/named-entity-recognition/">Named_Entity_Recognition</a> <a class="post-tag" href="/tags/ner/">NER</a> <a class="post-tag" href="/tags/ocr/">ocr</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
