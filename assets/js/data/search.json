[ { "title": "[paper-review] Scribble-Supervised LiDAR Semantic Segmentation", "url": "/posts/ScribbleKitti/", "categories": "paper-review, 3D Computer Vision", "tags": "deep_learning, autonomous_driving, distillation, lidar, point_cloud, scribble_kitti, 3d_semantic_segmentation", "date": "2023-08-07 16:30:00 +0900", "snippet": " Unal, O., Dai, D., &amp;amp; Van Gool, L. (2022). Scribble-supervised lidar semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 2697-2707). official implementation개인적인 논문해석을 포함하고 있으며, 의역 및 오역이 남발할 수 있습니다. :)1. Introduction자율주행 및 LiDAR 데이터에 대한 연구가 활발해지고 있으나 3D point cloud에 semantic segmentation label을 할당하는 작업은 매우 비용이 많이 소모됩니다. 저자들은 weak annotation 방법 중 하나인, scribbles가 2D semantic segmentation 작업에도 효과적임을 입증했으며 이를 LiDAR 3D point cloud semantic segmentation에 사용할 것을 제안합니다.이들은 3D semantic segmentation의 benchmark 데이터셋으로 많이 사용되는 SemanticKITTI 데이터셋 전체 labeled point 수의 8.06%에만 label을 할당해 ScribbleKITTI라는 이름의 benchmark 데이터셋을 공개합니다.또한 이 scribbles의 효과적인 학습을 위해 1) unlabeled points에 대하여 teacher 모델과 student 모델 간의 consistency loss를 도입하고 2) outdoor LiDAR 데이터에 적합한 self-training scheme을 도입하며 3) pyramid local semantic-context descriptor를 통한 augmentation으로 pseudo-label의 퀄리티를 향상시키는 방법을 사용합니다.결과적으로는 기존 모든 point들에 label이 할당되어 있는 경우보다 8%만의 label을 활용해 상대적으로 95.7%에 해당하는 성능을 달성했습니다.4. Scribble-Supervised LiDAR SegmentationProblem definition논문에선 전체 LiDAR point cloud 집합을 $P = \\set{p \\mid p=(x, y, z, I) \\in \\mathbb{R}^4}$로 정의합니다. 여기서 $x, y, z$는 각 point의 위치좌표이고 $I$는 intensity를 나타냅니다.이 집합 중 소수의 labeled points, $S \\subseteq P$에 대한 loss는 기존 fully supervised task에서 주로 사용하는 $H$ (typically cross-entropy)를 사용합니다. 이를 수식으로 표현하면 아래와 같죠.\\[\\min_\\theta \\sum^F_{f=1}\\sum^{\\lvert P_f \\rvert}_{i=1} \\mathbb{1}(p) H(\\hat y_{f,i\\mid\\theta}, y_{f,i})\\]$\\hat y_{f,i\\mid\\theta}$는 총 point cloud 프레임 $F$ 중 $f$번째 프레임 내 $i$번째 point $p_{f, i} \\in P_f$에 대한 모델의 예측 값입니다. 이 모델은 $\\theta$를 파라미터로 같습니다. 반대로, $y_{f,i}$는 ground truth label이겠죠.위에서 제시한 baseline loss는 전체 데이터 point의 약 8%에 해당하는 point에 대한 학습이기 때문에 성능저하는 필연적이고 빈번하게 출현하지 않는 class에 대한 신뢰도는 제한된 supervision 정보로 인해 당연하게 떨어질 것입니다.이후부턴 본 논문에서 제시하는 위와 같은 문제를 어떠한 방식으로 해결하는지를 제시하고 있습니다.4-1 Partial Consistency Loss with Mean TeacherMean Teacher논문에서는 unlabeled points에서 활용할 수 있는 weak supervision을 효과적으로 적용하기 위해 Mean teacher framework을 도입합니다.mean teacher framework란? 각각 student (parametrized by $\\theta$), teacher (parametrized by $\\theta^{\\text{EMA}}$)로 정의된 두 모델로써 구성됩니다. 이 때 teacher 모델의 파라미터는 student 모델의 것에 영향 받습니다. 학습 초기에는 거의 teacher 모델 본인의 파라미터로 모델 파라미터가 형성되지만, 점차 student 모델의 파라미터로 대체되죠. 이를 Exponential Moving Average (EMA)라고 말합니다. 말로 이해하려 했을 때 살짝 어렵지만, 수식으로 쉽게 이해할 수 있습니다. 학습이 진행되는 step을 $t$, teacher모델 파라미터가 student 모델의 것으로 대체되는 속도를 나타내는 smoothing coefficient를 $\\alpha$라고 했을 때, 아래와 같이 나타냅니다.\\[\\theta^{\\text{EMA}}_t = \\alpha \\theta^{\\text{EMA}}_{t-1} + (1-\\alpha) \\theta_t\\]저자들은 이를 통해 teacher model의 예측이 student 모델을 위한 weak supervision의 형태로 작용할 수 있다고 말하고 있습니다.Partial Consistency loss여기에 더불어 unlabeled points 집합 $U$에 대한 teacher model의 예측값, student model의 예측값 사이의 consistency loss를 도입합니다. 다른 consistency loss를 도입한 연구들과 다르게 unlabeled points에 대해서만 적용한다는 점을 강조하고 있습니다.이를 통해 labeled points $S$엔 teacher model의 불확실성이 배제된 깔끔한 supervision을 얻어낼 수 있겠고, 반면 unlabeled points엔 더 정확한 예측을 수행할 수 있는 teacher model의 예측 값을 student model이 근사하는 방향으로 학습하게 될 것 입니다.\\[\\min_\\theta \\sum^F_{f=1}\\sum^{\\lvert P_f \\rvert}_{i=1} G_{i,f} = \\begin{cases} H(\\hat{y}_{f,i\\mid\\theta}, y_{f,i}) &amp;amp; \\text{if } p_{f,i} \\in S \\\\ \\log(\\hat{y}_{f,i\\mid\\theta}) \\hat{y}_{f,i\\mid\\theta^{EMA}} &amp;amp; \\text{if } p_{f,i} \\in U\\end{cases}\\]하지만 여전히 unlabeled points에 대한 supervision은 전적으로 teacher model의 성능에 제한될 수 있다는 점, 학습 도중 model의 softmax prediction만으로 labeling을 수행하는 soft pseudo-labeling만으로는 최대 확률 클래스 이외의 다른 클래스들에도 약간의 supervision이 섞이는 문제점을 지적할 수 있습니다.4-2 Class-range-balanced Self-training (CRB-ST)4-1에서 언급한 것처럼 softmax prediction을 통해 pseudo-labeling을 수행했을 때 발생할 수 있는 불확실성을 배제하고 unlabeled points에 대한 확실한 예측 정보를 직접적으로 활용해야 합니다.이를 위해 저자들은 pseudo-labeling을 통해 labeled data의 수를 늘립니다. 우선 모델의 예측 값을 통해 pseudo-labeled points 집합 $L$을 정의하고, 이 집합과 기존 scribble labeled points 집합 $S$와 함께 모델을 다시 학습하는데요.기존 scribble-supervised semantic segmentation 연구들에서 이런식의 pseudo-labels 기반의 self-training이 효과적임을 입증했었습니다.하지만 자율주행 차량의 연구를 위해 수집된 해당 segmentation class들은 필연적으로 특정 클래스에 적은 수의 샘플들이 수집될 수 밖에 없습니다 (long tailed 특징).해당 문제를 해결하기 위해 이전 연구에선 전체 class distribution에 따라 pseudo-labeling에 대한 threshold를 class 별로 다르게 설정하는 방식으로 접근했습니다. 이 방법이 2D segmentation에 효과적인 결과를 가져다 주었지만, 3D LiDAR 데이터에는 또다른 문제가 존재함을 논문에서 지적하고 있습니다.그 문제는 바로 LiDAR 센서의 특성으로 인해 특정 공간에 따라 포인트의 밀도가 다르게 분포한다는 점입니다. 이러한 특성을 가진 LiDAR 센서 데이터에 대해 전역적인 class balance를 고려하게 된다면, 센서와 가까운 영역, 즉 LiDAR 센서와 가까운 영역에 편향될 수 있습니다.예를 들어, 일반적으로 차량 포인트들은 LiDAR 센서와 가까운 것에 반해, 수풀 포인트들은 LiDAR 센서와 거리가 멀리 떨어져 있다고 가정해봅시다. 이때 포인트의 class balance는 차량 클래스에 높은 분포를 가지고 반대로 수풀 클래스는 낮은 분포를 가질 것입니다. 왜냐하면 LiDAR 센서와의 거리가 멀면 멀수록 포집되는 포인트의 수가 적을 테니 말이죠.이로 인해 모든 unlabeled point가 할당될 확률은 차량에 높게 할당될 것입니다. 분명, 센서와 거리가 멀리 떨어진 unlabeled point들은 마땅히 수풀 클래스에 할당될 확률이 높아야 하지만요.이러한 점을 해결하기 위해 논문에서는 class balance를 전역적으로 살펴보는 대신, 구역을 나누어 class balance를 살펴봐야 한다고 말합니다.즉, LiDAR 센서와의 거리가 먼 포인트들끼리 클래스 분포를 살펴봐야 하는 것이죠.이를 기존 class balance pseudo-labeling 방법에서 확장해 class-range-balanced (CRB) pseudo-labeling이라고 명명하고 있습니다.아래 이미지처럼 센서와 거리가 멀리 떨어진 포인트들 간의 클래스 분포를 통해 pseudo-label을 할당하기 때문에, 수풀(green-colored) 포인트가 할당될 수 있습니다.위에서 서술한 내용을 알고리즘으로써 정리하면 다음과 같습니다.위 알고리즘의 결과물로 각 cylindrical 구역 $r$에 대하여 각 클래스 $c$에 클래스가 할당될 임계값 $k^{(c,r)}$ 집합이 도출됩니다. 이후의 pseudo-labeling 과정에서 이 임계값을 사용할 것입니다.여기서 또 주의해야할 점 한 가지는 알고리즘 14번째 line에 위치한 $\\beta$입니다. 논문에서는 $\\beta$를 두어 CRB 방법으로 pseudo-label을 할당할지, 그대로 unlabeled label로 남겨둘지 확률을 부여했습니다. 만약 $\\beta=0.5$라면, 전체 unlabeled point 중 절반은 CRB 방법으로 pseudo-label이 부여되고 나머지 절반은 그대로 unlabeled point로 남겨질 겁니다.이와 같은 방법을 채용한 구체적인 의도는 논문에서 언급하지 않고 있는 것 같아 넘어가도록 하겠습니다.CRB를 적용한 최종 목적함수는 아래와 같습니다.\\[\\min_{\\theta, \\hat{y}} \\sum_{f=1}^F \\sum_{i=1}^{\\lvert P_f \\rvert} \\left[ G_{i,f} - \\sum_{c=1}^C \\sum_{r=1}^R F_{i,f,c,r} \\right]\\]\\[G_{i,f} = \\begin{cases} H(\\hat{y}_{f,i\\mid\\theta}, y_{f,i}) &amp;amp; \\text{if } p_{f,i} \\in S \\cup L \\\\ \\log(\\hat{y}_{f,i\\mid\\theta}) \\hat{y}_{f,i\\mid\\theta^{EMA}} &amp;amp; \\text{if } p_{f,i} \\in U \\backslash L\\end{cases}\\]\\[F_{i,f,c,r} = \\begin{cases} (\\log(\\hat{y}^{(c)}_{f,i\\mid \\theta^{\\text{EMA}}}) + k^{(c,r)}) \\hat{y}^{(c)}_{f,i}, &amp;amp; \\text{if } r=\\lfloor \\lVert (p_{x,y})_{f,i} \\rVert / B \\rfloor \\\\ 0, &amp;amp; \\text{otherwise}\\end{cases}\\]여기서, negative log-threshold를 nonlinear integer optimization으로 다루기 위해, pseudo-label을 아래와 같이 정의합니다.\\[\\hat{y}^{(c)*}_{f,i} = \\begin{cases} 1, &amp;amp; \\text{if } c = \\text{argmax }\\hat{y}_{f,i \\mid \\theta^{\\text{EMA}}}, \\hat{y}_{f,i \\mid \\theta} \\gt \\exp(-k^{(c,r)}) \\\\ &amp;amp; \\text{with } r=\\lfloor \\lVert (p_{x,y})_{f,i} \\rVert / B \\rfloor \\\\ 0, &amp;amp; \\text{otherwise}\\end{cases}\\]레이블이 할당되는 각 조건에 대해 살펴볼까요??첫 번째 $\\text{argmax }\\hat{y}_{f,i\\mid \\theta^{\\text{EMA}}}$는 teacher model의 예측 클래스가 $c$일 때를 의미합니다.두 번째로 $\\hat{y}_{f,i \\mid \\theta} \\gt \\exp\\left(-k^{(c,r)}\\right)$는 student model의 예측 probability가 CRB 과정에서 도출해낸 임계값보다 큰 경우를 의미합니다. 보통 데이터의 분포를 살펴보니 구역 $r$에선 클래스 $c$가 할당되려면 이 정도 probability는 갖춰야 한다는 걸 나타내는 거죠.4-3 Pyramid Local Semantic-context (PLS)self-training은 pseudo-label의 퀄리티에 성능이 크게 좌우됩니다. 더 좋은 퀄리티의 pseudo-label을 확보하기 위해서 scribble points에서 최대한의 feature를 포착하기 위한 방법이 필요합니다.논문에서는 아래 두 가지 3D 공간에서의 semantic classes들의 분포에 대한 고찰을 이야기합니다. spatial smoothness constraint - 비슷한 공간 내의 포인트들은 같은 클래스를 가지고 있을 확률이 높습니다. semantic pattern constraint - semantic class들은 서로서로 물리적 연관성을 가지고 있습니다. 예를 들어 차량은 노면이나 주차공간 등의 지면 위에 위치하는 경우가 대부분이고, 보행자 클래스 역시 도보나 건물 등의 주변에 위치해 있는 경우가 많습니다.위 특성을 반영하기 위해 저자들은 multiple size of bins in cylindrical 좌표를 채택합니다.해당 좌표 feature의 특징은 cylinder3D에서 제안되었던 cylindrical 좌표 도입과 함께 1) 다양한 스케일의 좌표계 사용이 있습니다. 또한 해당 좌표계에서 histogram을 계산해 2) 가장 비율이 높은 class 정보를 학습 feature로 사용한다는 점이 있습니다.좌측 그림에서와 같이 녹색 cylindrical bins에선 비교적 지역적인 feature를, 적색 cylindrical bins에선 비교적 전역적인 feature를 포착할 수 있을 것입니다.논문에서 제안하는 Pyramid Local Semantic-context, PLS는 아래의 수식과 같이 정의합니다.\\[\\text{PLS} = \\left[ \\mathbf{h}^1_i/\\max(\\mathbf{h}^1_i) , ... , \\mathbf{h}^s_i / \\max(\\mathbf{h}^s_i) \\right] \\in \\mathbb{R}^{sC}\\]여기서 각 $\\mathbf{h}^s_i$는 각 cylindrical bin $b_i$에 대한 scribble label의 클래스 별 히스토그램을 나타냅니다.\\[\\begin{aligned} \\mathbf{h}_i &amp;amp;= \\left[ h^{(1)}_i, ..., h^{(C)}_i \\right] \\in \\mathbb{R}^C \\\\ h^{(c)}_i &amp;amp;= \\text{#} \\set{y_j = c \\forall j \\mid p_j \\in b_i}\\end{aligned}\\]위 PLS의 의미를 곱씹어보면 각 cylindrical 영역에 포인트가 위치하는 분포라고 해석할 수 있겠습니다.위와 같이 구축해낸 PLS feature를 기존 global geometry 정보($x, y, z$, intensity)와 함께 구성되어 $P_{avg} = \\set{ p \\mid p=(x,y,z,I,PLS) \\in \\mathbb{R}^{4+sC}}$를 형성하고, 바로 이 것이 모델 입력으로 사용됩니다.하지만, 학습이 완료된 후 pseudo-label을 inference에 사용할 수 없습니다. 그 이유는 scribble label 정보를 통해 위 PLS 정보를 확보할 수 없기 때문이죠.따라서 논문에서도 학습 과정의 마지막인 distillation 과정에선 이 PLS feature를 학습에서 제외하고 있습니다.5. Experiments 약어 정리 SS - scribble-supervised FS - fully supervised SS/FS - fully supervised 성능 대비 scribble-supervised 성능 비율 5-1 Results세 가지 baseline 모델 (Cylinder3D, MinkowskiNet, SPVCNN) 모두 제안하는 방법을 사용했을 때 성능이 향상될 수 있음을 확인할 수 있습니다.5-2 Ablation StudiesEffects of Network Components여기에선 논문에서 제안하는 framework의 각 요소가 성능에 미치는 영향을 실험합니다.train set에서의 성능과 validation set에서의 성능을 함께 보여주는데, 논문에선 train set의 성능을 통해 pseudo-label의 퀄리티를 짐작할 수 있을 것이라고 말하고 있습니다. 2번째 train mIoU와 4번째 train mIoU를 비교했을 때, PLS 방법 사용 여부에 따라 8% 가량 크게 향상됨을 확인할 수 있습니다. 이를 통해 논문에서 제안하는 PLS가 pseudo-label을 잘 포착해낼 수 있음을 짐작할 수 있습니다. 2번째 valid 성능과 3번째 valid 성능을 비교했을 때, CRB의 도입에 따라 약 2% 가량 향상됨을 확인할 수 있습니다. Pseudo-label Filtering for Self-trainingCRB pseudo-labeling 모듈의 기여를 파악하기 위해 여러 pseudo-labeling 방법을 비교합니다. naive sampling threshold-based sampling class-balanced (CB) sampling DARS class-range-balanced (CRB) samplinglabeling 정확도를 통해 outdoor LiDAR 데이터의 long-tailed 특성을 극복하기 위한 CB와 DARS 방식이 pseudo-label의 퀄리티를 크게 커버하는 모습을 확인할 수 있습니다. 더불어 validation set에 대한 성능 향상도 threshold-based 방식에 비해 크게 향상시킴을 볼 수 있습니다.하지만 저자들은 이와 같은 성능 향상폭이 2D semantic segmentation에서 CB나 DARS 방식이 보여주었던 것에 비해 기대에 못 미친다고 다시 한 번 지적하고 있습니다. 여기에서 저자들은 CRB 방식의 도입에 영감을 얻은 것으로 보이죠.CRB 방식을 적용했을 때 labeling 정확도는 앞선 두 방식에 비해 개선되지 않았지만, validation set에 대한 성능이 향상됨을 확인할 수 있습니다.Consistency-loss within Mean Teacher여기에서는 consistency loss의 적용 범위에 대해 실험합니다. 1) consistency loss를 전체 모든 point에 대해 적용하는 것과 2) unlabeled points에 대해 적용하는 것을 비교합니다.표의 결과는 두 가지로 나뉠 수 있습니다. 최초 scribble-label에 대한 성능 CRB를 통해 pseudo-label을 할당한 후에 대한 성능최초 scribble-label에 대한 성능에선 성능 차이가 크지 않습니다. 그 이유에 대해 저자들은 scribble-label이 전체 포인트 중 8%에만 해당하기 때문에 대부분의 unlabeled 포인트들로 구성되어 있음을 이야기합니다. 때문에 1) 전체 포인트에 대한 loss 적용과 2) unlabeled points에 대한 loss 적용의 두 가지 케이스의 loss 구성이 비슷해질 것입니다.하지만 CRB로 pseudo-labeling을 진행한 후에는 이야기가 달라집니다. mIoU 기준 0.9% 가량 나아지는 모습을 확인할 수 있습니다. 이를 바탕으로 labeled points에도 consistency loss를 적용할 경우 teacher model의 불확실성을 부여해버릴 수 있다고 생각해볼 수 있겠습니다.본문의 4.1절에서 언급한 것처럼 Partial consistensy loss의 적용이 labeled points에 teacher model의 불확실성을 배제하기 위함임을 이야기하고 있죠." }, { "title": "[paper-review] Denoising diffusion probabilistic models", "url": "/posts/DDPM/", "categories": "paper-review, Computer Vision, Diffusion", "tags": "deep_learning, diffusion, generative_model", "date": "2023-07-19 16:30:00 +0900", "snippet": " Ho, J., Jain, A., &amp;amp; Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in neural information processing systems, 33, 6840-6851. official implementation pytorch implementation개인적인 논문해석을 포함하고 있으며, 의역 및 오역이 남발할 수 있습니다. :)1. IntroductionDeep generative models, 심층 신경망을 활용한 생성 모델들은 여러 다양한 모달리티에서 널리 쓰이며 높은 품질의 데이터 샘플들을 생성하는 데 활용되어왔습니다. GAN(Generative adversarial networks)과 VAE(variational autoencoder)가 대표적입니다.본 논문에서는 diffusion probabilistic model, 즉 diffusion model을 생성 모델로써 제안합니다. 저자들은 diffusion model을 “똑같은 샘플을 생성하기 위해 variational inference를 통해 일정 시간 파라미터가 학습되는 Markov chain”으로 정의합니다. 위 이미지에서 \\(q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})\\) 과정을 diffusion process. 반대로, $p_{\\theta}(\\mathbf{x}_{t-1} \\mid \\mathbf{x_t})$ 과정을 diffusion process를 역으로 되돌리며 학습하는 과정인 reverse process라고 이해할 수 있습니다. 이와 같은 reverse process를 학습함으로써 노이즈가 쌓여있는 $\\mathbf{x}_T$로부터 원본 데이터 $\\mathbf{x}_0$를 복원할 수 있습니다.2. Background지금까지의 latent variable을 통해 생성해냈던 여타 다른 생성모델들과 이 diffusion model을 다르게 만드는 점이 바로 diffusion process입니다. latent variable을 활용하는 대표적 모델 VAE는 latent를 encoding하는 encoder와 이를 decoding하는 decoder를 모두 학습하는 반면, diffusion model은 forward/diffusion process는 고정된채로 이를 풀어내는 reverse process만을 학습합니다.forward/diffusion process위에서 언급한 것처럼 고정된 Markov chain을 여러번 거치는 방식으로 수행됩니다.\\[\\begin{align} q(\\mathbf{x}_t|\\mathbf{x}_{t-1}) &amp;amp;= \\mathcal{N}(\\mathbf{x}_t ; \\sqrt{1-\\beta_t} \\mathbf{x}_{t-1}, \\beta_t \\mathbf{I}), \\\\ q(\\mathbf{x}_{1:T} | \\mathbf{x}_0) &amp;amp;= \\prod^T_{t=1} q(\\mathbf{x}_t|\\mathbf{x}_{t-1})\\end{align}\\]첫 입력, 원본 이미지 \\(\\mathbf{x}_0\\)를 총 $T$번의 Markov chain을 거침으로써 이미지에 노이즈를 부여하게 됩니다. 이 때, $t$번의 step을 차근차근 밟아나가면서 $\\mathbf{x}_0$에서 $\\mathbf{x}_t$를 만들어 낼 수 있겠지만, 한 번에 이를 만들어 낼 수 있습니다.차근차근 밟아나가며 \\(\\mathbf{x}_t\\)를 생성하는 것은 memory는 물론 computing resource도 많이 소모하는 방법입니다. 저자들은 어차피 stochastic gradient를 사용하기 때문에 $t$에 대한 기댓값을 한 번에 계산하는 식으로 구현할 수 있다고 말합니다.\\[\\begin{align} q(\\mathbf{x}_t | \\mathbf{x}_0) &amp;amp;= \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\mathbf{\\alpha}}_t} \\mathbf{x}_0, (1-\\bar\\alpha_t)\\mathbf{I}), \\\\ \\alpha_t &amp;amp;= 1-\\beta_t, \\bar\\alpha_t = \\prod^t_{s=1}\\alpha_s\\end{align}\\]Reverse Process노이즈를 총 \\(T\\)번 부여한 뒤, 이 결과 \\(\\mathbf{x}_T\\)를 원복하려는 생성 태스크에선 diffusion process의 역 계산인 $q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)$를 타겟으로 학습하고자 하는 solution입니다. 따라서, 이 분포를 근사할 수 있는 모델 parameter, $\\theta$를 학습하는 것이 diffusion model의 주요 idea입니다.\\[\\begin{align} p_{\\theta}(\\mathbf{x}_{0:T}) &amp;amp;= p(\\mathbf{x}_T) \\prod^T_{t=1} p_{\\theta}(\\mathbf{x}_{t-1} | \\mathbf{x}_t), \\\\ p_{\\theta}(\\mathbf{x}_{t-1} | \\mathbf{x}_t) &amp;amp;= \\mathcal{N}(\\mathbf{x}_{t-1}; \\mu_{\\theta}(\\mathbf{x}_t, t), \\Sigma_{\\theta}(\\mathbf{x}_t, t))\\end{align}\\]위 식에서, 각 $t$ step에서의 평균 $\\mu_{\\theta}$와 표준편차 $\\Sigma_{\\theta}$는 학습되어야 하는 parameter들 입니다. 다만, 여기의 $\\Sigma_{\\theta}$는 학습 가능한 parameter로 두고 학습할 수 있겠지만 저자들의 경험상 상수로 두고 학습해도 무방하다고 합니다.이 단계들의 시작점인 $p(\\mathbf{x}_T)=\\mathcal{N}(\\mathbf{x}_T; \\mathbf{0}, \\mathbf{I})$는 충분히 Gaussian Noise가 층층히 쌓인 결과이기 때문에 표준정규분포의 형태로 정의할 수 있습니다.3. Diffusion models and denoising autoencoders이제 본 논문의 주요 기여점, 위에서 언급한 idea를 어떻게 하나의 loss function으로 정리할 수 있었는지 살펴보겠습니다.\\[\\mathbb{E}_q[\\underbrace{D_{KL}(q(\\mathbf{x}_T|\\mathbf{x}_0) || p(\\mathbf{x}_T))}_{L_T} + \\sum_{t&amp;gt;1}\\underbrace{D_{KL}(q(\\mathbf{x}_{t-1}|\\mathbf{x}_t, \\mathbf{x}_0) || p_{\\theta}(\\mathbf{x}_{t-1} | \\mathbf{x}_t))}_{L_{t-1}} \\underbrace{-\\log p_{\\theta}(\\mathbf{x}_0 | \\mathbf{x}_1)}_{L_0}]\\]위 최종 loss의 정의는 각 step $t$의 변화에 따라 loss term이 어떻게 구성되어 있는지 알 수 있습니다. 전반적인 loss의 구성은 각 step에 따라 forward process $q$분포와 reverse process $p$분포의 차이를 KL Divergence의 형태로 정의해 두 분포의 차이를 줄이는 컨셉으로 이루어져 있습니다.Forward process and Loss여기에서 \\(\\mathbf{x}_T\\)는 항상 Gaussian distribution을 근사하게 됩니다 ($p(\\mathbf{x}_T) = \\mathcal{N}(\\mathbf{x}_T; \\mathbf{0}, \\mathbf{I})$). 따라서 $q(\\mathbf{x}_T \\mid \\mathbf{x}_0)$는 사실상 표준정규분포로 빠르게 수렴할 것입니다. 때문에 학습 간 무시해도 되는 loss term입니다.$L_T$의 의미를 다시 생각해보면, 아래 두 분포의 차이를 의미하는 것을 알 수 있습니다. \\(p(\\mathbf{x}_T)\\): 노이즈가 쌓인 $\\mathbf{x}_T$의 분포 \\(q(\\mathbf{x}_T \\mid \\mathbf{x}_0)\\): $\\mathbf{x}_0$가 주어졌을 때, $\\mathbf{x}_T$로 diffusion process가 진행된 후의 분포따라서 두 분포는 모두 자연스럽게 랜덤한 정규분포일 것이고, 그 차이는 아주 미미할 것입니다.Reverse process and Loss해당 loss term이 DDPM의 핵심 loss term이라고 할 수 있습니다.그 의미를 생각해보면, $p$와 $q$의 reverse process 및 forward process 분포 차이를 줄이는 것으로 볼 수 있죠.허나 갑자기 \\(L_{t-1}\\)에 타겟으로 제공되어야 할 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)\\) 대신 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)\\)가 등장해 상당히 어색합니다. 이는 단일 조건이 주어진 \\(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)\\) 분포는 구할 수 없지만, 하나의 조건이 더 주어진 $q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)$ 분포는 Bayes’ rule에 의해 정리되고 추적이 가능하기 때문입니다.\\[\\begin{align} q(\\mathbf{x}_{t-1} | \\mathbf{x}_t, \\mathbf{x}_0) &amp;amp;= \\mathcal{N}(\\mathbf{x}_{t-1}; \\tilde\\mu(\\mathbf{x}_t, \\mathbf{x}_0), \\tilde\\beta_t\\mathbf{I}) \\\\ &amp;amp;= q(\\mathbf{x}_t | \\mathbf{x}_{t-1}, \\mathbf{x}_0) \\frac{q(\\mathbf{x}_{t-1} | \\mathbf{x}_0)}{q(\\mathbf{x}_t | \\mathbf{x}_0)} \\\\ &amp;amp;= \\cdots \\\\ &amp;amp;\\propto \\exp \\left( -\\frac{1}{2} \\left( (\\frac{\\alpha_t}{\\beta_t}+\\frac{1}{1-\\bar\\alpha_{t-1}})\\mathbf{x}^2_{t-1} - (\\frac{2\\sqrt{\\alpha_t}}{\\beta_t}+\\frac{2\\sqrt{\\bar\\alpha_{t-1}}}{1-\\bar\\alpha_{t-1}}\\mathbf{x}_0)\\mathbf{x}_{t-1} + C(\\mathbf{x}_t, \\mathbf{x}_0) \\right) \\right), \\\\ &amp;amp; \\text{where } C(\\mathbf{x}_t, \\mathbf{x}_0) \\text{ is some function not involving } \\mathbf{x}_{t-1}.\\end{align}\\]자세한 수식은 참조 포스팅에서 확인해주시기 바랍니다.위 수식에서 \\(\\mathbf{x}_{t-1}\\)에 대한 Gaussian density function의 정의에 의해 \\(\\mathbf{x}_{t-1}\\)에 대한 Gaussian 분포의 평균과 분산을 도출할 수 있게 됩니다.\\[\\begin{align} \\tilde\\beta_t &amp;amp;= \\frac{1-\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t} \\cdot \\beta_t \\\\ \\tilde\\mu_t(\\mathbf{x}_t, \\mathbf{x}_0) &amp;amp;= \\frac{\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}\\mathbf{x}_t + \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1-\\bar\\alpha_t}\\mathbf{x}_0 \\end{align}\\]다시, reparameterization trick에 의해 \\(\\mathbf{x}_0=\\frac{1}{\\sqrt{\\bar\\alpha_t}}(\\mathbf{x}_t - \\sqrt{1-\\bar\\alpha_t}\\epsilon_t)\\) 로 표현할 수 있고, 이를 위 식에 대입해\\[\\begin{align} \\tilde\\mu_t = \\frac{1}{\\sqrt{\\alpha_t}}(\\mathbf{x}_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar\\alpha_t}}\\epsilon_t), \\\\ \\text{where } \\epsilon_t, \\epsilon_{t-1}, ... \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\end{align}\\]위와 같이 평균을 도출할 수 있습니다.이제 다 왔습니다. 주어진 입력 \\(\\mathbf{x}_t\\)의 분포를 알아냈으니 우리의 모델이 이 \\(\\mathbf{x}_t\\)를 모방할 수 있게 차이를 줄이는 방향으로 학습할 수 있습니다.위에서 KL Divergence로 표현된 $L_{t-1}$를 아래와 같이 쓸 수 있는데요.\\[\\begin{align} L_{t-1} = \\mathbb{E}_q \\left[ \\frac{1}{2\\sigma^2_t} \\lVert \\color{red}{\\tilde\\mu_t(\\mathbf{x}_t, \\mathbf{x}_0)} - \\color{cyan}{\\mu_\\theta(\\mathbf{x}_t, t)} \\rVert ^2 \\right] + C, \\\\ \\text{where } C \\text{ is a constant that does not depend on } \\theta.\\end{align}\\]우리가 학습하고자 하는 분포가 \\(p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1} ; \\mu_\\theta(\\mathbf{x}_t, t), \\Sigma_\\theta(\\mathbf{x}_t, t))\\) 임을 생각해보면 학습 단계에서 입력으로 주어지는 \\(\\mathbf{x}_t\\)로부터 $\\tilde\\mu_t$를 추적할 수 있습니다. 따라서, $\\mu_\\theta$를 위 수식을 통해 $\\tilde\\mu_t$와의 차이를 줄이는 방향으로 학습할 수 있게됩니다.이 수식을 우리가 알고있는 정보들을 조합해 학습에 간단한 형태로 바꾸어보겠습니다.\\[\\begin{align} L_t &amp;amp;= \\mathbb{E}_{\\mathbf{x}_0, \\epsilon} \\left[ \\frac{1}{2\\lVert\\Sigma_\\theta\\rVert_2^2} \\lVert \\color{red}{\\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar\\alpha_t}}\\epsilon_t \\right) } - \\color{cyan}{\\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar\\alpha_t}}\\epsilon_\\theta(\\mathbf{x}_t, t) \\right)} \\rVert^2 \\right] \\\\ &amp;amp;= \\mathbb{E}_{\\mathbf{x}_0, \\epsilon} \\left[ \\frac{(1-\\alpha_t)^2}{2\\alpha_t(1-\\bar\\alpha_t)\\lVert\\Sigma_\\theta\\rVert_2^2} \\lVert \\epsilon_t - \\epsilon_\\theta(\\mathbf{x}_t, t) \\rVert^2 \\right] \\\\ &amp;amp;= \\mathbb{E}_{\\mathbf{x}_0, \\epsilon} \\left[ \\frac{(1-\\alpha_t)^2}{2\\alpha_t(1-\\bar\\alpha_t)\\lVert\\Sigma_\\theta\\rVert_2^2} \\lVert \\epsilon_t - \\epsilon_\\theta(\\sqrt{\\bar\\alpha_t}\\mathbf{x}_0 + \\sqrt{1-\\bar\\alpha_t}\\epsilon_t, t) \\rVert^2 \\right]\\end{align}\\]마지막으로 논문에서는 경험적인 결과로 계수를 제외해도 모델의 수렴에 상관없다고 언급합니다.\\[L_t^{\\text{simple}} = \\mathbb{E}_{t\\sim[1,T], \\mathbf{x}_0, \\epsilon_t}\\left[\\lVert \\epsilon_t - \\epsilon_\\theta(\\sqrt{\\bar\\alpha_t}\\mathbf{x}_0 + \\sqrt{1-\\bar\\alpha_t}\\epsilon_t, t)\\rVert\\right]\\]최종적으로 정리된 학습 알고리즘을 논문에서와 같이 아래로 정리할 수 있습니다.Appendix. Code Implementation코드 구현의 리뷰는 아래 repo를 참고 했습니다. pytorch implementation1) Training Overviewfrom denoising_diffusion_pytorch import Unet, GaussianDiffusion, Trainermodel = Unet( dim = 64, dim_mults = (1, 2, 4, 8), flash_attn = True)diffusion = GaussianDiffusion( model, image_size = 32, timesteps = 100, # number of steps sampling_timesteps = 100, # number of sampling timesteps (using ddim for faster inference [see citation for ddim paper]) objective = &#39;pred_noise&#39;)trainer = Trainer( diffusion, &#39;path/to/your/images&#39;, train_batch_size = 32, train_lr = 8e-5, train_num_steps = 700000, # total training steps gradient_accumulate_every = 2, # gradient accumulation steps ema_decay = 0.995, # exponential moving average decay amp = True, # turn on mixed precision calculate_fid = True # whether to calculate fid during training)trainer.train()2) Gaussian Diffusion - Training2-1) initialize먼저 생성자에서 논문에서 정의한 loss term과 sampling의 계산을 위한 여러 사전정보들을 초기화합니다.# denoising_diffusion_pytorch/denoising_diffusion_pytorch.py# L421def linear_beta_schedule(timesteps): &quot;&quot;&quot; linear schedule, proposed in original ddpm paper &quot;&quot;&quot; scale = 1000 / timesteps beta_start = scale * 0.0001 beta_end = scale * 0.02 return torch.linspace(beta_start, beta_end, timesteps, dtype = torch.float64)# denoising_diffusion_pytorch/denoising_diffusion_pytorch.py# L457class GaussianDiffusion(nn.Module): ... # L489 if beta_schedule == &#39;linear&#39;: beta_schedule_fn = linear_beta_schedule elif beta_schedule == &#39;cosine&#39;: beta_schedule_fn = cosine_beta_schedule elif beta_schedule == &#39;sigmoid&#39;: beta_schedule_fn = sigmoid_beta_schedule else: raise ValueError(f&#39;unknown beta schedule {beta_schedule}&#39;) betas = beta_schedule_fn(timesteps, **schedule_fn_kwargs) alphas = 1. - betas alphas_cumprod = torch.cumprod(alphas, dim=0) alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.) timesteps, = betas.shape self.num_timesteps = int(timesteps) # sampling related parameters self.sampling_timesteps = default(sampling_timesteps, timesteps) # default num sampling timesteps to number of timesteps at training assert self.sampling_timesteps &amp;lt;= timesteps self.is_ddim_sampling = self.sampling_timesteps &amp;lt; timesteps self.ddim_sampling_eta = ddim_sampling_eta # helper function to register buffer from float64 to float32 register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32)) register_buffer(&#39;betas&#39;, betas) register_buffer(&#39;alphas_cumprod&#39;, alphas_cumprod) register_buffer(&#39;alphas_cumprod_prev&#39;, alphas_cumprod_prev) # calculations for diffusion q(x_t | x_{t-1}) and others register_buffer(&#39;sqrt_alphas_cumprod&#39;, torch.sqrt(alphas_cumprod)) register_buffer(&#39;sqrt_one_minus_alphas_cumprod&#39;, torch.sqrt(1. - alphas_cumprod)) register_buffer(&#39;log_one_minus_alphas_cumprod&#39;, torch.log(1. - alphas_cumprod)) register_buffer(&#39;sqrt_recip_alphas_cumprod&#39;, torch.sqrt(1. / alphas_cumprod)) register_buffer(&#39;sqrt_recipm1_alphas_cumprod&#39;, torch.sqrt(1. / alphas_cumprod - 1)) # calculations for posterior q(x_{t-1} | x_t, x_0) posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod) # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t) register_buffer(&#39;posterior_variance&#39;, posterior_variance) # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain register_buffer(&#39;posterior_log_variance_clipped&#39;, torch.log(posterior_variance.clamp(min =1e-20))) register_buffer(&#39;posterior_mean_coef1&#39;, betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)) register_buffer(&#39;posterior_mean_coef2&#39;, (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod)) betas = $\\beta_t$ alphas = $\\alpha_t = 1 - \\beta_t$ alphas_cumprod = $\\bar\\alpha_t = \\prod_{i=1}^t \\alpha_i$ alphas_cumprod_prev = $\\bar\\alpha_{t-1} = \\prod_{i=1}^{t-1} \\alpha_i$ posterior_variance = $\\tilde\\beta_t = \\frac{1-\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t}\\cdot\\beta_t$ posterior_mean posterior_mean_coef1 = $\\frac{\\sqrt{\\bar\\alpha_{t-1}\\beta_t}}{1-\\bar\\alpha_t}$ posterior_mean_coef2 = $\\frac{\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}$ 정의는 본문에서 참고2-2) forward function# L457class GaussianDiffusion(nn.Module): ... # L796 def forward(self, img, *args, **kwargs): b, c, h, w, device, img_size, = *img.shape, img.device, self.image_size assert h == img_size and w == img_size, f&#39;height and width of image must be {img_size}&#39; t = torch.randint(0, self.num_timesteps, (b,), device=device).long() img = self.normalize(img) return self.p_losses(img, t, *args, **kwargs)diffusion의 학습을 시작하면 GaussianDiffusion 인스턴스의 forward method가 실행됩니다. 여기에서 diffusion process의 time step $t$를 샘플링하게 됩니다.이 경우엔 self.num_timesteps=100로 설정했고, batch_size는 32로, 총 32개의 [0, 99] 범위의 랜덤한 $t$를 생성할 것입니다.t = tensor([ 6, 0, 90, 76, 86, 47, 77, 8, 11, 50, 59, 59, 96, 46, 67, 55, 4, 71, 43, 76, 88, 62, 18, 26, 93, 29, 4, 82, 48, 46, 35, 63], device=&#39;cuda:0&#39;)2-3) method - p_losses이후 발전된 형태의 수식 구현코드들이 여럿 포함되어 있지만, 이를 제외하고 원조 DDPM의 구현코드(self.objective == &#39;pred_noise&#39;)만 표현하면 아래와 같습니다.# L457class GaussianDiffusion(nn.Module): ... # L741 def q_sample(self, x_start, t, noise = None): noise = default(noise, lambda: torch.randn_like(x_start)) return ( extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start + extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise ) ... # L749 def p_losses(self, x_start, t, noise = None, offset_noise_strength = None): b, c, h, w = x_start.shape noise = default(noise, lambda: torch.randn_like(x_start)) # noise sample x = self.q_sample(x_start = x_start, t = t, noise = noise) # predict and take gradient step model_out = self.model(x, t) target = noise loss = F.mse_loss(model_out, target, reduction = &#39;none&#39;) ...먼저, noise와 x_start($\\mathbf{x}_0$)를 통해 $t$ 시점에서의 posterior를 self.q_sample method를 통해 샘플링합니다.잘 살펴보면 self.q_sample method의 정의는 본문의 Algorithm 1의 다섯번째 줄 $\\epsilon_\\theta(\\sqrt{\\bar\\alpha_t}\\mathbf{x}_0 + \\sqrt{1-\\bar\\alpha_t}\\epsilon, t)$의 수식과 같다는 것을 알 수 있습니다.마지막으로 샘플링한 것을 noise($\\mathbf{\\epsilon}$)와의 MSE(Mean Squared Error)를 구하는 것을 loss로 정의할 수 있습니다.3) Generating Overview생성 과정은 아래의 예시와 같이 self.sample method를 수행하며 진행됩니다.sampled_images = diffusion.sample(batch_size=4)sampled_images.shape # (4, 3, 32, 32)4) Gaussian Diffusion - Generating4-1) method - sample발전된 형태인 DDIM의 구현(self.ddim_sample)을 제외하고, 구현코드를 보면 $T$ (self.num_timesteps)부터 0까지 $t$(t)를 차례차례 밟아가며 self.p_sample method를 수행하는 구조를 띄고 있습니다.따라서 self.p_sample 메서드의 인자 t는 99부터 시작해 0까지 총 100($=T$)번 실행되며, 100번 이미지를 생성해낼 겁니다.# L457class GaussianDiffusion(nn.Module): ... # L654 def p_sample_loop(self, shape, return_all_timesteps = False): batch, device = shape[0], self.device img = torch.randn(shape, device = device) imgs = [img] x_start = None for t in tqdm(reversed(range(0, self.num_timesteps)), desc = &#39;sampling loop time step&#39;, total = self.num_timesteps): self_cond = x_start if self.self_condition else None img, x_start = self.p_sample(img, t, self_cond) imgs.append(img) ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1) ret = self.unnormalize(ret) return ret ... # L715 def sample(self, batch_size = 16, return_all_timesteps = False): image_size, channels = self.image_size, self.channels sample_fn = self.p_sample_loop if not self.is_ddim_sampling else self.ddim_sample return sample_fn((batch_size, channels, image_size, image_size), return_all_timesteps = return_all_timesteps)이 과정 역시 본문의 Algorithm 2의 전체적인 프로세스 정의와 같음을 확인할 수 있었습니다.그렇다면 self.p_sample method의 정의를 살펴보겠습니다.4-2) method - p_sample# L35ModelPrediction = namedtuple(&#39;ModelPrediction&#39;, [&#39;pred_noise&#39;, &#39;pred_x_start&#39;])...# L457class GaussianDiffusion(nn.Module): ... # L600 def q_posterior(self, x_start, x_t, t): posterior_mean = ( extract(self.posterior_mean_coef1, t, x_t.shape) * x_start + extract(self.posterior_mean_coef2, t, x_t.shape) * x_t ) posterior_variance = extract(self.posterior_variance, t, x_t.shape) posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape) return posterior_mean, posterior_variance, posterior_log_variance_clipped ... # L609 def model_predictions(self, x, t, x_self_cond = None, clip_x_start = False, rederive_pred_noise = False): model_output = self.model(x, t, x_self_cond) maybe_clip = partial(torch.clamp, min = -1., max = 1.) if clip_x_start else identity pred_noise = model_output x_start = self.predict_start_from_noise(x, t, pred_noise) x_start = maybe_clip(x_start) if clip_x_start and rederive_pred_noise: pred_noise = self.predict_noise_from_start(x, t, x_start) return ModelPrediction(pred_noise, x_start) ... # L634 def p_mean_variance(self, x, t, x_self_cond = None, clip_denoised = True): preds = self.model_predictions(x, t, x_self_cond) x_start = preds.pred_x_start if clip_denoised: x_start.clamp_(-1., 1.) model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start = x_start, x_t = x, t = t) return model_mean, posterior_variance, posterior_log_variance, x_start ... # L645 def p_sample(self, x, t: int, x_self_cond = None): b, *_, device = *x.shape, self.device batched_times = torch.full((b,), t, device = device, dtype = torch.long) model_mean, _, model_log_variance, x_start = self.p_mean_variance(x = x, t = batched_times, x_self_cond = x_self_cond, clip_denoised = True) noise = torch.randn_like(x) if t &amp;gt; 0 else 0. # no noise if t == 0 pred_img = model_mean + (0.5 * model_log_variance).exp() * noise return pred_img, x_start먼저, self.p_sample method의 마지막 줄에서 x_start 변수는 역시 본문의 Algorithm 2의 의 것과 동일한 형태로 구현되어 있습니다.x_start 변수를 추적하다보면 아래의 self.predict_start_from_noise method에서 도출되는 것을 확인할 수 있는데요, 이 계산식이 본문의 Algorithm 2의 네 번째 줄의 것과 동일한 것을 확인할 수 있었습니다.# L457class GaussianDiffusion(nn.Module): ... # L576 def predict_start_from_noise(self, x_t, t, noise): return ( extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise )그렇다면 실제 생성될 이미지인 pred_img 변수는 어떻게 구성될까요?pred_img를 구성하는 model_mean과 model_log_variance 변수를 추적하다보면, 두 변수 모두 self.q_posterior method에서 도출되는 것을 확인할 수 있습니다.그 계산 결과는 diffusion model의 reverse process를 지속적으로 학습하며 추적이 가능해진 posterior 분포를 구성하는 과정으로 보여집니다. 다시 말해, 본문에서 언급한 아래의 평균과 분산으로 분포를 구성해 이미지를 생성함을 알 수 있죠. posterior_variance = $\\tilde\\beta_t = \\frac{1-\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t}\\cdot\\beta_t$ posterior_mean posterior_mean_coef1 = $\\frac{\\sqrt{\\bar\\alpha_{t-1}\\beta_t}}{1-\\bar\\alpha_t}$ posterior_mean_coef2 = $\\frac{\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}$ \\[\\begin{align} \\tilde\\beta_t &amp;amp;= \\frac{1-\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t} \\cdot \\beta_t \\\\ \\tilde\\mu_t(\\mathbf{x}_t, \\mathbf{x}_0) &amp;amp;= \\frac{\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}\\mathbf{x}_t + \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1-\\bar\\alpha_t}\\mathbf{x}_0 \\end{align}\\] References https://lilianweng.github.io/posts/2021-07-11-diffusion-models https://happy-jihye.github.io/diffusion/diffusion-1/ https://process-mining.tistory.com/188 pytorch implementation " }, { "title": "[paper-review] Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data", "url": "/posts/SLidR/", "categories": "paper-review, Computer Vision", "tags": "deep_learning, autonomous_driving, self-supervision, distillation, lidar, point_cloud", "date": "2023-02-20 17:30:00 +0900", "snippet": "Sautier, C., Puy, G., Gidaris, S., Boulch, A., Bursuc, A., &amp;amp; Marlet, R. (2022). Image-to-lidar self-supervised distillation for autonomous driving data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 9891-9901).개인적인 논문해석을 포함하고 있으며, 의역 및 오역이 남발할 수 있습니다. :)1. Introduction이전의 deep network을 이용한 3D segmentation 및 detection 성능은 전적으로 거대한 양의 데이터셋으로부터 비롯되었다고 할 수 있습니다. 반면에, 이 3D lidar point를 레이블링하는 작업은 매우 많은 비용이 필요하죠. 이러한 맥락에서 self-supervision을 도입하게 된다면 거대한 양의 non-annotated data에 먼저 사전 학습하고, 이를 적은 양의 annotated data에 미세조정하는 방식을 사용함으로써 annotating 비용 부담을 크게 완화할 수 있을 것입니다.본 논문에서는 대부분의 자율주행 차량들이 카메라와 lidar 센서를 함께 장착하고 있으며 이렇게 수집되는 데이터는 synchronized(time-aligned) &amp;amp; calibrated(카메라 렌즈로 인한 왜곡이 보정됨) 할 가능성이 높다는 점을 지적하며 다음과 같은 방법을 제시합니다.카메라로 촬영된 calibrated image에 self-supervised pre-trained image representations를 추출해내고 이를 point data에 지식 전이(distill)할 수 있다고 주장하며 그리고 이 과정을 2D-to-3D representation distillation이라 정의했습니다.해당 방법의 구현은 superpixel이라는 개념에서 출발하는데, 논문에서 말하기로는 “we use superpixels, which group visually similar regions that are likely to belong to the same object.” 시각적으로 유사한 영역, 즉 같은 객체일 것으로 추정되는 픽셀들의 그룹화라고 말합니다. 본문의 Fig 2.에서 그 예시를 알 수 있습니다.Figure 2. SLIC superpixels computed on an image from nuScenes본 연구에서 이 superpixel을 pooling mask로써 사용한다고 말합니다. 즉, 같은 superpixel 그룹에 속한 3D points와 2D pixels feature들에 대해 contrastive loss를 적용해 두 feature가 매칭되고 pooled 될 수 있도록 하는 것입니다.이러한 방법을 채택함에 따라 아래 두 가지 단점을 완화할 수 있다고 주장합니다. 카메라나 lidar 센서 중 한 가지 센서의 occlusion으로 인한 pixel 및 point의 오매칭, 미매칭 완화 특히 자율주행 환경과 같이 야외의 경우, 센서가 포집하지 못하는 구역도 존재할 수 있기 때문에 몇몇 point들에 대해선 데이터가 누락될 수 있겠죠. 하지만 superpixel 그룹 단위로 pixel과 point를 매칭하게 된다면 데이터 누락에 강건할 수 있겠습니다. 내가 누락되었더라도 내 옆에 누군가가 포집되었다면 그 포집된 친구를 따라갈 수 있겠죠? point cloud 영역 중 밀도있게 포집된(densely) 영역과 듬성듬성(sparsely) 포집된 영역 간의 contrastive loss 가중치를 균형있게 반영할 수 있음 자율주행 환경에서 중요한 객체는 cars, pedestrians, cyclists와 같이 드물게 포집될 수 있는 객체들인데, 이러한 객체들은 조그만 영역으로 등장할 것이므로 높은 가중치를 부여할 수 있다?? 라는 뜻으로 해석이 됩니다. 3. Our approach3.1. Image-to-Lidar Self-supervised Distillation본 논문의 주요 방향은 이미 거대한 데이터에 사전학습 되어 있는 2D image representations를 바탕으로 self-supervised distillation 방법을 통해 훌륭한 3D lidar representations를 획득하는 것입니다. 따라서 첫 단계는 2D image의 촬영 시점과 3D point의 포집 시점을 정렬하는 것으로 출발합니다.Synchronized Lidar and image data특정 촬영 시점 $t_0$에서 수집된 데이터는 아래와 같이 두 가지입니다. point cloud, $\\text{P} = (p_i)_{i=1,…,n} \\in \\mathbb{R}^{N\\times3}$ $C$개의 카메라에서 촬영된 $C$개의 이미지, $\\text{I}_1, …, \\text{I}_C \\in \\mathbb{R}^{M\\times3}$ $M$은 픽셀 수, 아마도 $H\\times W$ 수집된 데이터에서 카메라와 lidar 센서 간의 위치 차이는 알려져 있을 것이기 때문에 해당 정보를 사용해, $t_0$ 시점에서의 각 포인트 $p_i$에 대해 이미지 픽셀에 매핑을 수행합니다. 각 카메라 $c$에 대하여, $\\rho_c: \\mathbb{R}^3 \\rightarrow \\set{1,…,M} \\cup \\set{0}$ 여기서 $\\set{0}$은 point가 이미지 픽셀에 매칭되지 않을 때, 해당 point에 할당됩니다. 매칭의 방법은 자세하게 나타나있지 않습니다. camera calibration이나 혹은 다른 트릭에 대한 언급은 없습니다. 필자가 해당 데이터에 대해 익숙하지 않으므로, 해당 분야에 standard가 되는 변환 or 매칭의 방법이 있을 것이라 생각됩니다.Distilling network representations point cloud를 $D$차원의 feature로 추출해내는 3D deep neural network $f_{\\theta_{bck}}: \\mathbb{R}^{N\\times 3} \\rightarrow \\mathbb{R}^{N\\times D}$ self-supervised pre-trained image network $g_{\\bar\\omega_{bck}}: \\mathbb{R}^{M\\times 3} \\rightarrow \\mathbb{R}^{M^\\prime \\times E}$ 이 때 image network의 parameter $\\bar\\omega_{bck}$는 학습하지 않습니다. 특정 포인트 집합 \\(f_{\\theta_{bck}}(\\text{P})\\)에 대해서 \\(g_{\\bar\\omega_{bck}} (\\text{I}_1), ..., g_{\\bar\\omega_{bck}}(\\text{I}_C)\\)에 대해 superpixel 영역을 기반으로 contrastive loss를 걸어 point features와 pre-trained image features의 feature aligning을 바탕으로 \\(\\theta_{bck}\\)의 학습을 이루겠다는 것이 본 연구의 핵심입니다.3.2. Superpixel-driven Contrastive Distillation Loss먼저 superpixel을 어떻게 선택하게 되었는지 설명합니다. 저자들은 semantic segmentation과 object detection 같은 downstream task를 해결하고자 하고, 이를 위한 3D representation은 objects or object parts에 대한 “reason”이 필요하다고 생각했습니다. 따라서 지나치게 디테일한 픽셀 단계에선 feature간의 contrast를 계산하기 싫었고, object level에서의 contrast를 계산하고자 했습니다. 이에 적합한 정보가 superpixel이라고 생각했던 것이죠. ($c$ 시점에서 촬영된) 하나의 image $\\text{I}_c$에 대해 $Q$개의 superpixel로 쪼갭니다. 이 pixel indices 집합을 $\\mathcal{S}_1^c, …, \\mathcal{S}_Q^c$로 정의합니다. 이들 집합들은 서로 원소들을 공유하지 않는 집합들이 될 것입니다. 이 집합들을 앞서 정의했던 3D-to-2D mapping function을 이용해 일종의 “superpoint”와 매칭합니다. $\\mathcal{G}_1^c, …, \\mathcal{G}_Q^c$, where $\\mathcal{G}_s^c = \\set{i: \\rho_c(p_i) \\in \\mathcal{S}_s^c}$ 즉, point cloud 내 각 point들에 대해 $\\rho_c$ function을 적용하고 그 결과가 $\\mathcal{S}_s^c$ 집합에 포함되어 있으면 $\\mathcal{G}_s^c$ 집합에 할당하는 작업입니다. 저자들이 원하는 건 이 superpoint($\\mathcal{G}_s^c$)가 다수의 superpixel 중 가장 유사한 superpixel(\\(\\mathcal{S}_s^c\\))에 근접하기를 원합니다. 그 이외의 superpixel(\\(\\mathcal{S}_{s^\\prime}^{c^\\prime}\\))들은 제외하고 말이죠.때문에, 아래와 같은 방식으로 loss를 정의했습니다.Contrastive loss.각 카메라 $c$에 대해 superpoint 및 superpixel feature를 아래와 같이 average pooling을 통해 추출합니다. 여기서 각 $h$ layer는 superpoint와 superpixel feature 차원을 동일하게 맞추기 위해 붙인 것이 되겠죠.\\[f_s^c = \\frac{1}{\\left| \\mathcal{G}_s^c \\right|} \\sum_{i\\in \\mathcal{G}_s^c} (h_{\\theta_{head}} \\circ f_{\\theta_{bck}})(\\text{P})_i\\]\\[g_s^c = \\frac{1}{\\left| \\mathcal{S}_s^c \\right|} \\sum_{j\\in \\mathcal{S}_s^c} (h_{\\omega_{head}} \\circ f_{\\bar\\omega_{bck}})(\\text{I}_c)_j\\]\\[h_{\\theta_{head}}: \\mathbb{R}^{N\\times D} \\rightarrow \\mathbb{R}^{N \\times F}\\]\\[h_{\\omega_{head}}: \\mathbb{R}^{M^\\prime \\times D} \\rightarrow \\mathbb{R}^{M \\times F}\\]여기서 한 superpoint feature에 대해 아래와 같이 contrastive loss를 걸어줍니다. 아래 수식에서 $g_s^c$는 가장 연관있을 것이라 여겨지는 superpixel, 반면에 $g_{s^\\prime}^{c^\\prime}$은 매칭된 superpixel 중 나머지 superpixel 입니다.수식을 해석해보면 가장 유사하게 매칭되는 superpixel feature \\(\\mathcal{g}_s^c\\)와의 contrastive는 최대한 줄이면서, 그 이외의 다른 superpixel feature들인 \\(\\mathcal{g}_{s^\\prime}^{c^\\prime}\\)와의 contrastive는 최대한 벌어질 수 있도록 설계되어 있습니다. 학습이 진행될수록 superpoint feature \\(f_s^c\\)는 점점 \\(g_s^c\\)와는 근접하고 \\(g_{s^\\prime}^{c^\\prime}\\)와는 멀어질 겁니다.\\[\\mathcal{L}(\\theta_{bck}, \\theta_{head}, \\omega_{head}) = - \\sum_{c,s} \\log \\left[ \\frac{\\exp{(\\left&amp;lt; f_s^c, g_s^c \\right&amp;gt; / \\tau)}}{\\sum_{c^\\prime, s^\\prime} \\exp{(\\left&amp;lt; f_s^c, g_{s^\\prime}^{c^\\prime} \\right&amp;gt; / \\tau)}} \\right]\\]where $\\left&amp;lt; \\cdot, \\cdot \\right&amp;gt;$ denotes the scalar product in $\\mathbb{R}^F$Discussion Pre-training을 수행함에 있어 superpoint-superpixel level에서의 loss를 계산함의 효용성 저자들은 절대 한 superpixel이 서로 다른 두 객체를 포함하고 있지 않기 때문에 loss를 계산함에 있어 일관적이고 바람직한 point features만을 다룰 것 포집되는 point cloud의 밀도에 따라 달라질 수 있는 loss 가중치의 균형 일반적으로 포집되는 point cloud는 그 point들의 밀집도가 다름 superpixel 단위로 loss를 계산함에 따라 자연스럽게 high or low density에 관계없이 동등한 가중치로써 각각 객체들에 대한 loss를 계산할 수 있을 것임 카메라와 lidar 센서 간의 위치 오차는 0이 될 수 없음 따라서 센서 위치 오차를 보정하는 과정이 필수적인데, 이 과정에서 point-pixel 매칭 오차 역시 발생할 수 있음. 특히 occlusion 및 motion과 같은 노이즈까지 고려하면서 오차 보정은 어려움 superpixel 및 superpoint 단위로 average pooling을 수행함으로써 잘못된 매칭의 영향을 완화할 수 있을 것으로 기대 4. Experiments… 중략 …4.6. Technical Limitations 저조도 환경에서 object segment 어려움에 따른 성능 저하 두 superpixel feature가 유사한 경우에도 모델 설계상 target superpoint feature에 가장 유사한 단 하나의 superpixel feature에만 가까울 수 있도록 학습을 강요함 해당 단점은 superpixel feature extractor의 가중치를 고정해놓는 본 연구의 설계 특성상 더 치명적임 향후 연구에서 보완해야할 지점 한계점 2에 대해서 더 생각해보겠습니다. 아래 그림과 같이 superpixel이 주어졌습니다. 엄밀히 말하면 $f_1^c$는 point cloud 상의 superpoint를 의미하는데, 여기에선 편의상 superpixel 이미지 위에 표현했습니다.superpoint $f_1^c$에 대한 representation을 학습함에 있어 가장 유사한 superpixel이 $g_1^c$, 그 두 번째로 유사한 superpixel이 $g_2^c$라고 하겠습니다. 이 때, 충분히 $g_1^c$와 $g_2^c$는 충분히 유사해 보이기 때문에 같은 클래스로 보아도 무방하죠. $g_1^c$와 $g_2^c$를 모두 loss 식의 분자로 올려서 가깝게 하도록 학습을 진행하고 싶지만, 현재 모델의 구조상 단 하나의 superpixel 밖에 분자로 올라갈 특권을 얻게 됩니다. 따라서 가장 유사한 $g_1^c$만이 분자로 올라가 $f_1^c$와 가까워질 기회를 얻게됩니다. 저자들은 이 점을 한계점으로 지적했고, 이를 향후 연구과제로 언급하고 있습니다." }, { "title": "[paper-review] Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data", "url": "/posts/NEEDLE/", "categories": "paper-review, Natural Language Processing", "tags": "deep_learning, Named_Entity_Recognition, NER", "date": "2022-02-16 14:30:00 +0900", "snippet": "Jiang, H., Zhang, D., Cao, T., Yin, B., &amp;amp; Zhao, T. (2021). Named entity recognition with small strongly labeled and large weakly labeled data. arXiv preprint arXiv:2106.08977.개인적인 논문해석을 포함하고 있으며, 의역 및 오역이 남발할 수 있습니다. :)1. IntroductionDeep learning 방식의 NER은 많은 데이터를 필요로 합니다. NER task에 대한 레이블링 작업은 token-level의 레이블링이 필요하기 때문에 시간적, 금전적 비용이 많이 들고, 작업자의 실수가 발생할 수도 있습니다. 이러한 요소들이 의료 도메인과 같은 전분 분야 특정 도메인에 대한 NER을 어렵게 만드는 요인입니다.때문에 선행 연구에서는 labeled data가 한정적인 경우를 대응하기 위한 연구들을 진행했습니다. continually pre-train 큰 규모의 unlabeled data를 사용하는 방법 거대한 unlabeled open-domain data에 대해 사전학습을 진행한 BERT가 대표적 예시 이러한 open-domain pre-trained model은 특정 도메인 고유의 정보의 모델링 능력이 떨어짐 continually pre-train: Lee et al. (2020); Gururangan et al. (2020) 이미 학습된 open-domain pre-trained model을 이어서 사전 학습하는 방법을 제안 open-domain pre-trained model이 특정 도메인의 지식을 갖도록 하기 위함 weak supervision 도메인 지식 베이스에서 자동적으로 label을 생성하는 방법 Shang et al. (2018) “Biomedical dictionary”를 통해 unlabeled Biomedical documents에 레이블을 자동 생성 사람이 직접 생성하는 strongly labeled data는 위에서 언급한 것처럼 비용이 많이 소요되기 때문에 적은 양일 경우가 많습니다. 반면에 엄청난 양의 large scale unlabeled data와 전문 분야 domain knowledge를 이용해 자동적으로 생성하는 weakly labeled data는 그 양을 많이 확보할 수 있죠.논문에서 저자들은 Weak labels가 아래 세 가지 특징을 가진다고 말합니다. incompleteness: 세상에 존재하는 모든 entity를 지식 베이스가 모두 커버할 수 없음 labeling bias: 자동적으로 생성되는 weak label은 entity가 아닌 것을 entity라고 할당하는 등의 노이즈가 발생 ultra-large scale: 자동으로 생성될 수 있기 때문에 strong label에 비해 그 수가 많음위 특징을 가진 Weakly labeled data를 잘 활용할 수 있는 방법은 무엇일까요? 논문에서는 ultra-large scale의 특성으로 인해 유용한 도메인 지식은 확보할 수 있으나, 동시에 많은 양의 데이터가 incompleteness, labeling bias의 특성을 가지고 있기 때문에 이에 따른 노이즈의 영향도 많아질 수 밖에 없다고 말합니다. 때문에 NER 모델의 학습에 (Strongly + Weakly)의 단순히 조합하는 방식이나 label에 따른 가중치를 매겨 조합하는 방식 모두 incompleteness, labeling bias 특성이 야기하는 노이즈에 영향을 받을 수 있다고 말합니다.이를 해결하고자 논문에서는 프레임워크 NEEDLE을 제안합니다. NEEDLE; Noise-aware wEakly supErviseD continuaL prE-training Stage 1 open-domain pre-trained model을 target domain에 대해 사전학습 특정 도메인(target domain)의 큰 규모의 unlabeled data를 이용 Stage 2 target domain의 지식 베이스를 통해 unlabeled data에 weak label을 자동 생성 (Strongly labeled data + Weakly labeled data)를 이용 NER 학습 Stage 3 Strongly labeled data만으로 다시 한번 NER 학습 Contributions (Strong labels + Weak labels)의 단순한 조합, 가중치를 부여한 조합을 사용해 학습하는 경우 모델 성능이 떨어질 수 있음을 보임 압도적인 양(ultra-large scale)을 가진 weak label의 노이즈 영향 때문 세 단계의 프레임워크 NEEDLE을 제안, 압도적인 양의 weak label의 영향을 줄임 다양한 실험 환경 E-commerce query (온라인 쇼핑몰 검색어) NER task Biomedical NER task multi-lingual 환경 2. Preliminaries2.1. Named Entity RecognitionNER(Named Entity Recognition)은? $N$개의 tokens로 이루어진 한 문장 $\\mathbf X = [x_1, …, x_N]$에 대해서 한 토큰 범위 (a span of tokens) $s = [x_i, …, x_j] (0 \\le i \\le j \\le N)$를 하나의 엔티티(an entity)로 가집니다. $\\mathbf X$에 대한 레이블 시퀀스(a sequence of labels) $\\mathbf Y = [y_1, …, y_N]$는 BIO tag를 가지는 토큰 시퀀스입니다. BIO X entity type의 $s$의 첫 토큰은 B-X 이후 $s$의 다른 토큰들은 I-X entity가 아닌 토큰들은 O BIO로 구성된 a sequence of token labels $\\mathbf Y$를 예측하는 것이 NER(Named Entity Recognition) task라고 할 수 있습니다. Supervised NER $M$개의 주어진 문장과 이미 label이 할당된 집합 $\\set{(\\mathbf X_m, \\mathbf Y_m)}^M_{m=1}$, NER 모델 $f(\\mathbf X; \\theta)$에 대해 아래의 목적함수 최적화하며 학습하는 전형적인 딥러닝의 학습 기반 방법입니다. \\[\\hat\\theta = \\arg\\min_{\\theta} {1 \\over M} \\sum^M_{m=1} l(\\mathbf Y_m, f(\\mathbf X_m; \\theta))\\] $l(\\cdot, \\cdot)$: cross-entropy loss Weakly Supervised NER 반면에 $\\set{\\mathbf Y_m}^M_{m=1}$을 자동으로 생성한 weakly labeled data로 교체하여 위와 동일한 목적함수를 통해 학습하는 방법을 말합니다. 3. Method3.1. Stage 1: Domain Continual Pre-training over Unlabeled Data선행 연구를 따라 the large in-domain unlabeled data \\(\\set{\\tilde{\\mathbf X}_m}^{\\tilde M}_{m=1}\\)을 사용해 사전 학습된 언어 모델 \\(f_{LM}(\\cdot; \\theta_{enc}, \\theta_{LM})\\)을 추가 학습하는 과정입니다. 논문에서는 Masked Language Model 사전 학습 task를 이용해 학습한다고 언급했습니다. 이를 통해 Introduction에서 언급했던 특정 분야 도메인에 모델을 적응할 수 있게 할 수 있습니다. masked language model: 언어 모델의 사전 학습에 가장 많이 사용되는 학습 방법 (Devlin et al. (2019) 참조) $\\theta_{enc}$: 언어 모델 encoder의 parameter, 예) BERT의 parameter $\\theta_{LM}$: token classification head의 parameter, 예) fully-connected layer의 parameter3.2. Stage 2: Noise-Aware Continual Pre-training over both Strongly and Weakly labeled Data본격적으로 NER을 학습하기 이전에 대규모의 weakly labeled data를 만들어내야 합니다. 논문에서는 전문 용어 사전(dictionary)을 구축하고 이 용어 사전 내 단어와 정확한 string matching이 발생하는 구간을 태깅하는 방식을 사용합니다. 이에 대한 자세한 설명은 논문의 Appendix E를 참고하면 되겠습니다.자동 생성된 weakly labeled data: \\(\\set{(\\tilde{\\mathbf X}_m, \\tilde{\\mathbf Y}_m^w)}^{\\tilde M}_{m=1}\\) 를 사용해 BERT-CRF구조의 모델로 본격적인 NER 학습이 진행됩니다. 이 때 모델의 가중치는 Stage 1의 BERT parameter(\\(\\theta_{enc}\\)) + 랜덤하게 초기화된 CRF layer의 parameter(\\(\\theta_{CRF}\\)) 조합으로 구성됩니다.이 모델에 약간의 Strongly labeled data를 통한 NER 학습을 진행시킨 뒤(Initial model) 논문의 주요 기여점이라 할 수 있는 Weak Label Completion과 Noise-Aware Loss Function의 테크닉을 적용해 Strongly + Weakly labeled data 모두 사용해 본격적인 학습이 시작됩니다. Weak Label Completion 해당 테크닉은 Initial model의 예측 값으로 weak label을 대체하는 과정이라고 요약할 수 있는데, 간단하게 입력값(inputs)과 출력값(output)을 수식으로 표현할 수 있습니다. inputs a sentence $\\tilde{\\mathbf{X}} = [x_1, …, x_N]$ the original weak labels $\\tilde{\\mathbf{Y}^w} = [y_1^w, …, y_M^w]$ the predictions from the initial model $\\tilde{\\mathbf Y}^p = \\arg\\min_{\\mathbf Y} l(\\mathbf Y, f(\\tilde{\\mathbf X}; \\theta_{enc}, \\theta_{CRF})) = [y_1^p, …, y_N^p]$ output $\\tilde{\\mathbf Y}^c = [y_1^c, …, y_N^c]$ \\[y_i^c = \\begin{cases} y_i^p &amp;amp;&amp;amp; \\text{if } y_i^w = \\bigcirc \\\\y_i^w &amp;amp;&amp;amp; \\text{otherwise}\\end{cases}\\] weak label이 O; non entity로 태깅되었다면, 모델의 예측 값을 사용 weak label이 B-X 혹은 I-X와 같이 entity로 태깅되었다면 weak label을 그대로 사용 누락된 entity를 Strong label에 학습한 모델의 힘을 빌려 weakly label의 불완전성을 보완하는 효과를 얻을 수 있을 것 같습니다. Noise-Aware Loss Function 저자들은 Introduction에서도 언급했듯이 weak labels은 ultra-large scale의 특성을 가지므로 불완전한 weakly labeled data 분포에 모델이 지나치게 적합되는 것을 완화하고자 했습니다. 따라서, Weak Label Completion 테크닉으로 수정된 corrected weak labels $\\tilde{\\mathbf Y^c}$의 신뢰도를 기반으로 한 손실함수를 제안합니다. confidence: $\\hat P(\\tilde{\\mathbf Y}^c = \\tilde{\\mathbf Y} \\mid \\tilde{\\mathbf X})$: $\\tilde{\\mathbf Y}^c$가 실제 label이 될 것으로 추정되는 확률 confidence가 높은 경우 log-likelihood confidence가 낮은 경우 log-unlikelihood \\[\\begin{aligned}&amp;amp; l_{NA}(\\tilde{\\mathbf Y}^c, f(\\tilde{\\mathbf X}_m; \\theta)) \\\\&amp;amp;= \\mathbb E_{\\tilde{\\mathbf Y}_m = \\tilde{\\mathbf Y}^c_m \\mid \\tilde{\\mathbf X}_m} \\mathcal L(\\tilde{\\mathbf Y}^c, f(\\tilde{\\mathbf X}_m; \\theta), \\mathbb 1(\\tilde{\\mathbf Y}_m = \\tilde{\\mathbf Y}_m^c)) \\\\&amp;amp;= \\hat P(\\tilde{\\mathbf Y}^c = \\tilde{\\mathbf Y} \\mid \\tilde{\\mathbf X})l(\\tilde{\\mathbf Y}^c, f(\\tilde{\\mathbf X}; \\theta)) + P(\\tilde{\\mathbf Y}^c \\ne \\tilde{\\mathbf Y} \\mid \\tilde{\\mathbf X})l^-(\\tilde{\\mathbf Y}^c, f(\\tilde{\\mathbf X}; \\theta))\\end{aligned}\\]\\[\\begin{aligned}&amp;amp; l(\\tilde{\\mathbf Y}^c, f(\\tilde{\\mathbf X}; \\theta)) = -\\log P_{f(\\mathbf X; \\theta)}(\\mathbf Y) &amp;amp;\\\\&amp;amp;l^-(\\tilde{\\mathbf Y}^c, f(\\tilde{\\mathbf X}; \\theta)) = -\\log [1 - P_{f(\\mathbf X; \\theta)}(\\mathbf Y)]\\end{aligned}\\] confidence 추정 (Appendix A) corrected weak label의 confidence, $\\hat P(\\tilde{\\mathbf Y}^c = \\tilde{\\mathbf Y} \\mid \\tilde{\\mathbf X})$를 추정하는 과정은 Appendix A에서 다루고 있습니다. 최종적으로는 original weak label과 model prediction의 신뢰도의 선형 결합으로 추정하게 됩니다. original weak label이 model prediction과 일치하는 경우 weak label의 신뢰도 가중 original weak label이 model prediction과 일치하지 않는 경우 model prediction의 신뢰도를 가중 이 때 model prediction의 신뢰도; $\\hat P(\\tilde{\\mathbf Y}^p = \\tilde{\\mathbf Y} \\mid \\tilde{\\mathbf X})$의 추정은 CRF score의 분포에 histogram binning을 적용하고 각 bin에 대해 validation set에 대한 결과를 바탕으로 confidence를 추정합니다. 추정 방법에 대해서는 정확히 이해하지 못하였습니다…ㅜ confidence가 너무 높은 경우에는 이를 완화하는 후처리 작업도 진행했습니다. $P(\\tilde{\\mathbf Y}^c = \\tilde{\\mathbf Y} \\mid \\tilde{\\mathbf X}) = \\min(0.95, P(\\tilde{\\mathbf Y}^c = \\tilde{\\mathbf Y} \\mid \\tilde{\\mathbf X}))$ corrected weak label의 신뢰도는 weak label &amp;amp; model prediction의 신뢰도 선형 결합으로 정의된다.논문에서 제시한 예시, CRF score histogram bin과 confidence의 상관관계를 확인할 수 있다. Noise-Aware Loss Function의 해석 이렇게 정의된 Noise-aware loss를 자세히 살펴보면 log-unlikelihood loss는 regularization term으로 보이고, confidence는 loss term (여기서는 log-likelihood)과 regularization term (여기서는 log-unlikelihood) 사이의 가중을 결정하는 adaptive weight로 보이기도 합니다. 최종 손실함수 최종적으로 Stage 2의 Initial model 학습에 strongly labeled data에 대해서는 일반적인 negative log-likelihood를, weakly labeled data에 대해서는 Noise-Aware Loss Function를 사용하게 됩니다. \\[\\min_\\theta {1\\over M+\\tilde M} \\left[ \\sum^M_{m=1} l(\\mathbf Y_m, f(\\mathbf X_m ;\\theta)) + \\sum^{\\tilde M}_{m=1}l_{NA}(\\tilde{\\mathbf Y^c}_m, f(\\tilde{\\mathbf X_m}; \\theta))\\right]\\]3.3 Stage 3: Final Fine-tuningStage 2에서 Initial model을 만들었던 방법대로 Strongly labeled data에 다시 한 번 최종적으로 미세 조정하게 됩니다. Experiment 파트에서 이 효과를 설명하고 있습니다.4. Experiments baseline models transformer-based open-domain pretrained models + CRF e.g., BERT, mBERT, RoBERTa-Large + CRF BIO tagging scheme으로 entity tagging Adam optimizer4.1. Datasets E-commerce query domain English NER 10 different entity types multilingual NER 12 different entity types unlabeled in-domain data &amp;amp; weak annotation 쇼핑 웹사이트(Amazon)의 사용자 행동 데이터를 수집 Biomedical domain unlabeled data PubMed 2019 baseline 수집 weak annotation 수집한 dictionary에서 exact string matching을 통해 할당 dictionary수집 방법에 대해선 언급하지 않았음 Weak labels performance Table 1의 weak label이 strong(golden; 데이터셋이 제공하는 원래 label) label과 비교 E-commerce query domain에서 golden label에 비해 Recall 값이 많이 떨어짐 entity가 아닌 것을 entity로 태깅하는 경우가 많은 것으로 보임 Table 1. E-commerce query domain의 Weak label의 퀄리티가 다소 떨어지는 것이 보인다.4.2. Baselines 모든 baseline 모델들은 in-domain unlabeled data에 continually pre-training을 진행하였습니다 (Stage 1). baseline 모델들에 대한 설명은 생략…4.3. E-commerce NER4.3.1. Main Results weakly supervised baselines WSL(weakly supervised learning) 방식이 가장 성능이 떨어집니다. 이를 통해 weakly labeled data가 모델의 성능을 떨어뜨릴 수 있음을 알 수 있습니다. semi-supervised baselines 재미있게도, semi-supervised 방식이 오히려 supervised 방식의 성능을 능가하는데, 모델이 만들어낸 pseudo label이 weak label보다 좋을 수 있다고 추정할 수 있습니다. 최종적으로 NEEDLE은 semi-supervised 방식보다 성능이 좋았으며 pseudo label만을 사용하는 것보다 적절한 weak label을 함께 활용하는 것이 좋을 수 있다고 저자는 말합니다. 4.3.2. AblationWLC - Weak label completion; NAL - Noise-aware loss function; FT - Final fine-tuning4.4. Biomedical NER4.5. Analysis Size of Weakly Labeled Data Figure 2(a) &amp;amp; Figure 2(b) weakly labeled data의 양을 변경하며 실험했을 때, SST 방식은 성능 변화 폭이 미미했습니다. 반면 NEEDLE의 경우 weakly labeled data가 늘어날수록 성능 역시 향상되는 것을 볼 수 있고 NEEDLE이 weakly labeled data를 비교적 잘 활용하고 있다고 볼 수 있습니다. Figure 2(c) weakly labeled data의 수가 늘어날수록 fine-tuning 수행 유무에 따라 성능 차이가 벌어지는 것을 볼 수 있는데, final fine-tuning을 통해 학습할 유용한 정보가 있을 수 있음을 나타내고 있습니다. Two Rounds of Stage 2 Training Stage 3의 효과를 관찰한 저자들은 “Final fine-tuning” 대신 Stage 2의 학습을 한 번 더 수행하면 어떨지 실험을 진행했습니다. Stage 3 이후에, 새로운 모델로 weak label을 수정 strongly + weakly labeled data로 noise-aware training을 지속 strongly labeled data로 final fine-tuning 전체적인 학습 과정을 한 바퀴 더 돌렸을 때, 약간의 성능 향상이 있었습니다. 반면에 SST 방식, NEEDLE w/o NAL의 경우엔 변화가 없었음을 볼 수 있습니다. Noise-aware loss가 없는 경우 Noise-aware loss가 주요 요소인 Stage 2를 한 바퀴 더 굴린다는 개념이 의미없는 행위라고 생각이 드네요. Size of Strongly Labeled Data 이번에는 Stronly labeld data의 수를 조절하며 실험하는데, 30% ~ 50%의 strongly labeled data가 있으면 fully supervised 방식의 성능에 도달함을 확인했습니다. 또한 20%의 데이터만으로 다른 60% 데이터를 사용한 baseline의 성능과 동등함을 근거로 들며 3배 정도 strongly labeled data에 대해 효율적이라고 주장하고 있네요. Strongly labeled data의 1% 만으로도 70% 초반의 성능을 보인다.4.6. Weakly Label Errors in E-commerce NER여기에서는 weak label이 유발하는 error에 대해서 이해하고, 이를 바로 잡을 수 있는 방법들에 대해 이야기합니다. Label Distribution Mismatch 먼저 저자들은 weak label의 분포가 strong label의 분포와의 차이가 심하다고 말합니다. Figure 4에서도 볼 수 있듯이 분포가 많이 다릅니다. 반면 SST 방식의 모델이 만들어낸 pseudo label의 분포는 strong label의 분포와 거의 비슷한데, 이를 두고 SST 방식이 직접적으로 학습했을 때 성능이 좋은 이유라고 말합니다. Systematical Error 저자들은 weakly labeled data의 시스템 내부적 error는 Stage 3; Final fine-tuning으로 쉽게 바로잡을 수 있다고 주장합니다. 여기서 예시로 들고 있는 “amiibo”는 “nintendo”사의 Product Line 중 하나입니다. 따라서 “amiibo” 앞에 나타나는 “zelda”나 “wario”와 같은 entity는 Misc 타입으로 태깅되는 것이 옳습니다. 하지만 weak label에는 이를 Color 타입이라고 잘못 태깅되어 있어 corrected weak label에도 Color 타입이라고 잘못 태깅되어 있죠. Stage 2에서는 이러한 샘플들을 Color + ProductLine 조합으로 잘못 학습하지만 Stage 3에 가서 Misc + ProductLine의 조합이라고 쉽게 수정될 수 있습니다. 이와 같이 끝에 수정되는 방향이, 애초에 적은 양의 Misc + ProductLine의 옳게된 조합으로 학습되는 것보다 모델이 학습하기에 쉬운 방향이라고 주장하고 있습니다. Entity BIO Sequence Mismatch in Weak Label Completion Stage 2에서 Weak Label Completion 과정을 수행하는 과정에서 BIO 태깅 시퀀스가 깨져버리는 경우가 발생할 수 있습니다. 예를 들어 B-ProductType &amp;gt; O &amp;gt; O로 태깅되어 있던 시퀀스에서 첫 번째 O 태그가 Weak Label Completion 과정에 의해 수정되면서 B-ProductType &amp;gt; I-Color &amp;gt; O로 깨져버리는 경우죠. 저자들은 이러한 경우가 E-commerce English query 데이터의 1.39% 있었다고 말하고, 이 경우를 학습에서 제외하는 실험도 진행했습니다. 그 결과 F1 score 기준, Stage 2에서는 +1.07의 향상이 있었지만 마지막 Stage 3에서 -0.18의 성능 저하가 있었다고 밝힙니다. 이 결과 굳이 해당 샘플들을 제거하는 과정은 필요없다고 결론짓고 있습니다. Quantify the Impace of Weak Labels 여기에서는 구체적인 비교를 통해 weak label을 대하는 NEEDLE 프레임워크의 효용성을 밝힙니다. Stage 2의 Initial model은 2384개의 예측 오류를 가지는데, NEEDLE 프레임워크 종료 시점에는 이 중 454개의 오류가 해결되고 새로운 오류가 311개 추가됩니다. 여기에는 weak label만의 영향만 있는게 아니기 때문에 weak label에 대해서만으로 추리면 각각 171개, 93개로 관찰됩니다. 이 비율이 즉, “weak label을 바로 잡는 비율” = “NEEDLE이 바로 잡은 샘플 수” : “NEEDLE이 추가로 범한 예측 오류” = 171 : 93으로 그 비율이 $171/93 = 1.84 &amp;gt; 1$로 영향이 적지 않고 NEEDLE이 weak label에 효용성이 있다고 말하고 있습니다. 5. Discussion and Conclusion본 연구는 fully weakly supervised 방식, semi-supervised 방식과 모두 연관성이 있습니다. 각각 weakly labeled data를 사용한다는 점, 전체 데이터셋 중 일부만 레이블이 할당되어 있다는 점에서 말이죠.Fully weakly supervised NER 방식이 fully supervised 방식에 비해 성능이 떨어짐을 꼬집고 fully weakly supervised 방식은 실제 애플리케이션에는 적합하지 않은 방식이며 반면에 Semi-supervised 방식은 weak supervision을 활용하지 않아 부분적으로만 성능을 끌어올릴 수 있다고 볼 수 있습니다.이전의 fully weakly supervised 방식의 연구들은 weak labels에 strong labels를 단순하게 섞어버려 학습하는데 이를 NEEDLE 프레임워크에서는 weak labels의 노이즈를 억제하는 방식으로 완화하고 있죠.그런 의미에서 본 연구는 supervised NER과 weakly supervised NER을 연결하는 다리와 같은 연구라고 볼 수 있습니다." }, { "title": "[paper-review] TEBNER: Domain Specific Named Entity Recognition with Type Expanded Boundary-aware Network.", "url": "/posts/TEBNER/", "categories": "paper-review, Natural Language Processing", "tags": "deep_learning, Named_Entity_Recognition, NER, BERT", "date": "2022-01-17 14:30:00 +0900", "snippet": "Fang, Z., Cao, Y., Li, T., Jia, R., Fang, F., Shang, Y., &amp;amp; Lu, Y. (2021, November). TEBNER: Domain Specific Named Entity Recognition with Type Expanded Boundary-aware Network. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (pp. 198-207).개인적인 논문해석을 포함하고 있으며, 의역 및 오역이 남발할 수 있습니다. :)1. Introduction 현존하는 대부분의 NER 방법들은 많은 양의 labeled data가 필요함 특정 도메인의 전문가가 직접 레이블링을 해야하는 경우, 사람의 노력이 많이 필요하며 시간도 많이 소요 distant supervision: 자동적인 레이블링 할당 방법들이 제안되어 왔음 특정 도메인에 대한 raw corpus 혹은 dictionary를 사용 정확한 문자열 매칭(string matching)으로 레이블을 할당 위 방법으로 진행할 경우, 아래 두 가지 문제점이 존재할 수 있음 Incomplete annotations original dictionary에 entity가 모두 포함되지 않을 수 있음: out-of-dictionary 기존 데이터셋에 dictionary가 전체 domain entity에 50% 정도만 포함하고 있음을 확인 일부 선행 연구에서는 heuristic rule로 dictionary를 확장 다른 도메인에 일반적으로 적용될 수 없음 the difficulty of recalling new entities supervised model을 통해 NER을 수행했던 선행 연구에서도 모델의 한계로 인해 새로운 entity를 찾아내는 것이 어려웠음 선행 연구에서는 sequence label model이나 boundary detection model과 같은 방식으로 사용, 맥락적 정보(context information)만을 활용했음 sequence label model: 문장의 단어마다 레이블을 예측하는 모델링 boundary detection model: 문장의 특정 구간이 entity임을 예측하는 모델링 특정 도메인 corpus에 대한 전역적인 통계적 특징(global statistical features)은 선행 연구에서 배제되었음 TEBNER: Type Expanded Boundary-aware NER) dictionary expansion raw corpus에서 높은 퀄리티의 어구(phrase)을 추출, 잠재적인 entity로 간주 수집된 어구에 entity typing model(with context information)로 분류 및 필터링 필터링된 어구들을 dictionary에 추가 Incomplete annotations 문제 완화 multi-granularity boundary labeling strategy 서로 다른 관점에서 문장 시퀀스를 태깅하고 이를 통해 명확한 entity boundary(문장 내 어느 범위가 entity인지)를 찾아냄 token interaction tagger: entity token 간의 내부적 연결성 관점 sequence labeling: 문장 내 명확한 entity 범위 관점 global statistical features: 특정 domain corpus의 전역적 관점 Contributions 애매한 방식이나 heuristic rule 등에 의존하지 않고 semantic context를 통한 새로운 dictionary extension method를 제안 word, sentence, corpus level의 정보들을 통합(fusing)하여 entity boundary를 특정할 수 있는 Multi-granularity boundary-aware network 제안 세 가지 벤치마크 데이터셋에 대한 폭넓은 실험 설계 및 결과 제시2. Related WorkSupervised Models 최근 일부 연구에서는 각 entity의 범위(boundary)만을 찾아내는 연구가 일부 있음 Wang et al., 2018; Zhang et al., 2019; Li et al., 2020b 사전학습 모델의 등장에 따라 ELMo(Peters et al., 2018)나 BERT(Devlin et al.,2019)를 사용하여 NER 태스크를 수행 많은 양의 labeled data가 필요함Distant supervision methods AutoNER; Shang et al., 2018b out-of-dictionary 어구들을 unknown type으로 매칭 잠재적인 entity 후보로 선정 HAMNER; Liu et al., 2020 entity boundary를 예측하는 entity classification model을 도입 3. Problem Definition Named Entity Recognition 주어진 단어 시퀀스 $X = [x_1, x_2, …, x_n]$에서 $t$ 타입의 entity 범위 $e_t = \\left[x_i, …, x_j\\right] (0 \\le i \\le j \\le n)$를 찾아냄 Distant Supervision NER dictionary $D$를 입력으로 제공 dictionary는 이름(surface name)과 entity type을 적어놓음 4. The Proposed Method 두 가지 요소로 구성 Dictionary Extender NER 레이블을 생성 target domain에 대해서 많은 어구(phrase)를 생성함 Entity Recognizer entity boundary와 entity type을 예측 4.1. Dictionary Extender High-quality phrase extraction 선행 연구에서 처럼 AutoPhrase(Shang et al., 2018a)를 통한 어구 생성 AutoPhrase distantly supervised phrase mining tool 빈도 수에 따라 후보 어구를 만들고 검증하며 어구를 생성하는 툴 Threshold 좋은 퀄리티의 entity만을 확보하기 위한 임계값, AutoPhrase의 신뢰도 점수 기반 single word phrase: 0.9 multi-word phrase: 0.5 AutoPhrase의 아키텍처, Shang et al., 2018a Entity classification AutoPhrase가 생성한 어구 리스트를 필터링하는 과정 Entity typing model을 아래와 같이 구성 일차적인 Entity 후보의 엔티티 타입을 예측 ex.) $none$ type으로 예측되는 경우 dictionary에서 제외 PreTrained BERT &amp;gt; Linear &amp;gt; softmax를 태워 cross-entropy로 학습 BERT&#39;s input: $[CLS] \\space ctxt_l \\space [x_i] \\space … \\space [x_j] \\space ctxt_r \\space [SEP]$ (텍스트 시퀀스) Linear layer&#39;s input: $V_h = V_{[cls]} \\oplus V_{[x_i]} \\oplus V_{[x_j]}$ Relu &amp;gt; softmax: $P(y_t\\mid e_t) = softmax(W^2_t (Relu(W^1_t V_h + b^1_t)) + b^2_t)$ cross-entropy: $L_{type} = -\\sum_{i=1}^n y_i \\log(P(y_i\\mid e_i))$ $none$ entity 추가 Entity typing model이 $none$ entity type을 식별할 수 있도록 AutoPhrase의 신뢰도 점수가 0.3보다 낮은 어구를 $none$ entity type으로 레이블링하여 학습 데이터에 추가 Entity Filtering Entity typing model의 예측 값이… $none$ entity type으로 분류된 어구 제거 여러 entity type으로 분류 예측된 어구 제거 남은 어구들을 original dictionary에 추가하여 확장 구성된 dictionary를 entity recognizer의 학습에 사용 4.2. Entity Recognizer 개요 distantly supervised 방식으로 레이블이 할당된 데이터는 error가 포함할 확률이 높음 entity boundary detection &amp;amp; entity classification을 동시에 모델링할 경우 과적합될 가능성이 높음 따라서, boundary, type classification을 각각 학습 Section 4.1.은 type classification의 학습 (Entity typing model) Section 4.2.는 boundary의 학습 (token interaction model $M_w$, sequence label model $M_s$의 학습) 아래 3가지의 tagging schema를 fusing하여 다방면의 정보(multi-granularity)를 종합 “Break or Tie” Tagging Schema ($M_w$) “BIO” Tagging Schema ($M_s$) “Phrase Matching” Tagging Schema (AutoPhrase) “Break or Tie” Tagging Schema for word level entity boundary ($M_w$) entity가 분절되는 토큰들에 “Break” entity가 지속되는 토큰들에 “Tie” $M_w$: output representation from BERT에 대해 아래와 같은 방식으로 학습 \\[V^\\prime_i = concatenate(i\\text{-th token}, i+1\\text{-th token})\\] $i$번째 representation과 $i+1$번째 representation의 특징 결합 \\[P(c_i|V^\\prime_i) = {\\exp(c^T_i V^\\prime_i) \\over \\sum_{c_k\\in C} \\exp(c^T_k V^\\prime_i)}\\] $C = \\set{[Break], [Tie]}$ $P(c_i\\mid V^\\prime_i)$: $V_i^\\prime$이 토큰 $c_i$가 될 확률 “Break or Tie”, 파란색 entity는 모델이 인식하지 못하는 경우를 가정 “BIO” Tagging Schema for word level entity boundary ($M_s$) 기존의 NER 태깅 방식과 동일 entity 범위가 시작하는 지점에 “B” entity 범위가 지속되는 지점에 “I” entity 범위에 속하지 않는 지점에 “O” $M_s$: BERT &amp;gt; Linear &amp;gt; softmax 학습 $C = \\set{[B], [I], [O]}$ “BIO”, 파란색 entity는 모델이 인식하지 못하는 경우를 가정 “Phrase Matching” Tagging Schema for corpus level entity boundary 선행 연구들은 corpus 내부의 통계적 특징들을 간과함 AutoPhrase의 결과를 태깅 corpus의 통계적 특징들을 반영한 AutoPhrase의 결과를 태깅했으니 통계적 요소를 반영하였다고 생각하는듯…?! “Phrase Matching”, 파란색 entity는 모델이 인식하지 못하는 경우를 가정 결론 각기 다른 entity tagging 방식을 결합하여 마지막 분류의 입력으로 사용 Algorithm: The process of TEBNER5. Experiments5.1. Experiment Setup Dataset BC5CDR Chemical and Disease domain NCBI-Disease Disease domain LaptopReview review sentences AspectTerm mentions Training Details 사전 학습 가중치 biomedical domain: &quot;biobert-base-cased-v1.1&quot; technical domain: &quot;bert-base-cased&quot; maximum sentence length: 256 tokens hidden representation size: 786 learning rate: 3e-5 dropout probability: 0.15 AdamW optimizer Multi-layer perceptron for “entity classifier”: a depth of 2 and a hidden size of 256 5.2. Comparing with Previous Work Baselines supervised model BiLSTM-CRF ELMo-NER BERT-NER distantly supervised model Dictionary Match: 주어진 dictionary $D$의 단순 string matching SwellShark: biomedical 도메인에 특화된 방법, regular expressions가 필요하며 특별한 케이스에 대해선 직접 조정이 필요함 AutoNER: BiLSTM 모델을 사용해 인접 토큰들의 연결성을 학습, false-negative labels의 수를 줄임 HAMNER: 기존 SOTA, headword-based matching을 통해 dictionary를 확장하고 entity typing model로 entity 범위(spans)를 예측 Results AutoNER &amp;amp; HAMNER과 동일한 dictionary &amp;amp; phrases를 사용하였음에도 더 좋은 성능 SwellShark가 biomedical 도메인에 특화된 구조이지만 더 나은 성능 AutoNER도 “Break or Tie” tagging을 적용해 다방면의 정보를 종합하고자 했으나 인접한 토큰들의 연결성만을 모델링하여 성능이 떨어짐 5.3. Impact of Different Modules Effectiveness of Dictionary Extension 복잡한 방법들을 사용하지 않았음 ex). headword matching, semantic similarity calculation, annotations weight setting) 문맥적 의미 정보만을 사용해 dictionary 확장 어느 domain에나 적용할 수 있음 Influence of the number of Annotations 확장한 dictionary로 만든 distantly annotations의 수를 조절하며 성능 비교 데이터의 수가 늘어날수록 증가하다가 80% 부근에서 수렴하는 경향이 있음 Ablation studies6. Conclusion 새로운 dictionary extension 방법 제안 distant supervision 방식으로 특정 도메인에 적합한 boundary-aware model을 설계 어느 도메인에서나 적용될 수 있도록 entity classification model을 사용해 dictionary를 확장하였음 3가지 tagging schema를 적용해, 지역적 &amp;amp; 전역적 측면에서의 entity 인식을 시도하였음" }, { "title": "[paper-review] Donut: Document Understanding Transformer without OCR", "url": "/posts/Donut/", "categories": "paper-review, Computer Vision", "tags": "deep_learning, document_understanding, multimodal, transformer, GRU, swin_transformer", "date": "2021-12-27 12:20:00 +0900", "snippet": "Kim, G., Hong, T., Yim, M., Park, J., Yim, J., Hwang, W., … &amp;amp; Park, S. (2021). Donut: Document Understanding Transformer without OCR. arXiv preprint arXiv:2111.15664.개인적인 논문해석을 포함하고 있으며, 의역 및 오역이 남발할 수 있습니다. 올바르지 못한 내용에 대한 피드백을 환영합니다 :)1. Introduction Visual Document Understanding (VDU) 스캔된 형태 혹은 사진으로 촬영한 형태의 전자 파일 형태의 문서에 대한 이해 문서 처리 자동화를 위해 필요한 중요한 단계임 document classification, parsing, visual question answering 등등 Optical Character Recognition (OCR) 시스템 딥러닝을 활용한 OCR 시스템이 발전함에 따라 Document understanding을 위한 기존 연구들은 독립적인 OCR 모듈을 사용해 텍스트를 추출 해당 연구들의 문제점 OCR은 유지비용이 많이 든다: 최근의 OCR 모델들은 GPU를 사용하기도 하며 큰 규모의 학습 데이터셋이 필요함 OCR이 발생시키는 error는 후속 프로세스에 부정적인 영향을 미친다: 한국어나 일본어와 같이 비교적 OCR이 어려운 언어에 대해 빈번하게 발생함 Donut: Document understanding transformer raw 입력 이미지만으로 원하는 형태의 출력으로 매핑할 수 있도록 모델링 end-to-end로 학습하며, 다른 모듈(OCR은 물론)에 의존적이지 않음 SynthDoG: Synthetic Document Generator 합성 문서 이미지 생성기 모델 사전학습에 꼭 필요한 대규모 문서 이미지를 생성할 수 있음 대규모 실제 문서 이미지에 대한 의존성을 완화 [@ Contributions] Visual Document Understanding을 위한 새로운 방법론 제시 최초로 OCR-free한 transformer 아키텍처에 기초한 방법을 제안 end-to-end로 학습 가능 SynthDoG제안 &amp;amp; 모델의 사전학습을 위한 간단한 사전학습 task 제시 광범위한 실험 설계 공공 벤치마크 뿐만 아니라 저자들의 산업 서비스에서 수집한 데이터셋에 대해서도 분석 논문에서 제시하는 방법론이 뛰어남을 보임 2. Method2.1. Preliminary: background 기존 VDU는 BERT의 출현과 함께 Computer Vision(CV)과 Natural Language Processing(NLP)를 결합한 transformer 형태의 방법들이 제안되어 왔음 일부 문제점 대규모의 실제 문서 이미지 데이터셋을 사용해야함 독립적인 OCR 엔진에 의존하여 텍스트를 추출해야함 (VDU 모델의 전체 성능을 유지하려면 OCR 엔진을 유지보수 해야하는 추가적인 노력이 필요함) 2.2. Document Understanding Transformer Encoder visual encoder: $\\mathbf x \\in \\mathbb R^{H\\times W\\times C} \\to \\left( \\mathbf z_i \\mid \\mathbf z_i \\in \\mathbb R^d, 1\\le i \\le n \\right)$ $n$은 feature map size 혹은 이미지 패치 수 $d$는 transformer의 입력 벡터 차원 수 본 연구에서는 Swin Transformer를 사용 Decoder textual decoder: ${\\mathbf z} \\to (\\mathbf y_i)^m_1$ $\\mathbf y_i \\in \\mathbb R^v$: one-hot vector for the token $i$ $v$: the size of token vocabulary $m$: a hyperparameter multilingual BART의 첫 4개의 레이어 사용 Model Input Training phase teacher-forcing 방식으로 학습 진행 Test phase GPT-3와 같이 토큰 시퀀스 생성 각각의 downstream task에 맞게 약간의 special token으로 prompt를 제공하여 출력할 수 있도록 함 prompt?: GPT-3는 약간의 예시를 제공받고 이에 파생되는 결과물을 예측하는데, 이 때 GPT-3가 제공받는 제한된 소스 및 샘플 혹은 예시를 prompt라고 함 input prompt의 예시 Output Conversion downstream task의 종류와 관계없이 최종 결과물로 JSON 파일 형태로 변환하였음 [START_*], [END_*] 토큰을 활용해 파싱 ex.) [START_class],[receipt],[END_class] $\\to$ {&quot;class&quot;: &quot;receipt&quot;} [START_name] 토큰만 출현하고, [END_name] 토큰은 출현하지 않았을 경우 $\\to$ &quot;name&quot; 필드 요소는 출현하지 않은 것으로 처리하였음 모델이 예측한 token sequence에서 JSON 파일로 변환하는 예시2.3. Pre-training Synthetic Document Generator (SynthDoG) 이미지 렌더링 파이프라인은 SynthTIGER (Synthetic Text Image GEneratoR)의 것을 따름 Background ImageNet의 이미지를 샘플링하여 배경으로 사용 texture of Document 문서의 종이 질감은 수집된 사진에서 샘플링함 Text 단어나 문장들은 Wikipedia에서 수집 Layout rule based 랜덤 패턴으로 실제 문서의 레이아웃을 따라하고자 했음 Task SynthDoG가 렌더링한 1.2M개의 합성 문서 이미지로 학습 영어, 일본어, 한국어가 이미지 생성에 사용되었으며 각 언어당 400K개의 이미지를 생성 사전학습 task로는 단순히 이미지 내의 문자를 top-left $\\to$ bottom-right 방향으로 텍스트를 잘 읽어내는 task를 수행 SynthDoG로 렌더링한 이미지 예시2.4. Applications 사전학습 단계에서 how to read를 학습했다면 미세 조정 단계에서는 how to understand를 학습 섹션 2.2. Output Conversion의 형태, JSON 파일 형태를 생성할 수 있도록 미세 조정3. Experiments and Analysis3.1. Downstream tasks and Datasets예측 단계에서도 모든 downstream task에 대해 JSON 파일에 나타난 정보를 읽어들이는 형식으로 진행한다.3.1.1. Document Classification RVL-CDIP dataset 320,000 train / 40,000 validation / 40,000 test grayscale images 16 classes classification 타 연구에서는 token 임베딩에 softmax를 씌워 class label을 예측 본 연구에서는 JSON 파일에서 class label 정보를 읽어들이는 방식으로 예측 3.1.2. Document Parsing 문서 내 layouts, formats, contents를 예측하는 task Indonesian Receipts dataset 1K Indonesian receipt images OCR annotations 포함 Japanese Business Cards dataset (In-Service Data) 20K 일본 명함 이미지 JSON 형태로 구조 정보 포함 Korean Receipts dataset (In-Service Data) 55K 한국 영수증 이미지 JSON 형태로 구조 정보 포함 타 데이터셋보다 복잡한 형태를 갖춤 3.1.3. Document VQA 문서 이미지와 주어진 질문을 입력 받아 올바른 답변을 출력해야하는 task DocVQA dataset 12K document images 내 50K questions 포함 training: validation: test = 39,463: 5,349: 5,188 images test set에 대한 ANLS (Average Normalized Levenshtein Similarity) 수치 제시3.2. Common Settings pre-train on the 1.2M synthetic document images for an epoch mini-batch size: 8 Adam optimizer initial learning rate is selected from $2e-5$ to $8e-5$ learning rate is scheduled 스케줄링 방법은 제시하지 않았음 OCR 의존적인 타 모델에 대한 OCR 세팅 OCR 레이블이 지정되어 있는 데이터셋의 경우 특별한 언급이 없을 때 정답 레이블을 이용 몇몇 task에 대해서는 Microsoft OCR API 사용 document parsing task에서는 CLOVA OCR 중 영수증 및 명함 이미지에 적합한 모델을 적용 3.3. Results3.3.1. Document Classification3.3.2. Document Parsing3.3.3. Document VQA 그룹 구분 첫 번째 그룹: 데이터셋에서 제공하는 OCR 레이블 사용 두 번째 그룹: CLOVA OCR 사용 세 번째 그룹: Microsoft OCR API 사용 성능 성능이 만족스럽지 못하지만 추론 시간 면에서 앞섬 실제 문서 이미지를 사용한 사전학습 마지막으로 10K개의 실제 문서 이미지로 Donut 모델을 사전학습하였을 때, 성능 향상 폭이 매우 컷음 실제 문서 이미지로 학습하는 것이 중요함 5. Concluding Remarks Donut 새로운 형태의 visual document understanding에 대한 end-to-end 방법론 입력 이미지를 곧바로 구조화된 JSON 파일로 매핑 다른 OCR 엔진이나 큰 규모의 실제 문서 이미지에 의존적이지 않음 SynthDoG 사전학습 데이터를 만들 수 있는 합성 문서 생성기 제안 실험 결과 논문에서 제안하는 방법이 cost effective함을 보임 future work document understanding과 관련한 다른 도메인이나 task로 확장 Naver CLOVA AI 팀의 문서 이미지를 다루는 태도를 알 수 있었다.pre-training task가 단순 텍스트 리딩에 그친 것을 마주하고 다소 실망한 감이 있었지만, 확실히 OCR의 도움을 벗어내려는 접근과 output의 형태를 JSON으로 통일하려는 접근 자체는 매우 재미있었다.pre-training task가 단순한 이유 때문인지 Document VQA에 대한 성능은 많이 떨어지는 듯 (애초에 네이버에서 서비스하고 있는 국내 영수증이나 일본 명함에 specific하게 접근한 것으로 보인다). 사실 타 연구들은 텍스트 정보까지 활용하는 multi-modal 모델인 만큼 이에 파생되는 많은 pre-training task를 적용할 수 있겠지만, 이미지만 사용하는 Donut의 경우에는 그 종류가 제한적이라는 면에서 이해는 된다.빨리 github에서 공개된 코드를 마주할 날이 오길 바란다.4. Related Work4.1. Optical Character Recognition 초기 text detection method CNN으로 local segments를 잡고, 휴리스틱한 방식으로 segment들을 결합하여 최종 detection line으로 만드는 형태를 취함 이후에 object detection을 적극적으로 사용 region proposal, bounding box regression 개념 적용, 텍스트 탐지 최근 텍스트 고유의 동질성, 지역성에 집중 텍스트 내부의 구성요소, 즉 sub-text(혹은 알파벳)를 탐지하고 하나의 텍스트 인스턴스로 결합하는 형태 curved, long, oriented 텍스트에 대응할 수 있었음 Text recognition 위에서 탐지한 영역을 잘라내 CNN으로 인코딩-디코딩하여 문자를 해석 4.2. Visual Document Understanding Classification 초기에는 일반적인 이미지처럼 문서 이미지도 처리 (CNN의 활용) BERT의 출현 이후 CV and NLP 조합이 트렌드로 자리잡음 여기에는 OCR 엔진을 통한 텍스트 추출이 필수적 OCR로 추출한 텍스트를 token sequence의 형태로 하여 BERT의 입력으로 제공 (가능하다면 visual feature도 사용) 아이디어는 간단하지만 훌륭한 성능을 보였음 Document parsing 초기 구조화된 형식의 문서에 대한 알맞은 database scheme에 따라 사람이 parsing하는 형태 최근 OCR을 통한 parsing이 활용되고 있음 Visual Question Answering 대부분의 SOTA 방법론은 BERT류의 transformer 구조를 띄고 있음 주어진 이미지에 질문에 대한 답이 나타나지 않을 경우가 있음 위 우려점에 대응하기 위한 생성 기반의 방법도 제안한 연구가 있음 " }, { "title": "[paper-review] DocFormer: End-to-End Transformer for Document Understanding", "url": "/posts/DocFormer/", "categories": "paper-review, Computer Vision", "tags": "deep_learning, document_understanding, multimodal, transformer", "date": "2021-12-23 13:20:00 +0900", "snippet": "Appalaraju, S., Jasani, B., Kota, B. U., Xie, Y., &amp;amp; Manmatha, R. (2021). DocFormer: End-to-End Transformer for Document Understanding. arXiv preprint arXiv:2106.11539.개인적인 논문해석을 포함하고 있으며, 의역 및 오역이 남발할 수 있습니다. 올바르지 못한 내용에 대한 피드백을 환영합니다 :)1. Introduction Visual Document Understanding (VDU) PDF 형태 혹은 이미지 형태인 디지털 문서에 대한 이해 entity grouping, sequence labeling, document classification 문서에서 OCR(Optical Character Recognition)을 수행하는 연구는 많지만 VDU를 위해선 구조와 레이아웃을 모두 반영해야 함 최근 Transformer 구조를 통해 text, spatial, image를 모두 반영하여 이를 해결하려는 연구가 다수 진행 각 연구마다 text, spatial, image의 세 가지 modality를 결합하는 방식이 각자 다름 NLP(Natural Language Processing)에서 그랫듯이 unsupervised 방식으로 사전 학습(pre-training)하고 downstream task에 맞게 미세 조정(fine-tuning)하는 것이 일반적 Cross-modality feature correlation multi-modal 학습은 텍스트를 임의의 범위의 시각적 영역에 매핑하는 과정 “사람”이라는 단어를 설명하는 텍스트 modality와 달리, 이에 해당하는 visual modality는 단순 픽셀 집합에 불과함 때문에 modality간의 feature 상관관계를 모델링하는 것이 어려움 DocFormer Architecture 특징 multi-modal self-attention shared spatial embeddings pre-training with 3 unsupervised multi-modal tasks multi-modal masked language modeling task (MM-MLM) learn-to-reconstruct (LTR) text describes image (TDI) [@ Contributions] 문서 이미지에서 text, visual, spatial features를 결합할 수 있는 새로운 형태의 multi-modal attention layer 제안 Multi-modal feature collaboration을 위한 세 가지 unsupervised pre-training task 제안, 이 중 두 가지는 해당 분야에 새로운 방법: MM-MLM, LTR end-to-end로 학습 가능하고 사전 학습된 object detection 모델을 사용하지 않음 VDU의 4가지 downstream task에 대해 DocFormer는 state-of-the-art 성능을 달성 문서 이미지에서 텍스트를 추출하기 위한 Custom OCR 모델을 사용하지 않음2. Background[@ Grid based methods using CNN] 구조적인 형태가 많은 문서(예: forms, tables, receipts, invoices) invoice(송장) 문서에서 표의 형태를 통해 유형 분류 수행 송장번호, 날짜, 공급업체 이름, 주소, … [@ BERT transformer-encoder based methods] LayoutLM BERT 아키텍처를 문서 이미지에 맞게 수정하여 document understanding 수행 2D spatial coordinate embeddings 1D position text token embeddings visual features and its bounding box coordinates for each word token, obtained using a Faster-RCNN 11M 개의 unlabeled page를 통해 사전학습 LayoutLMv2 LayoutLM을 향상 모델에 visual feature가 입력되는 방식을 개선 text token에 더해주는 대신 개별적이고 독립적인 token으로 처리 pre-training task를 추가 BROS 2D spatial embeddings graph-based classifier text token 사이의 엔티티 상관관계 예측에 사용 [@ Multi-modal transformer encoder-decoder based methods] Layout-T5 a question answering task on a database of web article document images TILT convolutional features + T5 architecture 3. Approach[@ Conceptual Overview] Joint Multi-Modal vision feature와 text feature가 하나의 긴 시퀀스로 결합 cross-modality feature correlation의 측면에서 self-attention의 학습이 어려울 것임 Two-Stream Multi-Modal 각 modality를 담당하는 별도의 branch를 사용 이미지와 텍스트의 결합이 끝에 이르러서야 발생하기 때문에 이상적인 방법이 아님 Single-Stream Multi-Modal visual feature도 text token과 동일한 형태로 만들어 서로 더해줌 vision과 language feature는 서로 다른 유형의 데이터, 단순하게 더하는 것은 부자연스러운 방법 Discrete Multi-Modal (paper’s) visual, spatial feature를 text feature에서 분리하여 반복되는 각각의 transformer layer마다 residual하게 입력 text feature에 대한 visual, spatial feature의 영향이 각 transformer layer마다 달라질 것을 기대하였음 3.1. Model Architecture[@ features extract &amp;amp; processing] Visual features 입력 이미지 $v\\in \\mathbb R^{3\\times h\\times w}$를 ResNet50 CNN으로 feature 추출: $f_{cnn}(\\theta, v)$ ResNet50의 4번째 블록 feature 사용 $v_{l_4} \\in \\mathbb R^{c\\times h_l \\times w_l}$ $v_{l_4} = f_{cnn}(\\theta, v)$ $c=2048, h_l = {h\\over 32}, w_l = {w\\over 32}$ $1\\times 1$ convolution을 통해 채널 $c$ 축소 transformer encoder의 입력 token의 수인 $d$로 축소 $(c, h_l, w_l) \\to (d, h_l, w_l)$ flatten &amp;amp; linear transformation $(d, h_l, w_l) \\to (d, h_l \\times w_l) \\to (d, N)$ $d=768, N=512$ \\[\\bar V = linear(conv_{1\\times 1}(f_{cnn}(\\theta, v)))\\] Language features OCR로 추출한 텍스트 $t$를 tokenizing word-piece tokenizer 사용 $t_{tok} = {[CLS], t_{tok_1}, t_{tok_2}, …, t_{tok_n}}, (n=511)$ 한 문서에서 나타나는 토큰 수가 511보다 큰 경우 나머지는 무시 511보다 작은 경우 나머지 공간은 $[PAD]$ 토큰으로 채움, $[PAD]$ 토큰은 self-attention 계산 과정에서 무시 trainable embedding layer $W_t$로 사영 해당 레이어의 가중치는 LayoutLMv1의 사전학습 가중치를 사용 \\[\\bar T = W_t(t_{tok})\\] Spatial features 각 단어 $k$개 대하여 bounding box 좌표값 $b_k=(x_1, y_1, x_2, y_2, x_3, y_3, x_4, y_4)$ 위 $b_k$만 사용하는 것에 그치지 않고 추가적인 정보를 더 인코딩 bounding box의 높이 $h$, 너비 $w$ 네 모서리와 중점에 대해 word box간의 상대적 거리 \\[A_{rel}=\\left\\{A^{k+1}_{num}-A^k_{num}\\right\\}\\] $A\\in (x, y); num\\in(1, 2, 3, 4, c)$ $c$는 중점 $k$ index는 top-left $\\to$ bottom-right 방향으로 증가 즉, $k+1$번째 word box는 $k$번째 word box의 우하향 방향에 위치 $P^{abs}$: 1D positional encoding $x$와 $y$, visual feature $\\bar V$와 language feature $\\bar T$에 대해 각각 따로 임베딩 행렬을 두고 따로 학습하여 계산 spatial 영향은 modality 별로 각각 발생할 것이라고 생각 \\[\\bar V_s = W^x_v(x_1, x_3, w, A^x_{rel}) + W^y_v(y_1, y_3, h, A^y_{rel}) +P^{abs}_v,\\]\\[\\bar T_s = W^x_t(x_1, x_3, w, A^x_{rel}) + W^y_t(y_1, y_3, h, A^y_{rel}) +P^{abs}_t\\][@ Multi-Modal Self-Attention Layer] a transformer encoder $f_{enc}$ outputs a multi-modal feature representations $\\bar M$ of the same shape as each of the input features ($d=768, N=512$) $\\eta$는 transformer의 파라미터 \\[\\bar M = f_{enc}(\\eta, \\bar V, \\bar V_s, \\bar T, \\bar T_s)\\] $i$번째 입력 토큰에 대한 $l$번째 transformer layer의 multi-modal feature\\[\\bar M^l_i = \\sum_{j=1}^L {\\exp(\\alpha_{ij})\\over \\sum_{j^\\prime=1}^n \\exp(\\alpha_{ij^\\prime})}(x^l_j W^{V, l})\\]\\[\\alpha_{ij} = {1\\over \\sqrt d} (x^l_i W^{Q, l})(x^l_j W^{K, l})^T\\]generalization을 위한 $\\sqrt d$와 $l$번째 레이어의 것이라는 표기를 제거하여 간단하게 나타내면,\\[\\alpha_{ij} = (x_i W^{Q})(x_j W^{K})^T\\]여기서 visual feature와 text feature에 대해 각각 다른 연산 흐름을 갖는다. attention distribution for visual feature $x^v$는 각 토큰의 visual feature query 1D relative attn., key 1D relative attn., visual spatial attn.을 모두 더해주어 local feature를 포착하는데 힘을 쏟음 attention distribution for text feature 위 visual feature에서의 연산과 거의 비슷함 \\[\\alpha_{ij}^t = (x_i W^Q_t)(x_j W^K_t) + (x_i W^Q_t\\alpha_{ij}) + (x_j W^K_t\\alpha_{ij}) + (\\bar T_s W_s^Q)(\\bar T_s W_s^K)\\] 단, 입력 토큰 $x_i$는 이전 transformer layer의 output인 multi-modal feautre (만약 첫 번째 레이어라면 word embedding일 것) 모든 transformer layer에 visual feature는 모두 동일하게 입력이 주어지지만, text feature는 transformer layers stack을 따라 흐르기 때문이다. (아래 그림 참조) visual feature는 각 transformer layer에 따로따로 투입, text feature(word embedding)은 transformer layers stack을 타고 흐름 multi-modal feature output $l$번째 transformer layer의 output은 $\\bar M_l = \\hat V_l + \\hat T_l$ 본 논문에서 강조하는 점 중 하나는 Spatial embeddings를 위한 attention 가중치를 공유하고 있다는 것인데, 이는 위 attention 연산 흐름을 나타내는 그림에 spatial Query, Key matrix가 visual과 text 모두 분홍색으로 같은 색으로 칠하는 것으로 강조하고 있고, 아래 실험(4. Experiments)을 통해 이에 대한 효과를 입증하고 있다.3.2. Pre-trainingMulti-Modal Masked Language Modeling (MM-MLM) 기존 BERT에서 소개되었던 기존 Masked Language Modeling (MLM) task를 개선 텍스트 시퀀스 $t$에서 일부를 마스킹한 corrupted 시퀀스 $\\tilde t$ multi-modal feature embedding인 $\\bar M$을 바탕으로 원래 시퀀스 $t$로 복구하는 task LayoutLMv2와 같은 연구에서는 마스킹한 텍스트에 해당하는 이미지 영역도 같이 마스킹하였으나, 본 연구에서는 이미지 영역에 대한 마스킹을 하지 않음 마스킹 비율은 BERT의 것과 동일 Cross-entropy loss로 학습Learn To Reconstruct (LTR) MM-MLM task의 이미지 버전 (image reconstruction) multi-modal feature embedding $\\bar M$을 shallow decoder로 투입, 원래 입력 이미지로 복구 auto-encoder를 통한 image reconstruction과 비슷함 smooth-L1 loss로 학습Text Describes Image (TDI) 위 두 사전학습 task(MM-MLM, LTR)가 local features에 집중하였던 것과 다르게 global features에 집중한 task multi-modal feature embedding $\\bar M$을 입력으로 단일 linear layer를 통과해 binary classification 수행 한 배치 내에서 80%는 올바른 text-image pair, 20%는 잘못된 text-image pair를 구성하도록 하였음 단, 잘못된 text-image pair가 구성될 경우 LTR task의 loss는 무시 binary cross-entropy로 학습final pre-training loss\\[L_pt = \\lambda L_{MM-MLM} + \\beta L_{LTR} + \\gamma L_{TDI}\\]\\[\\lambda=5, \\beta=1, \\gamma=5\\]4. Experiments 개요 모든 실험에 걸쳐 training set을 통해 fine-tuning test나 validation set에 대한 결과 데이터셋 별로 별도의 특화된 hyper-parameter tuning은 진행하지 않았음 Models 기존 transformer encoder model에 대한 terminology를 따름 12개의 transformer layer가 있는 경우 -base 모델 (768 hidden state and 12 attention heads) 24개의 transformer layer가 있는 경우 -large 모델 (1024 hidden state and 16 attention heads) text and spatial features만 사용하는 모델에 대해서도 실험 DocFormer의 유연성 강조 visual features가 포함됨으로써 더 나아지는 모습 강조 4.1. Sequence Labeling task FUNSD dataset form understanding을 위한 데이터셋 문서 내 각 요소들의 entity 유형 및 클래스가 레이블링 되어 있음 (아래 그림 참조) 149 train / 50 test pages Sequence labeling task 각 구성 요소의 클래스를 예측하는 task LayoutLMv2가 11M의 데이터로 사전학습한 것에 반해 DocFormer는 5M의 데이터로 학습한 결과임을 강조하고 있음Qualitative result4.2. Document Classification task RVL-CDIP dataset 320,000 train / 40,000 validation / 40,000 test grayscale images 16 classes text 및 layout 정보는 Tesseract OCR로 도출 4.3. Entity Extraction Task OCR로 추출한 정보들(text, spatial)과 문서 이미지(image)를 조합 각 entity의 정보를 예측하는 task CORD dataset 영수증 이미지 데이터셋 30 fields under 4 categories Kleister-NDA dataset legal NDA documents 4개의 fixed labels를 추출하는 데이터셋 4.4. More ExperimentsShared or Independent Spatial embeddings? vision and language modalities 간의 sharing spatial embedding의 효과 sharing spatial embedding을 통해 feature correlation을 학습할 수 있다고 주장Do our pre-training tasks help? 비교적 학습 데이터 수가 적은 데이터셋에서는 사전 학습이 필요한 것으로 보임Does a deeper projection head help? 위 실험 결과들은 모두 downstream task에 단일 linear layer만으로 예측한 결과임 ($fc\\to ReLU \\to LayerNorm \\to fc$) 형태의 더 깊은 projection head를 사용하는 것의 효과에 대한 실험 학습 데이터가 비교적 많은 데이터셋에 대해 효과를 보였음4.5. Ablation Studypre-training task의 유무에 따른 결과 비교DocFormer의 구성요소 유무에 따른 결과 비교5. Conclusion DocFormer 제안, 다양한 Visual Document Understanding tasks에 대한 end-to-end trainable transformer based model multi-modal attention 방법론 제안 두 가지 새로운 vision-plus-language 사전학습 task 제안 실험을 통해 DocFormer가 비교적 규모가 작은 모델임에도 4가지 데이터셋에 대해 SOTA 성능을 달성 Future works multi-lingual 환경에 대한 DocFormer 연구 info-graphics, maps, web-pages와 같은 다양한 형태의 문서에 대한 연구 Microsoft의 LayoutLM를 잇는 Amazon의 document를 위한 transformer 모델이다.LayoutLM과 비교했을 때 사용하는 문서의 요소들이나 방법들이 크게 다르지는 않는 것 같다. 다만 그 형태를 조금씩 바꾸었음에도 11M $\\to$ 5M으로 사전학습에 사용한 데이터셋 수를 크게 줄인 것은 흥미로운 부분이다." }, { "title": "[paper-review] Learning to Understand Traffic Signs", "url": "/posts/Learning_to_Understand_Traffic_Signs/", "categories": "paper-review, Computer Vision", "tags": "deep_learning, ocr, form_understanding, traffic_sign_understanding, CTSU_Dataset", "date": "2021-12-15 12:10:00 +0900", "snippet": "Guo, Y., Feng, W., Yin, F., Xue, T., Mei, S., &amp;amp; Liu, C. L. (2021, October). Learning to Understand Traffic Signs. In Proceedings of the 29th ACM International Conference on Multimedia (pp. 2076-2084).개인적인 논문해석을 포함하고 있으며, 의역 및 오역이 남발할 수 있습니다. 올바르지 못한 내용에 대한 피드백을 환영합니다 :)1. Introduction 최근 교통 표지판(traffic sign)에서 텍스트나 기호를 인식하는 task의 눈에 띄는 발전이 있었음 각각의 요소를 따로 인지하는 것은 교통 표지판을 이해하는 첫 단계에 불과 본 논문에서는 “traffic sign understanding”이라는 새로운 task를 소개 교통 표지판에 포함된 요소들을 인식하고 그 요소들 간의 관계를 파악하여 “semantic description”(&amp;lt;key: value&amp;gt;의 형태)을 생성하는 것 key: 표시 정보 (예: 현재 위치, 차선 번호, 전방 오른쪽 방향) value: 특정 내용 (예: 장소 이름, 도로 이름, 설명 단어) 대부분의 교통 정보가 “indicative information + content”의 형태로 구성 자율주행, positioning assistance, map correction과 같은 애플리케이션에 활용될 수 있음[@ Traffic sign understanding] Traffic sign understaning은 크게 세 가지 subtask로 구성 텍스트 및 기호의 위치와 semantic 정보를 추출하는 Component detection task 텍스트와 기호의 인식 결과를 어떻게 조합할 것인지, 구성요소들 간의 관계를 모델링할 수 있는 Relation reasoning task Graph Convolutional Network과 같은 관계 예측 모델을 사용 교통 표지판의 다양한 유형을 분류하는 Sign classification task 다양한 유형의 표지판에 따라같은 기호가 다른 의미를 나타낼 수 있음 예) 차선 정보 표지판(그림 1(b))에서의 윗방향 화살표는 “직진”을 의미하고 안내 정보 표지판(그림 1(a))에서는 “전면”을 의미한다. 교통 표지판의 유형을 예측함으로써 기호 간의 관계를 파악하는데에도 도움을 줄 수 있을 것 [@ CASIA-Tencent Chinese Traffic Sign Understanding Dataset (CTSU Dataset)] 복잡한 형태의 교통 표지판과 그 semantic description 레이블을 포함하는 첫 번째 데이터셋 실제 dashcam 영상에서 잘라낸 5000개의 교통 표지판 이미지를 포함 이미지 description, 기호, bounding box, 이미지 내의 텍스트, 기호의 카테고리 요소들 간의 관계에 대한 레이블도 지정 [@ Contributions] 교통 표지판 이미지 및 레이블 정보를 포함하는 데이터셋 CTSU 제안 다양한 유형의 교통 표지판을 이해하고 description을 생성하기 위한 새로운 unified 모델 제안 위 모델의 접근 방식이 traffic sign understanding에 효과적임을 실험을 통해 밝힘2. Related Work2.1. Traffic Sign Recognition 이전 교통 표지판에 관한 연구는 텍스트의 detection에 관한 연구가 주를 이룸 Peng et al. proposed a two-stage cascade detection deep learning model, which used improved EAST for text line detection, and changed the size of feature maps to suit the text size of traffic signs. Hou et al. introduced an Attention Anchor Mechanism (AAM), which is used to weight the bounding boxes and anchor points to detect text in scene and traffic signs. Traffic sign에 대한 공개 datasets는 주로 원형이나 삼각형 형태의 몇몇 간단한 형태의 분류에 대해서만 존재했음 German Traffic Sign Recognition Benchmark - citation DFG Traffic Sign Data Set - citation Chinese Traffic Sign Database (CTSD) - citation 위 데이터셋들은 복잡한 교통 표지판 속 요소들 사이의 관계에 대한 정보가 없음 2.2. Scene Understanding 이미지를 기반으로 semantic description을 생성하는 task를 총칭하여 image caption이라 할 수 있음 Recurrent Neural Network (RNN) 기반 CNN만으로는 맥락 정보를 반영할 수 없음 Rowan et al. first used LSTM to predict the object categories and then sent the object features and category information into LSTM for relationship prediction. Li et al. proposed a network that leveraged the feature of region, phrase, and object to generate scene graph and caption. 위 연구들은 RNN이나 LSTM을 사용해 맥락 정보를 반영했음 하지만, 공간적 정보(spatial information)을 반영하지 못함 Graph Neural Network (GNN) 기반 더 나은 scene graph 구조를 생성하기 위해 GNN 기반의 모델들이 제안되었음 Yang et al. proposed an attentional Graph Convolutional Network (aGCN) that uses contextual information to better reason about the relationship between objects 이러한 방법들은 이미지 내의 맥락 정보(contextual information)를 포함할 수 있지만, 위치정보나 의미적 정보(semantic information)을 반영할 수 없음 3. CTSU Dataset– 중략 –3.3. Evaluation Metric Image caption에서 이용되던 척도들은 자연어 처리에서 비롯된 척도 주로, 정답 값과 sequence matching의 형태이거나 유사도를 측정하는 형태 교통 표지판의 semantic description은 문법적인 규칙을 신경쓰지 않아도 되는 형태이므로 비효율적임 [@ Information Matching (IM)] 먼저 indicative information(&amp;lt;key: value&amp;gt;에서 key)에 대해 ground truth와 예측 값을 매칭 각각 예측된 predicted description에 오직 하나의 ground truth description을 매칭할 수 있음 매칭 결과에 따라 각각 specific content(&amp;lt;key: value&amp;gt;에서 value)가 일치할 때 True Positive를 부여 위 결과에 따라 각 이미지당 하나씩의 recall, precision, F1-Measure를 계산3.4. Statistics and Analysis CTSU Dataset contains 5,000 traffic signs 16,463 &amp;lt;key: value&amp;gt; descriptions 31,536 relationship instances 43,722 components, including 18,280 texts [@ Category Statistics] 교통 표지판 클래스 별로 복잡도(Complexity)를 계산하고 이를 해석함 클래스 $i$에 대한 Complexity $C(c_i)$는 각 클래스 내 샘플들의 components 수($a_{c_i}$)와 각 샘플 내의 정보 엔트로피(information entropy; $H(c_i)$)의 곱으로 정의 \\[\\begin{matrix}C(c_i) = a_{c_i}*H(c_i) = {1\\over \\lvert c_i \\rvert} \\sum_{p\\in c_i} N_p * {1\\over\\lvert c_i \\rvert} \\sum_{p\\in c_i} H_p\\\\H_p = -P_{pa}\\log P_{pa}-P_{pp}\\log P_{pp}-P_{pn}\\log P_{pn}\\\\\\\\\\text{where } \\lvert c_i \\rvert \\text{ is the total number of images of class } i\\\\N_p \\text{ is the total number of components in a particular sign } p\\\\P_{pa}, P_{pp}, \\text{and } P_{pn} \\text{ are the frequency of association relation, pointing relation, }\\\\\\text{and no relation in the sign } p\\end{matrix}\\] CTSU 데이터셋은 샘플 수가 많은 클래스에서는 복잡도가 높고, 샘플 수가 비교적 적은 클래스에서는 복잡도가 낮음sign 클래스 당 복잡도 분석 결과[@ Detection Imbalance] 교통 표지판 내의 구성요소들의 분포도 불균형 ‘text’ 요소는 수가 많고 다른 요소들은 비교적 수가 적음 이를 잘 다룰 수 있어야 함 [@ Description Bias] indicative information(&amp;lt;key: value&amp;gt;에서 key)을 담당하는 ‘text’ 요소는 20%를 미치지 못함 ‘text’ 요소가 전체 요소의 40%를 차지함에도 불구하고 이 때문에 indicative information의 생성에 ‘text’의 “기여”가 과장될 것 과장된 ‘text’의 영향을 줄일 수 있도록 모델링해야 함 indicative information을 담당하는 요소들의 분포, ‘text’ 요소는 제외함, 가장 많은 수를 차지하는 ‘Crossroad’의 수도 ‘text’ 요소에 비하면 수가 적은 편4. Method논문에서 제안하는 모델의 전체 프레임워크4.1. Component Detection 교통 표지판의 요소들을 object detector를 통해 탐지해내는 과정 single-stage anchor-freed object detection model인 FCOS를 사용 물체의 중앙을 기준으로 bounding box를 예측하는 FCOS 알고리즘 특성상 두 물체가 겹쳐서 존재하면 잘 탐지하지 못하는 특성이 있음 논문에서는 ‘texts’, ‘symbols’, ‘arrowheads’를 각각 따로따로 탐지하는 detection head를 두어 이를 해결했음 \\[L_{DET} = L_{FCOS_T} + L_{FCOS_S} + L_{FCOS_A}\\]\\[\\begin{matrix}\\text{where } L_{FCOS_T}, L_{FCOS_S}, \\text{ and } L_{FCOS_A} \\text{ are the single head losses of}\\\\\\text{texts, symbols, and arrowheads}\\end{matrix}\\] 두 요소가 겹쳐서 존재하는 경우4.2. Relation Reasoning 그래프 구조 구성요소(‘texts’, ‘symbols’, ‘arrowheads’)를 그래프의 node 이 요소들의 관계를 그래프의 edge 관계를 일부 node들에 대해서만 정의할 수 없음 두 요소간 물리적 거리가 가깝다고 해서 꼭 관계를 형성하지는 않음 따라서, 모든 node들이 연결된 그래프 구조로 초기화 관계를 형성하고 있음에도 요소간 거리가 멀리 떨어진 경우가 많음: 때문에 논문에서는 fully connected graph 구조를 사용[@ Feature generation] node feature 및 edge feature를 생성하는 방법 RoI feature: object detector가 만든 bounding box에 RoIAlign을 적용 Position Mask: object detector가 만든 bounding box 위치에 masking한 이미지를 fully connected layer에 연결하여 feature 생성 Semantic Encoding: object detector가 예측한 클래스 정보를 fully connected layer에 연결하여 feature 생성 (클래스 정보로는 edge를 표현할 수 없기 떄문에 edge에 대한 Semantic Encoding은 없다) annotations node feature: $F_N = V_{ds} + V_{ps} + V_{ss}=\\text{RoI feature} + \\text{Position feature} + \\text{Semantic feature} \\in R^D$ edge feature: $F_E = V_{du} + V_{pu}=\\text{RoI feature} + \\text{Position feature} \\in R^D$ $D$는 각 feature 차원 수 각 feature가 업데이트 되는 것을 $\\prime$을 붙임으로써 표현 [@ Graph Attention Network (GAT)] GAT는 Graph Network에 Attention 메커니즘을 추가하여 node간 edge에 중요도를 부여하여 Graph 구조 정보를 모델링 Attention coefficient node $N_i$에 연관된 모든 node 및 edge에 대해 영향력이 있는 정도(중요도)를 계산 edge $E_{ji}$는 node $N_j$에서 node $N_i$로의 관계를 의미 $f_A: R^{2D} \\to R^D$: fully connected layer $\\parallel$: concatenation operation \\[\\alpha_{ji} = {\\exp(\\sigma(f_A(F_{E_{ji}} \\parallel F_{N_i})))\\over \\sum_{k\\in \\mathbb N_i} \\exp(\\sigma(f_A(F_{E_{ki}} \\parallel F_{N_i})))}\\] node feature의 업데이트 edge feature에 Attention coefficient를 element-wise 곱을 하고 node feature를 concat하여 fully connected layer에 태움 $\\otimes$: element-wise/hadamard product $\\sigma$: activation function, 논문에서는 leaky ReLU사용 $f_N: R^{2D} \\to R^D$: fully connectec layer \\[F^\\prime_{N_i} = \\sigma\\left(f_N\\left(\\left(\\sum_{j\\in\\mathbb N_i}\\alpha_{ji} \\otimes F_{E_{ji}}\\right) \\parallel F_{N_i}\\right)\\right)\\] edge feature의 업데이트 두 노드 $N_i$와 $N_j$에 Attention coefficient를 element-wise 곱을 하고 더한 후 edge feature를 concat하여 fully connected layer에 태움 $f_E: R^{2D} \\to R^D$: fullly connected layer \\[F_{E_{ji}}^\\prime = \\sigma\\left(f_E\\left((\\alpha_{ji}\\otimes F_{N_j} + \\alpha_{ji} \\otimes F_{N_i}) \\parallel F_{E_{ji}}\\right)\\right)\\] 학습 과정에서는 ground truth bounding box를 통해 학습하여 GAT가 더 잘 수렴할 수 있게하였음4.3. Sign Classification 교통 표지판의 관계를 파악하는데 교통 표지판의 유형을 분류하는 것이 도움이 될 수 있음 backbone feature map 중 가장 작은 feature($FP_s$)와 GAT를 거친 node feature($F_{N}$)들을 더하여 교통 표지판 유형 예측에 사용\\[\\begin{matrix}F_{FPN} = f_{FPN}(Conv(Resize(FP_s))),\\\\F_{GAT} = \\sum_i f_{GAT}(F_{N_i})),\\\\f(F_{FPN}, F_{GAT}) = f_{CLS}(F_{FPN} + F_{GAT})\\end{matrix}\\][@ Loss Function] section 4.1, 4.2, 4.3의 요소들의 loss를 선형 결합함\\[\\begin{matrix}L=L_{DET} + \\lambda_{REL}L_{REL} + \\lambda_{CLS}L_{CLS}\\\\\\text{where } \\lambda_{REL} \\text{ and } \\lambda_{CLS} \\text{ are set to 1.0 for training.}\\end{matrix}\\]4.4. Semantic Description Inference objects detected box를 필터링하는 threshold = $0.2$ relationship을 필터링하는 relationship threshold = $0.5$ text box에 대해서는 사전학습된 OCR모델로 텍스트를 추출함 heuristic method to generate semantic descriptions relation 예측 결과를 통해 relation trees를 구성 symbol에 따라 의미 태그 부여 교통 표지판 유형에 따라 symbol에 의미를 부여 &amp;lt;key: value&amp;gt; structure 구성 대부분 value의 위치에 명확한 요소들이 위치함 (예. 지명, 지하철역명 등등) 교통 표지판 유형에 따라 key의 위치에 명확한 요소들이 위치하는 경우도 있음 (예. lane information sign에서는 차선 번호(lane 1, lane 2, …)가 key에 위치함) 따라서, 해당 작업도 교통 표지판 유형에 따라 부여 5. Experiments CTSU 데이터셋에 대해 실험 수행 학습과 검증 데이터셋에 대한 설명이 없는 것으로 봐서 아래 실험 결과는 모두 학습 데이터에 대한 실험 결과인 것으로 보인다. 5.1. Implementation Details backbone: ImageNet에 사전학습한 ResNet-50, Deformable Convolutional Network (DCN) 테크닉 사용 GAT의 레이어 수는 5개 50 epochs learning rate 0.01 epoch 35, 45에 각각 0.1씩 감소 batch size 8 on 2 GPUs5.2. Ablation Studies[@ Multi-head Detection] FCOS 특성 상 겹쳐있는 물체를 포착하지 못하는 문제를 밝힘 약 1%정도의 성능 향상 Faster R-CNN도 겹쳐있는 물체에 대한 해결책이 될 수 있음 사전 정의된 anchor 세팅이 성능에 제한이 되었음 약 0.7% 정도의 성능 차이 기존 FCOS(a)와 논문에서 제안하는 Multi-head detection 모델(b)과의 차이[@ Semantic Encoding]semantic feature를 제외했을 때, arrowhead가 아닌 symbol에 relation이 할당됨(c)5.3. Comparison with the state-of-the-art6. Conclusion intelligent transportation을 위한 traffic sign understanding이라는 task를 새롭게 제안 bounding boxes, relations, semantic description 레이블을 할당한 CTSU Dataset 제안 Component detection, relation reasoning, sign classification, semantic description generation의 multi-task 학습 프레임워크를 제안" }, { "title": "[paper-review] Mask TextSpotter v3: Segmentation Proposal Network for Robust Scene Text Spotting", "url": "/posts/Mask_TextSpotter/", "categories": "paper-review, Computer Vision", "tags": "deep_learning, ocr, scene_text, segmentation", "date": "2021-11-22 18:20:00 +0900", "snippet": "Liao, M., Pang, G., Huang, J., Hassner, T., &amp;amp; Bai, X. (2020). Mask textspotter v3: Segmentation proposal network for robust scene text spotting. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XI 16 (pp. 706-722). Springer International Publishing.개인적인 논문해석을 포함하고 있으며, 의역 및 오역이 남발할 수 있습니다. 올바르지 못한 내용에 대한 피드백을 환영합니다 :)1. Introduction[@ Scene text spotter] 최근 scene text spotting task에는 end-to-end 학습 방식의 딥러닝이 많이 적용되고 있음 좋은 scene text spotting 아래 세 가지 능력을 갖추어야 함 Rotation robustness: 텍스트가 이미지 축에 잘 정렬되어 있지 않았을 때에 강건함 Aspect ratio robustness: non-Latin scripts에는 주로 word 단위보다는 긴 텍스트 라인으로 텍스트 인스턴스가 구성되어 있는데, 이처럼 다양한 텍스트 인스턴스 종횡비에 강건함 Shape robustness: Logo같은 텍스트에 주로 나타나는 일반적이지 않은 모양의 텍스트에 강건함 [@ Mask TextSpotter series] Region Proposal Network(RPN)의 한계 manually pre-designed anchors를 사용하므로 극단적인 종횡비를 가지는 텍스트 인스턴스를 포착하기 쉽지 않음 RPN이 생성하는 axis-aligned rectangular proposals는 그 box안에 인접한 다른 텍스트 인스턴스들이 함께 포함되는 경우가 많음 anchor-box, 출처2번 한계의 예시 선행연구, Mask TextSpotter v1, Mask TextSpotter v2에서는 Region Proposal Network (RPN)을 통해 RoI feature를 추출하고 이 RPN이 제안한 proposal box들에 detection과 recognition을 수행 rotation robustness, shape robustness를 갖출 수 있는 RPN을 제안 But, aspect ratio robustness까지 모두 갖추지는 못했음 [@ Segmentation Proposal Network (SPN)] Segmentation Proposal Network을 통해 정확한 polygonal 형태의 proposal 표현을 할 수 있음 더 나아가 정확한 형태의 proposal 표현을 통해 hard RoI masking 방법을 적용, 인접한 텍스트 인스턴스나 배경 노이즈의 간섭을 억제할 수 있음[@ Contributions] Segmentation Proposal Network(SPN) 극단적인 종횡비나 특이한 형태를 가진 텍스트 인스턴스를 정확하게 포착할 수 있는 SPN를 제안 hard RoI masking SPN이 생성해낸 proposal에 적용하여 배경 픽셀이나 인접한 다른 텍스트 인스턴스가 유발할 수 있는 노이즈를 제거 Mask TextSpotter v3 rotation, aspect ratio, shape에 모두 robust한 text spotter 모델 다양한 벤치마크에 높은 성능을 보임 2. Related Work[@ Two-stage scene text spotting] Wang et al. tried to detect and classify characters with CNNs. Jaderberg et al. proposed a scene text spotting method proposal generation module a random forest classifier to filter proposals a CNN-based regression module for refining the proposals a CNN-based word classifier for recognition TextBoxes and TextBoxes++ combined thier proposed scene text detector with CRNN Zhan et al. proposed to apply multi-modal spatial learning into the scene text detection and recognition system.[@ End-to-end trainable scene text spotting] Mask TextSpotter v1 is the first end-to-end trainable arbitrary-shape scene text spotter consisting of a detection module based on Mask R-CNN and character segmentation module for recognition. Mask TextSpotter v2 extends Mask TextSpotter v1 by applying a spatial attention module for recognition spatial attention module: character 수준의 공간적 왜곡을 바로 잡아줄 수 있는 모듈 Qin et al. also combine a Mask R-CNN detector and an attention-based recognizer to deal with arbitrary-shape text instances Qin et al.의 연구에선 mask map을 recognition에 성능향상을 위해 RoI feature에 대해 RoI masking을 수행 하지만, mask map을 생성하는 데 RPN을 사용하기 때문에 proposals을 생성하는 데 부정확한 결과를 만들어낼 수 있음 (Introduction에서 밝힌 RPN의 단점) Xing et al. propose to simultaneously detect/recognize the characters and the text instances, using the text instance detection results to group the characters. TextDragon detects and recognizes text instances by grouping and decoding a series of local regions along with their centerline[@ Segmentation-based scene text detectors] Zhang et al. first use FCN to obtain the salient map of the text region then estimate the text line hypotheses by combining the salient map and character components. Finally, another FCN predicts the centroid of each character to remove the false hypotheses. He et al. propose Cascaded Convolutional Text Networks (CCTN) for text center lines and text regions. PSENet adopts a progressive scale expansion algorithm to get the bounding boxes from multi-scale segmentation maps. DB proposes a differentiable binarization module for a segmentation network. 본 논문에서는 기존 Segmentation-based scene text detector에 비해 다양한 단서와 추가적인 모듈을 결합하여 detection task를 수행함 proposal generation에 segmentation network을 사용한다는 점을 강조할 수 있음 3. MethodologyMask TextSpotter v3 consists of … a ResNet-50 backbone, a Segmentation Proposal Network (SPN) for proposal generation a Fast R-CNN module for refining proposals a text instance segmentation module for accurate detection a character segmentation module and a spatial attentional module for recognition추가적으로 Mask TextSpotter v3는 RoI feature의 형태를 다각형, polygonal 형태로 생성하기 때문에 정확한 detection을 할 수 있고 recognition의 성능에도 좋은 영향을 줄 수 있음3.1. Segmentation proposal network U-Net의 형태를 사용, 다양한 크기의 다양한 feature를 사용 SPN의 output $F$는 위 feature들을 결합하여 ${H\\over 4} \\times{W\\over 4}$ 크기로 생성됨 $H, W$는 각각 입력 이미지의 높이, 너비 $F$를 통해 Segmentation을 수행하여 최종적으로 $1\\times H \\times W$ 크기의 predict segmentation map $S$를 생성함 최종 Segmentation 수행 모듈의 구조[@ Segmentation label generation]Segmentation 성능 향상을 위해 text instance들의 크기를 축소시킴으로써 인접한 text instance들을 분리하려는 테크닉이 일반적임 Vatti clipping algorithm $d$ pixel 만큼 텍스트 영역을 축소시키는 테크닉 the offset pixel $d=A(1-r^2)/L$ $A$는 텍스트 인스턴스 polygon의 면적 $L$은 텍스트 인스턴스 polygon의 둘레 $r$ is the shrink ratio [@ Proposal generation] 먼저 Segmentation map $S$를 이진화하여 binary map $B$를 계산\\(B_{i,j} = \\begin{cases}1 &amp;amp; \\mathrm{if} \\space S_{i,j} \\ge t,\\\\0 &amp;amp; \\mathrm{otherwise.}\\end{cases}\\\\ \\mathrm{Here,} \\space t=0.5\\) 이후 _Vatti clipping algorithm_을 원복\\[\\begin{matrix}\\hat d = \\hat A \\times \\hat r / \\hat L\\\\\\mathrm {Here,} \\space \\hat r = 3.0\\end{matrix}\\]3.2. Hard RoI masking 직사각형의 binary map $B$에서 각 text instance RoI feature와 크기가 동일한 polygon mask $M$을 생성\\(M=\\begin{cases}1, &amp;amp; \\mathrm{if \\space in \\space the \\space polygon \\space region}\\\\0, &amp;amp; \\mathrm{else}\\end{cases}\\) RoI feature $R=R_0 * M$, $*$는 element-wise multiplication hard RoI masking을 통해 배경 영역이나 인접한 다른 텍스트 인스턴스들의 방해를 억제할 수 있음 결과적으로 detection과 recognition 모두에 성능 향상을 도모할 수 있음3.3. Detection and recognition text detection and recognition의 설계는 Mask TextSpotter v2의 것과 동일 Mask TextSpotter v2가 당시 최고의 detection, recognition 모델임 RPN-based scene text spotter와 (본 논문에서 제안하는) SPN-based scene text spotter의 공정한 비교를 위함 hard RoI masking을 거친 masked RoI features는 Fast R-CNN의 입력으로 주어지고 localization을 가다듬고 character segmentation module과 spatial attentional module로 recognition을 수행3.4. Optimization\\(L = L_s + \\alpha_1L_{rcnn}+\\alpha_2L_{mask}\\) $L_{rcnn}$ is defined in Fast R-CNN $L_{mask}$ is defined in Mask TestSpotter v2, consisting of a text instance segmentation loss, a chracter segmentation loss, and a spatial attentional decoder loss. $L_s$ indicates the SPN loss SPN loss엔 Dice loss를 사용\\(I=\\sum(S*G); \\space U=\\sum S + \\sum G; \\space L_s=1-{2.0\\times I\\over U}\\) $S$ is the segmentation map, $G$ is the target map, $*$ represents element-wise multiplication. $\\alpha_1=\\alpha_2=1.0$4. Experiments4.1. Datasets SynthText 800K 텍스트 이미지를 포함한 합성 데이터셋 annotations for word/character bounding boxes and text sequences. Rotated ICDAR 2013 dataset (RoIC13) ICDAR 2013 데이터셋에서 $15^\\circ, 30^\\circ, 45^\\circ, 60^\\circ, 75^\\circ, 90^\\circ$를 회전시켜 직접 제작 ICDAR 2013의 텍스트 인스턴스들이 모두 수평적으로 정렬되어 있기 때문에 이 특성을 이용해 텍스트의 회전 방향에 대한 강건함(rotation robustness)을 테스트 할 수 있음 MSRA-TD500 영어와 중국어로 구성된 multi-language scene text detection benchmark 많은 수의 텍스트 인스턴스가 극단적인 종횡비로 구성됨 recognition annotations가 포함되지 않음 Total-Text 다양한 형태의 텍스트 인스턴스, 가로세로 방향의 인스턴스, 곡선 형태의 텍스트들이 포함 polygonal bounding box와 transcription annotations 포함 ICDAR 2015 (IC15) quadrilateral bounding boxes로 레이블 구성 대부분의 이미지가 저해상도이고 작은 텍스트 인스턴스를 포함 4.2. Implementation details[@ Mask TextSpotter v2] 공정한 비교를 위해 같은 학습 데이터와 Data augmentation 과정을 거침 한 가지 차이점 SPN이 더 극단적인 형태의 text instance에도 강건하기 때문에 rotation 각도 범위를 $[-30^\\circ, 30^\\circ]$에서 $[-90^\\circ, 90^\\circ]$로 확장 [@ hyper-parameters &amp;amp; training details] optimizer: SGD with a weight decay of 0.001 and momentum of 0.9 SynthText로 사전학습 수행 후 데이터셋을 조합하여 미세조정 수행 SynthText: ICDAR 2013: ICDAR 2015: SCUT: Total-Text $= 2:2:2:1:1$ 학습에 사용되는 batch를 위 비율로 구성, 즉 batch를 8로 구성 pre-training learning rate는 0.01로 시작 100K, 200K iteration에서 각각 $1/10$씩 감소 fine-tuning 학습 시와 동일한 환경을 사용 initial learning rate만 0.001로 시작 pre-training, fine-tuning 모두 250K번째의 가중치를 사용했음 4.3. Rotation robustness자체적으로 구축한 RoIC13에 테스트 수행[@ Detection task][@ End-to-end recognition task] Evaluation protocol of IC15 (출처) ground truth bounding box와 50% 이상 겹치고 text 내용이 일치할 경우 true positive 일부 작은 텍스트에 대해 “do not care”의 레이블이 되어있는 경우가 있음 ground truth bounding box와 50% 이상 겹치는 경우, 혹은 찾아내지 못하는 경우에도 evaluation에 포함되지 않는다. 4.4. Aspect ratio robustness 극단적인 종횡비의 text instance가 많이 출현하는 MSRA-TD500 데이터셋에 대해 평가 진행 recognition annotation이 없기 때문에 detection task에 대한 평가만 수행4.5. Shape robustness horizontal, oriented, and curved 형태의 다양한 형태가 많이 포함된 Total-Text에 대해 평가 진행4.6. Small text instance robustness TextDragon이 Strong, Weak task에서는 가장 좋은 성능을 보였지만 일반적인 경우라고 볼 수 있는 Generic task에서 가장 좋은 성능을 큰 차이로 보임 Strong: Weak: Generic = text word의 종류 수 $100: 1000+: 90k$ 4.7. Ablation study options 1 direct: segmentation/binary map을 곧바로 사용 indirect: segmentation/binary map을 추가적인 레이어를 더해 후처리를 가하여 사용 options 2 soft: soft probability map(값의 범위가 $[0,1]$)을 사용 hard: mask map의 값이 0과 1로만 구성된 것을 사용 논문에서 제안하는 hard RoI masking 방법이 의미가 있음5. Conclusion end-to-end 학습 모델 Mask TextSpotter v3를 제안 SPN을 도입하여 정확한 텍스트 영역 폴리곤을 생성할 수 있음 다양한 데이터셋에 대한 검증 수행 Rotated ICDAR 2013 데이터셋에 rotation robustness 검증 MSRA-TD500 데이터셋에 aspect ratio robustness 검증 Total-Text 데이터셋에 shape robustness 검증 IC15 데이터셋에 작은 텍스트 인스턴스에 대한 강건함도 검증 " }, { "title": "[paper-review] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "url": "/posts/EfficientNet/", "categories": "paper-review, Computer Vision", "tags": "deep_learning, CNN, convolution, efficientnet, transfer_learning", "date": "2021-10-07 20:48:00 +0900", "snippet": "Tan, M., &amp;amp; Le, Q. (2019, May). Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning (pp. 6105-6114). PMLR.개인적인 논문해석을 포함하고 있으며, 의역 및 오역이 남발할 수 있습니다. 올바르지 못한 내용에 대한 피드백을 환영합니다 :)1. Introduction[@ Scaling up ConvNets] ConvNets의 모델 크기를 늘리는 것은 정확도를 향상시키기 위한 가장 보편적인 방법 ResNet이 대표적이며, ResNet-18에서 ResNet-200으로 더 많은 레이어를 사용함으로써 모델의 크기를 늘릴 수 있다. 하지만 ConvNets의 Scaling에 대한 연구는 잘 이루어지지 않았다. 이전에 모델의 depth, width, resolution을 확장하여 모델 성능을 높이려는 시도가 있었지만 이 세 가지 측면 중에서 한 가지 측면에만 모델을 확장하는 것이 일반적이었다. 세 가지 측면 모두에 대한 확장도 있었긴 했지만 사람이 임의의 값으로 지정했었다. depth: 모델 구성에서 레이어 수를 늘리는 것, ex) from ResNet-18 to ResNet-200 width: ConvNet 구성에서 convolution layer의 채널 수를 늘리는 것 resolution: 입력 이미지의 해상도를 키우는 것 [@ Rethinking Model Scaling] 본 논문에서는 ConvNets의 크기를 확장하는 프로세스를 다시 생각해보고 연구했다. 정확도(accuracy)와 효율성(efficiency)를 모두 달성할 수 있는 원칙적인 방법에 대한 연구 depth, width,resolution 모든 측면에서 균형을 맞추는 것이 중요 기존의 임의의 비율로 depth, width, resolution을 조정하던 것과 다르게 일정하게 고정된 scaling coefficient로 모델의 크기를 조정하게 된다. $2^N$배 더 많은 컴퓨팅 자원을 사용할 수 있다면 depth, width, resolution을 각각 $\\alpha^N, \\beta^N, \\gamma^N$배 하기만 하면 된다. $\\alpha, \\beta, \\gamma$는 간단한 grid search를 통해 찾을 수 있는 constant coefficient이다. 직관적으로도 이 scaling 방법은 합리적인 방법이다. 입력 이미지 해상도(resolution)를 늘리면 더 많은 레이어를 쌓아 receptive field를 늘리고(depth), 더 많은 채널을 계산에 포함해 세분화된 패턴을 잘 포착해야 한다(width). 2. Related Work[@ ConvNet Accuracy] 점차 ConvNets는 정확해지고는 있지만 그 크기도 커져가고 있다. 2014년 ImageNet competition winner GoogleNet이 6.8M의 파라미터 수를 가지고 top-1 accuracy 74.8% 2017년 ImageNet competition winner SENet은 145M의 파라미터 수를 가지고 top-1 accuracy 82.7% 논문 발표 당시 GPipe는 557M의 파라미터 수로 84.3%의 top-1 accuracy 더 높은 정확도가 우리에게는 필요한데 현재 하드웨어의 메모리 한계에 도달했다.[@ ConvNet Efficiency] Neural Architecture Search (NAS)가 효율적인 mobile-size의 ConvNets를 설계하는 데 널리 사용되고 있다. depth, width, convolution kernel types and sizes 등등의 광범위하게 조정하여 최적의 ConvNets을 설계할 수 있다. 다만, 훨씬 더 큰 사이즈를 가진 모델들에 대해서는 많은 계산 비용이 필요하기 때문에, 이 방법을 적용하기 어렵다. 본 논문에서, 거대한 ConvNets에 의 모델 효율성을 극대화하면서 정확도를 향상시킬 수 있는 방법에 대해 연구한다.[@ Model Scaling] depth 확장 대표적인 scaling 방법으로 레이어의 수를 조정하여 크기를 조정한다. ResNet은 ResNet-18 부터 ResNet-200까지 모델의 크기를 조정할 수 있다. (He et al., 2016) width 확장 WideResNet(Zagoruyko &amp;amp; Komodakis, 2016) 이나 MobileNets(Howard et al., 2017)은 모델의 크기를 Conv Layer 채널 수로 조정한 대표적 예시 depth와 width가 모두 중요하다라고 생각한 연구 (Raghu et al., 2017; Lin &amp;amp; Jegelka, 2018; Sharir &amp;amp; Shashua, 2018; Lu et al.,2018) 여전히 ConvNet의 효율, 성능을 모두 향상 시킬 수 있는 방법에 대한 논의가 필요하다. 3. Compound Model Scaling3.1. Problem Formulation[@ Annotations] ConvNet Layer $i$를 함수 $Y_i=\\mathcal F_i(X_i)$로 나타낼 수 있다. ($X_i=\\left &amp;lt; H_i, W_i, C_i\\right &amp;gt;$) ConvNet $\\mathcal N$은 레이어들이 합성된 리스트의 형태로 표현할 수 있다:$\\mathcal N = \\mathcal F_k \\bigodot…\\bigodot\\mathcal F_2\\bigodot\\mathcal F_1(X_1)$ 사실 ConvNet layer들이 모여 하나의 stage로 구성되고, 이 stage들의 모여 하나의 ConvNet을 이루며 이 stage 구조가 반복되는 경우가 많다. 예를 들어, ResNet은 각 stage가 여러 layer로 구성된 5 stages가 존재 모든 layer들은 같은 convolution type을 갖는다. 모든 stage들은 같은 아키텍처를 갖는다. 따라서 아래와 같이 ConvNet을 정의할 수 있다. \\(\\mathcal N = \\bigodot_{i=1...s}\\mathcal F_i^{L_i}(X_{\\left &amp;lt; H_i, W_i, C_i\\right &amp;gt;})\\) $i$번째 stage에서 $\\mathcal F_i$ layer가 $L_i$번 반복 -&amp;gt; $\\mathcal F_i^{L_i}$ [@ 최적화문제 정의]Model scaling은 baseline 아키텍처에서 ConvNet layer의 디자인을 변경하지 않은 채로 모델의 length($L_i$), width($C_i$), resolution($H_i, W_i$)을 수정하는 것이 일반적이다. 각 레이어 $i$에 대해 이 변수들($L_i, C_i, H_i, W_i$)을 최적으로 찾는 것만으로도 엄청난 경우의 수를 갖게 된다.즉, 우리의 목적함수는 아래와 같이 정의할 수 있게 된다.\\(\\begin {aligned}\\max_{d, w, r} &amp;amp;&amp;amp;&amp;amp; Accuracy(\\mathcal N(d, w, r)) \\\\s.t. &amp;amp;&amp;amp;&amp;amp; N(d, w, r) = \\bigodot_{i=1...s}\\mathcal {\\hat{F_i}}^{d*\\hat{L_i}}(X_{\\left &amp;lt; r \\cdot \\hat{H_i}, r\\cdot \\hat{W_i}, w \\cdot \\hat{C_i} \\right &amp;gt;}) \\\\&amp;amp;&amp;amp;&amp;amp; \\mathrm{Memory}(\\mathcal N) \\le \\mathrm{target\\_memory} \\\\&amp;amp;&amp;amp;&amp;amp; \\mathrm{FLOPS}(\\mathcal N) \\le \\mathrm{target\\_flops}\\end {aligned}\\)\\(\\mathrm{where} \\space \\mathcal {\\hat{F_i}}, \\hat{L_i}, \\hat{H_i}, \\hat{W_i}, \\hat{C_i} \\space \\text{are predefined parameters in baseline network}\\)3.2. Scaling Dimensions위 최적화에 있어 주요 문제점은 변수 $d, w, r$이 서로서로 영향이 있으며 각각 변수들을 조정함에 있어 서로 다른 컴퓨팅 자원의 한계 하에서 조정해야 한다는 점이다.[@ Depth] ConvNets를 조정하는 데 가장 보편적인 방법 그 근거는 모델을 깊게 쌓을수록 더 풍부하고 복잡한 특징들을 포착하여 새로운 task에도 잘 일반화될 수 있을 것이라는 직관적인 추론이다. But. 신경망을 깊게 구성할수록 vanishing gradient 문제는 피할 수 없다. (ResNet-1000은 ResNet-101보다 훨씬 더 많은 레이어로 구성했음에도 비슷한 정확도를 보인다.)[@ Width] ConvNets의 width를 조정하는 방법은 주로 작은 사이즈의 모델을 크게할 때 사용된다. 더 넓은 신경망을 구성할수록 fine-grained features를 잘 포착할 수 있으며 학습이 더 쉬워진다는 이전 연구도 있었다. (Zhagoruyko &amp;amp; Komodakis, 2016) But. 얇고 넓은 모델들은 higher level features를 포착하는 데 어려움을 가지는 경향이 있다. 본 논문에서의 실험(Figure 3 (left))에서도 모델이 넓어질수록 금새 모델의 정확도가 포화됨을 볼 수 있다.[@ Resolution] 입력 이미지의 해상도가 높은 해상도를 가질수록 ConvNets는 더 fine-grained patterns를 포착할 수 있다. 초기 ConvNets는 $224\\times 224$의 해상도로 출발했으나 최근에는 $299\\times 299, 331\\times 331$ 크기의 이미지를 사용하는 경향이 있다. Object detection과 같은 task에서는 $600 \\times 600$의 해상도를 높은 해상도를 사용한다. But. 본 논문에서의 실험 결과(figure 3 (right))를 보면 해상도가 점점 커질수록 정확도가 늘어나는 비율은 점차 줄어든다.[Observation 1] - 신경망의 크기를 width, depth, resolution 중 한 가지 측면에서 조정하는 것은 정확도를 향상시킬 수 있지만 모델이 커질수록 그 효과는 줄어든다.3.3. Compound Scaling앞선 실험을 통해 depth, width, resolution 각각의 측면에서 크기를 조정하는 것이 독립적이지 않다는 점을 알 수 있었다. 직관적으로도 높은 해상도의 이미지는 더 깊은 모델을 사용하는 것이 올바르다고 느껴진다.따라서 본 논문에서는 서로 다른 depth, width, resolution 측면의 스케일링 정도를 조정하고 각각이 균등하게 조정되야 함을 주장하고 있다.Figure 4는 이러한 직관적인 추론을 증명하는 결과이다.$d$와 $r$을 조정하며 서로 다른 네 가지의 환경에서 $w$를 점차 늘려가며 실험한 결과, $d=1.0, r=1.0$일 때 즉, $w$만 차별적으로 조정할 때 정확도가 포화되는 속도가 가장 빨랐다.[Observation 2] - 정확도와 효율성을 모두 잡아내기 위해선, width, depth, resolution을 균형있게 조정하는 것이 중요하다.사실 일부 선행 연구에서 depth, width, resolution을 균형있게 조정하는 연구를 진행했었지만 이 수치를 모두 수동으로 조정했었다. (Zoph et al., 2018, Real et al., 2019)[@ Compound Scaling Method]본 논문에서는 사용자가 자신의 하드웨어 스펙에 맞게 크기 조정 정도를 compound coefficient $\\boldsymbol\\phi$로 조정할 수 있는 방법을 제안한다.\\[\\begin {aligned}\\mathrm{depth:} &amp;amp;&amp;amp;&amp;amp; d=\\alpha^\\boldsymbol\\phi \\\\\\mathrm{width:} &amp;amp;&amp;amp;&amp;amp; w = \\beta^\\boldsymbol\\phi \\\\\\mathrm{resolution:} &amp;amp;&amp;amp;&amp;amp; r = \\gamma^\\boldsymbol\\phi \\\\\\mathrm{s.t.} &amp;amp;&amp;amp;&amp;amp; \\alpha\\cdot\\beta^2\\cdot\\gamma^2 \\approx 2 \\\\&amp;amp;&amp;amp;&amp;amp; \\alpha \\ge 1, \\beta \\ge 1, \\gamma \\ge 1\\end {aligned}\\] @ 제약조건 $\\alpha\\cdot\\beta^2\\cdot\\gamma^2 \\approx 2$ 에 대하여. ConvNets의 depth, width, resolution를 조정함에 따라 연산의 FLOPS는 각각 $d, w^2, r^2$으로 비례한다. 즉, depth를 두 배 늘리면 FLOPS가 두 배 늘어나지만 width, resolution을 각각 두 배 늘리면 FLOPS는 각각 네 배씩 증가한다. ConvNet 중심의 모델들은 대부분의 계산 비용이 ConvNets에서 발생하기 때문에 총 모델의 FLOPS는 $\\alpha\\cdot\\beta^2\\cdot\\gamma^2$에 근접하게 늘어난다. 본 논문에서는 $\\alpha\\cdot\\beta^2\\cdot\\gamma^2$의 값을 $2$에 근접하게 제약조건을 걸었기 때문에 총 FLOPS는 $2^\\boldsymbol\\phi$만큼 증가한다고 볼 수 있다. 4. EfficientNet ArchitectureModel Scaling은 baseline 모델의 각 레이어 연산을 수정하지 않기 때문에 좋은 baseline 모델을 갖추는 것이 중요하다. 현존하는 ConvNets 아키텍처들에도 본 논문에서 제안하는 scaling 방법을 적용할 것이지만 그 효과를 더 잘 보여주기 위해 EfficientNet이라는 새로운 baseline 모델을 제안한다.Tan et al., 2019에서의 연구에서 처럼 Multi-objective neural architecture search (AutoML) 방법을 사용해 정확도와 FLOPS를 모두 최적화하는 모델 구조, EfficientNet-B0를 찾았다. Tan et al., 2019 에서의 연구와 같은 search space optimization goal: $ACC(m) \\times [FLOPS(m)/T]^w$ $ACC(m), FLOPS(m)$: 모델 $m$에 대한 정확도와 FLOPS $T$: target FLOPS $w$: 정확도와 FLOPS의 trade-off를 조정하는 hyperparameter 논문에서는 $-0.07$ 사용 [@ Compound Scaling Method의 사용]이 EfficientNet-B0를 baseline으로 compound scaling method를 사용 Step 1. $\\boldsymbol\\phi=1$로 고정하고, 두 배의 컴퓨팅 자원이 사용 가능함을 가정하고 $\\alpha, \\beta, \\gamma$를 작은 grid search를 통해 최적값 $\\alpha=1.2, \\beta=1.1, \\gamma=1.15$를 얻을 수 있었다. Step 2. $\\boldsymbol\\phi$를 점차 늘려가며 차례로 EfficientNet-B1 ~ B7을 획득한다. 모델의 몸집을 키운 다음에 최적의 $\\alpha, \\beta, \\gamma$ 값을 찾는 것이 훨씬 더 나은 성능을 보장하겠지만 이 방법은 너무 계산비용이 크다. 앞선 실험을 통해 $\\alpha, \\beta, \\gamma$의 최적 조합이 있음을 밝혀내고 이를 작은 모델에서 최적 조합을 찾고 이를 점차 몸집을 늘려 성능이 뛰어난 모델을 만들었다는 점이 본 연구의 주요 contribution이라 볼 수 있을 것이다. 5. Experiments5.1. Scaling Up MobileNets and ResNetsTable 3에서 MobileNets, ResNets 모두에서 논문에서 제안하는 Compound Scale Method가 FLOPS는 비슷하게 유지하면서 성능을 크게 향상시킴을 확인할 수 있다.5.2. ImageNet Results for EfficientNetTable 2는 각 EfficientNet이 비슷한 성능을 보이는 모델들이 계산 비용 면에서는 크게 앞서는 모습을 보여준다.5.3. Transfer Learning Results for EfficientNet전이학습에 사용되어도 파라미터 수를 현저하게 줄일 수 있다.6. Discussion[@ Compound Scaling Method 빼기 EfficientNet Architecture]EfficientNet에서 논문에서 제안하는 Compound Scaling Method의 효과를 덜어내보았다. depth, width, resolution 한 가지 측면에서만 scaling을 진행하는 것보다 Compound Scaling Method가 확실히 더 나은 성능을 보임을 알 수 있다.[@ Class Activation Map]위 실험에서 사용된 모델들의 예측에 집중하는 영역을 살펴보면 Compound Scaling Method를 사용했을 때 물체의 디테일에 잘 집중하는 모습을 볼 수 있다.7. Conclusion 중요하면서도 이전의 많은 연구들이 간과하고 있던 ConvNets을 몸집을 늘릴 때 depth, width, resolution을 균형있게 늘려야 한다는 점을 체계적으로 밝혀냈다. 이를 위한 Compound Scaling Method를 제안하였으며 간단하고 효과적으로 baseline ConvNets의 몸집을 키울 수 있었다. 이 과정에서 보다 원칙적인 방법으로 모델의 효율성까지 유지할 수 있었다. EfficientNet이라는 mobile-size의 baseline 모델을 제시했으며, 이 모델을 기반으로 Compound Scaling Method를 사용해 모델을 효과적으로 더 나은 성능을 위해 사이즈를 늘릴 수 있음을 보였다. 결과적으로 EfficientNet+Compound Scaling Method로 계산 비용을 최소화하며 최고 수준의 성능을 달성할 수 있었다." }, { "title": "[project-review] ArcFace를 활용한 한국인 안면 인식", "url": "/posts/ArcFace_project/", "categories": "project-review", "tags": "deep_learning, ArcFace, face_recognition", "date": "2021-09-30 12:20:00 +0900", "snippet": "개요이번 학기 학부 졸업 프로젝트 과제의 주제로 현재 COVID-19의 확산과 무인화 경향에 힘입어 “딥러닝 기반 얼굴인식을 활용한 본인인증 시스템”을 개발해보았다.이번 포스팅에서는 위와 같은 프로젝트 수행 과정에서 ArcFace를 활용한 얼굴인식 모델을 구현해보는 과정을 간략하게 소개한다.1. ArcFace를 활용한 안면인식 모델 개발@ ArcFaceDeng, J., Guo, J., Xue, N., &amp;amp; Zafeiriou, S. (2018). ArcFace: Additive Angular Margin Loss for Deep Face Recognition. In 2019 IEEE. In CVF Conference on Computer Vision and Pattern Recognition (CVPR) (pp. 4685-4694).ArcFace는 Metric Leaning의 한 종류로 Metric Leaning은 그 활용과 학습 방법이 여타 다른 딥러닝 활용 방법과 조금은 다르다. 우선 매우 많은 클래스 종류를 가진 분류(Classification) 문제를 학습시킨다. 안면인식의 경우에는 매우 많고 다양한 사람들의 얼굴 이미지를 입력으로, 그 사람이 어떤 사람인지 분류해내는 것을 학습하게 된다. 이 분류 문제를 학습하면서 입력 이미지에 대한 Embedding을 부가적으로 학습하게 된다. 동일한 클래스의 이미지들은 서로 가깝게 위치하도록, 다른 클래스의 이미지들은 서로 멀리 떨어지도록 Embedding을 구성할 수 있게 학습하게 된다. 따라서 ArcFace에 동일한 두 사람의 얼굴 이미지를 입력하면 두 이미지에 대한 Embedding은 서로 매우 가깝게 위치하도록, 다른 두 사람의 얼굴 이미지를 입력하면 서로 매우 멀리 떨어뜨리도록 학습하게 된다. 이렇게 학습한 Embedding network를 활용할 때는 분류 레이어 (Classification layer)는 떼어내고 활용하게 된다. 이번 프로젝트에서는 이렇게 학습된 Embedding network에 두 사람의 얼굴 이미지를 입력해 각각 이미지에 대한 Embedding을 추출하고 이 두 Embedding의 거리를 계산해 일정 거리보다 멀면 다른 사람으로, 가까우면 동일한 사람으로 판별하는 시스템을 만들게 된다. @ 한국인 얼굴 사진(K-Face) 데이터데이터의 출처.한국인 얼굴 사진(K-Face) 데이터베이스는 “한국정보화진흥원”의 “영상분야 지식베이스 구축” 사업과 “인공지능학습용 AI데이터 구축”사업의 일환으로 구축되었다. 데이터는 무료이지만 AI Hub의 허가를 통해 제공받아야 한다(링크).데이터의 구성데이터의 구성은 AI Hub의 공식 GitHub repository에서 공개되어 있는 내용을 참고했다.제공받은 K-Face 데이터셋의 구조는 다양한 영상 해상도, Class ID, 액세서리 유무, 조명 위치 및 세기, 표정, 포즈 방향에 따라 구성되어 있다. 해상도 3종 (High [864, 576], Middle [346, 230], Low [173, 115]) 액세서리 6종 (S001 ~ S006) 조명 30종 (L1 ~ L30) (조도 30 lightings) 표정 3종 (E01 ~ E03) 각도 20종 (C1 ~ C20) (20 views)AI Hub: K-Face 데이터셋 구성특징점 위치 및 인덱스뿐만 아니라 다양한 얼굴의 특징점, Bounding Box도 일부 데이터에 한해 제공하고 있다.bounding box annotation 예시 Bounding Box 검출 대상 촬영 각도: 촬영되는 모든 각도 조명(4 종류): L1 (1000 Lux), L3 (200 Lux), L6 (40 Lux), L7 (0 Lux) 표정(3종류): E01 (무표정), E02 (활짝웃음), E03 (찡그림) 액세서리(1종류): S001 (보통) 데이터 전처리 과정 하드웨어의 한계와 학습의 효율을 고려하여 고, 중, 저화질의 이미지 중 중화질의 이미지를 사용했다. 전체 데이터에서 얼굴인식 task에 적합하도록 얼굴 영역을 추출할 수 있는 조명 4종류, 표종 3종류 액세서리 1종류에 대한 이미지만을 사용했다. 총 4종류의 조명 세기 중에 피사체가 확실하게 식별되는 L1, L3 조명 세기만 학습에 사용했다. 나머지 L6, L7의 밝기는 굉장히 어두워서 제외했다. 다양한 촬영 각도 중 얼굴의 정면이 확실하게 식별되는 4종류의 촬영 각도(“C6”, “C7”, “C8”, “C9”)를 선별하고 이를 학습에 사용했다. (400명) x (조명 2종) x (촬영각도 4종) x (표정 3종) = 총 9,600장 이 중 10명의 인원에 대한 이미지는 모델의 검증을 위한 데이터로 사용했다. train set: (390명) x (조명 2종류) x (촬영각도 4종류) x (표정 3종류) = 총 9,360장 validation set: (10명) x (조명 2종류) x (촬영각도 4종류) x (표정 3종류) = 총 240장 이미지 crop 전/후@ 얼굴인식 모델 개발ArcFace 모델의 소스코드는 “peteryuX”님의 GitHub을 참고했다. 이 GitHub에서 감사하게도 거대한 얼굴인식 데이터셋인 MS-Celeb-1M 데이터에서 학습한 모델을 제공하고 있었으며 이를 활용했다.이번 프로젝트의 목표는 한국인에 대한 얼굴인식이었기 때문에 사전학습 모델을 우리가 확보한 K-Face 이미지에 적합하게 미세조정(Fine-Tuning)하는 방향으로 수행했다.@ 모델 검증검증 데이터셋 10명, 240장에 대한 동일인물 구별 실험을 설계했다. 240장의 사진 중 랜덤한 두 사진을 선택하여 사진쌍을 생성 생성할 때 선택된 사진쌍이 동일인물인 경우 200쌍과 동일인물이 아닐 경우 200쌍, 총 400쌍의 사진쌍을 생성했다.앞에서 학습한 모델이 각각 사진쌍에 대해서 512차원의 임베딩 벡터를 추출하도록하고 그 두 임베딩의 거리를 측정하여 임계값보다 작으면 동일인물, 크면 동일인물이 아닌 것으로 분류했다.마지막으로 여기서 사용되는 임계값을 $[1, 4]$의 범위에서 $0.01$씩 늘려가며 반복실험했다.동일인물 검증 실험@ 검증 결과위에서 수행한 실험에 대해 ROC Curve를 그려본 결과이다.미세조정 이전에도 충분히 높은 성능을 보여주었지만, 미세조정 후에 AUC가 0.1정도 향상되었다2. 본인인증 시스템 구축위에서 학습한 ArcFace 딥러닝 모델을 기반으로 아주 기본적인 형태의 본인인증 시스템을 만들 수 있었다.시스템의 입력 값은 자신의 신분증과 실시간으로 촬영된 자신의 얼굴 사진이다. 먼저 사용자는 자신의 신분증을 입력한다. 이번 프로젝트에서 구현한 형태는 미리 이미지 파일로 저장해놓고 사진 파일명을 입력하는 형태로 이루어지는 형태이다. 신분증에서 Tesseract를 활용한 OCR을 진행한 후 생년월일 부분을 추출하여 성인임을 판별한다. 사용자의 실시간 얼굴 사진을 촬영한다. 이 때, 사진을 도용하는 것을 방지하기 위해 시스템은 특정 모션을 수행하도록 요구 모션을 올바르게 수행했을 때만 다음 단계로 이동할 수 있다. 입력받은 신분증 이미지와 촬영한 실시간 얼굴 이미지에서 얼굴 영역을 dlib 오픈소스 라이브러리를 통해 추출한다. 추출한 얼굴 영역을 입력으로 하여 위에서 학습한 ArcFace 모델을 통해 두 이미지에 대한 Embedding을 추출한다. (5)에서 추출한 두 Embedding의 거리를 계산하여 일정 값보다 가까우면 동일인물임을, 가깝지 않으면 동일인물이 아님을 출력한다. GitHub repository" }, { "title": "[paper-review] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows", "url": "/posts/Swin_Transformer/", "categories": "paper-review, Computer Vision", "tags": "deep_learning, vision_transformer, ViT, self-attention, transformer, swin_transformer", "date": "2021-08-27 12:20:00 +0900", "snippet": "Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., … &amp;amp; Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030.개인적인 논문해석을 포함하고 있으며, 의역 및 오역이 남발할 수 있습니다. 올바르지 못한 내용에 대한 피드백을 환영합니다 :)1. Introduction본 논문에서는 Transformer의 Computer Vision 분야로의 확장 가능성에 대한 연구를 진행했으며 Computer Vision에서의 general purpose backbone으로 사용될 수 있도록 하려했다.[@ Scale]언어 modality와 비전 modality 간의 차이 중 하나는 Scale의 포함 여부이다. 언어 태스크에서 활용되었던 Transformer가 언어를 처리함에 있어 가장 기본적인 단위인 단어 토큰과 다르게 시각적 요소는 크기가 다양하게 출현하게 된다. 이 점은 기존 Object detection과 같은 태스크에서 주된 연구 주제이기도 했다.현존하는 Transformer 기반의 모델들은 모두 고정된 scale의 토큰들을 가지고 있으며 이는 vision task에 적합하지 않은 특성이다.[@ Resolution &amp;amp; Computation complexity]또 하나의 차이점은 텍스트 구절에 비해 이미지의 픽셀 해상도(데이터 밀집도)가 훨씬 높다는 점이다. 픽셀 수준에서 고밀도 예측(dense predictions)이 필요한 semantic segmentation과 같은 vision task에서는 Self-attention의 계산 복잡성이 이미지 크기에 따라 제곱으로 증가하기 때문에 고해상도 이미지에서는 Transformer의 사용이 어렵다.[@ Swin Transformer] Scale 아래 그림 1(a)에서처럼 Swin Transformer는 작은 크기의 패치(patch)에서 시작해 점차 더 모델이 깊어질수록 인점한 패치들을 병합하며 계층적인 특징표현(hierarchical representation)을 구성할 수 있게 된다. Swin Transformer의 이러한 특성으로 인해 기존 Computer vision community에서 주로 사용되던 Object detection이나 Semantic segmentation과 같은 advanced task에도 backbone으로써 사용될 수 있다. Resolution &amp;amp; Computation complexity Swin Transformer에서는 이미지를 분할하여 그 분할한 window에 대해서만 Self-attention을 계산하게 된다. 따라서 window 크기를 한 번 지정하면 이미지가 늘어남에 따라 window 내부의 패치 수는 고정되며 따라서 계산복잡도는 이미지 크기에 대해 window 수에 따라서 선형적으로만 증가하게 된다. 이 점은 기존 Vision Transformer가 이미지 크기에 대해 제곱으로 증가하는 것과 대비된다. 논문에서 제안하는 계층적 feature의 구성, 이로 인해 기존 CNN이 그러하였듯이 다른 vision task의 backbone으로써 사용될 수 있다.3. Method3.1 Overall ArchitectureSwin Transformer의 구조[@ Patch Partition, Patch Splitting Module](Vision Transformer와 같이) 입력되는 RGB 이미지를 서로서로 겹치지 않도록 patch로 분할하고 각 patch를 1차원으로 펼친다(flatten).논문에서는 각 patch의 크기를 $4\\times 4$로 사용했으며 따라서 펼친 1차원 patch는 $4\\times 4 \\times 3(\\mathrm{RGB})=48$ 차원을 가진다.입력 이미지를 patch를 sequence 형태로 펼치는 과정[@ Stage 1] 먼저 Linear Embedding을 거쳐 $C$차원으로 사영(projection)된다. ${H\\over 4} \\times {W\\over 4} \\times 48 \\rightarrow {H\\over 4} \\times {W\\over 4} \\times C$ 이렇게 만들어진 $({H\\over 4} \\times {W\\over 4})$개의 $C$차원 벡터들은 Transformer에서의 “token”으로써 사용된다. 각 token들은 일정 갯수의 Transformer block을 통과한다.[@ Stage 2]여기에서는 hierarchical한 feature map을 생성하기 위해 patch의 크기를 조정하게 된다. patch merging layer를 통과하여 서로서로 인접한 $(2\\times 2)=4$개의 patch들끼리 결합하여 하나의 큰 patch를 새롭게 만든다. 아래는 patch merging layer의 계산 과정을 시각화한 것이다. 인접한 $(2\\times 2)=4$개의 patch들을 concatenate하는 과정에서 차원이 $4C$로 늘어나기 때문에 linear layer를 통과하여 $2C$로 조정한다. Stage 1에서와 같이 일정 갯수의 Transformer block을 통과한다. Output: ${H\\over 8} \\times {W\\over 8} \\times 2C$ patch merging[@ Stage 3 &amp;amp; 4]Stage 2와 같은 방식으로 점차 patch size는 커지고 patch의 수는 많아지며 각 flattened patch(=token)의 차원은 두 배씩 늘어간다. Stage 3: ${H\\over 16} \\times {W\\over 16} \\times 4C$ Stage 4: ${H\\over 32} \\times {W\\over 32} \\times 8C$ 각 Stage에서의 Output은 기존 computer vision task에서 많이 사용되는 형태의 feature map으로 활용될 수 있게된다.[@ Swin Transformer Block]각 Stage들은 Swin Transformer Block들을 여러 차례 거치게 된다. Swin Transformer block들은 아래와 같이 구성된다. block 내부의 각 요소들에 대한 설명은 Section 3.2에서 설명하고 있다.Swin Transformer block\\[\\hat{\\mathbf z}^l = \\text{W-MSA}(\\text{LN}(\\mathbf z^{l-1})) + \\mathbf z^{l-1},\\]\\[\\mathbf z^l = \\text{MLP}(\\text{LN}(\\hat{\\mathbf z}^l)) + \\hat{\\mathbf z}^l,\\]\\[\\hat{\\mathbf z}^{l+1} = \\text{SW-MSA}(\\text{LN}(\\mathbf z^l)) + \\mathbf z^l,\\]\\[\\mathbf z^{l+1} = \\text{MLP} (\\text{LN}(\\hat{\\mathbf z}^{l+1})) + \\hat{\\mathbf z}^{l+1}\\]\\[l = 1 ... L\\]3.2 Shifted Window based Self-Attention[@ 기존 Vision Transformers의 한계]Image Classification에 활용된 기존 Transformer 아키텍처와 그 변형들은 토큰과 모든 토큰 사이의 관계를 계산하는 global self-attention을 적용했다.하지만 이 방법은 계산 복잡도가 입력 이미지 해상도에 대해서 제곱으로 증가하게 되고 dense한 예측이나 높은 해상도의 이미지에 적용하기에는 어려움이 있다.[@ Self-attention in non-overlapped windows]논문에서는 위에서 제시한 한계 때문에 효율적인 모델링을 위해 window들 내부에서만 self-attention을 계산하는 것을 제안한다.$M\\times M$개의 패치들로 window가 구성되어 있다고 생각하면 아래와 같이 계산복잡도를 나타낼 수 있다.\\[\\Omega(\\text{MSA}) = 4hwC^2 + 2(hw)^2C,\\]\\[\\Omega(\\text{W-MSA}) = 4hwC^2 + 2M^2hwC\\]기존 MSA 모듈에서 패치 내부의 해상도 $hw$에 제곱으로 증가하는 반면 W-MSA 모듈에서는 패치 내부에서만 계산이 발생하기 때문에 한 window 내부의 패치의 수 $M \\times M$에 따라 증가한다.[@ Shifted window partitioning in successive blocks]위에서 언급한 W-MSA(window-based self-attention) 모듈은 window간의 연결성이 부족하고, 이는 모델링 성능을 저해시키는 요소이다.논문에서는 연산의 효율을 유지하면서도 window들 간의 연결성을 반영할 수 있는 shifted window partitioning 방법을 제안한다.Shifted window partitioning그림 2의 좌측은 W-MSA 모듈의 window 분할 방식, SW-MSA 모듈의 window 분할 방식이다. W-MSA $8\\times 8$ feature map을 $2\\times 2 = 4$개의 window로 나누면 그림과 같이 $4\\times 4(M=4)$ 크기의 window들로 구성되게 된다. SW-MSA W-MSA 모듈에서 분할이 발생한 패치에서 $\\left( \\lfloor {M \\over 2} \\rfloor, \\lfloor {M \\over 2} \\rfloor \\right)$칸 떨어진 패치에서 window 분할이 발생 그림 2의 예시에선, $\\left( \\lfloor {M \\over 2} \\rfloor, \\lfloor {M \\over 2} \\rfloor \\right) = (2, 2)$. [@ Efficient batch computation for shifted configuration]SW-MSA 모듈을 실제로 적용함에 있어 생각해야할 점이 존재한다. 먼저 SW-MSA 모듈에서처럼 window를 나누게 되면 window의 수가 늘어나게 된다. W-MSA에서 $\\lceil {h \\over M} \\rceil \\times \\lceil {w \\over M} \\rceil (=2\\times 2)$ 였던 window 수가 SW-MSA 모듈에서는 $(\\lceil {h \\over M} \\rceil + 1) \\times (\\lceil {w \\over M} \\rceil + 1) (=3\\times 3)$으로 늘어난다. 또한 일부 window들은 $M\\times M$보다 작아지게 된다.논문에서는 이를 위한 두 가지 방법을 제시한다. Naive solution 작아진 window들에 padding을 두어 $M\\times M$ 사이즈로 만드는 방법 window의 수는 여전히 늘어나게되고 그렇게 되면 W-MSA 모듈의 window 수, $\\lceil {h \\over M} \\rceil \\times \\lceil {w \\over M} \\rceil (=2\\times 2)$와 SW-MSA 모듈에서의 window 수, $(\\lceil {h \\over M} \\rceil + 1) \\times (\\lceil {w \\over M} \\rceil + 1) (=3\\times 3)$로 달라지게 된다. 따라서 이 방법은 계산복잡도 상으로도, 효율 면으로도 적절하지 않다. Cyclic-shifting 좌상단(top-left) 패치들부터 그림 4처럼 패치를 옮겨 $M\\times M$ 크기로 만든다. 이렇게 패치를 새롭게 구성하면 여러 window들이 인접하지 않는 패치들과 인접하게 되므로 이를 분리할 수 있는 계산을 적용해야 한다. 논문에서는 여기에 마스킹 메커니즘(masking mechanism)을 통해 이를 분리한다. 예를 들어 A window에 속하는 패치에 대한 self-attention을 계산할 때 B window에 속하는 패치들에는 마스킹을 적용하는 방식이다. 이 방법을 통해 window들의 크기, 갯수를 W-MSA 모듈에서의 것과 동일하게 유지할 수 있으며 효율적인 방법이다. cyclic shift[@ Relative position bias]Self-attention을 계산하는 과정에서 Relative position bias $B \\in \\mathbb R^{M^2 \\times M^2}$를 더함으로써 위치적 정보를 모델링할 수 있도록 했다. 이 bias는 window 내부에서의 위치를 모델링하는 것이다.\\(\\text{Attention}(Q, K, V) = \\text{SoftMax}(QK^T/\\sqrt d + B)V,\\)\\(Q, K, V \\in \\mathbb R^{M^2 \\times d}, d= \\text{dimension \\space of \\space} query/key\\)Vision Transformer에서 사용했던 position embedding(Absolute position embedding: 모든 패치의 위치에 따른 임베딩)을 사용했을 때 위에서 제시한 relative position bias를 사용했을 때보다 오히려 성능이 저하되는 것을 관찰했다.3.3 Architecture Variants Swin-T: $C=96$, layer numbers $={2, 2, 6, 2}$ Swin-S: $C=96$, layer numbers $={2, 2, 18, 2}$ Swin-B: $C=128$, layer numbers $={2, 2, 18, 2}$ base model, ViT-B와 DeiT-B의 모델 크기 및 복잡도가 비슷하도록 설계 Swin-L: $C=192$, layer numbers $={2, 2, 18, 2}$ $C$는 Stage 1의 hidden layers의 채널 수 window size, $M=7$ query dimension of each head, $d=32$ each MLP, $\\alpha=4$4. Experiments4.1 Image Classification on ImageNet-1K표 1(a)는 ImageNet-1K에 학습한 경우를 나타내며, 표 1(b)는 ImageNet-22K에 사전학습하고 ImageNet-1K에 미세조정한 경우를 나타낸다.위와 같이 1. 거대한 데이터에 사전학습하고 미세조정을 수행할 경우는 물론이고 2. 사전학습을 거치지 않은 경우에도 최고의 결과를 보였다.4.2 Object Detection on COCO4.3 Semantic Segmentation on ADE20K4.4 Ablation Study아래 세 가지 task에 대해 Ablation study를 진행 Image Classification ImageNet-1K Cascade Mask R-CNN을 방법으로 하여 COCO Object detection UperNet을 방법으로, ADE20K Semantic Segmentation[@ Shifted windows] 세 가지 task 모두에게서 shifted windows 방법이 효과있음을 보이고 있다. 이 방법이 window들 간의 연관성을 모델링할 수 있음을 알 수 있다.[@ Relative position bias] 기존 Vision Transformer에서 사용했던 [abs](absolute position embedding의 사용이 효과적임을 볼 수 있다. [app](the first scaled dot-product term)는 아래 수식에서 $\\sqrt d$를 의미하는 것으로 보인다.\\[\\mathrm{Attention}(Q, K, V) = \\mathrm{SoftMax}(QK^T/\\sqrt d + B)V\\][@ Different Self-attention methods] 아래 표 5와 표 6은 cyclic shifting + shifted windows를 통한 window 내의 self-attention 계산의 효용성을 보인다. 기존 Vision Transformer 모델들의 sliding window에 비해 성능은 대등하지만 그 계산 복잡도는 현저한 차이를 보인다.가장 빠른 Trnasformer 아키텍처 중 하나인 Performer와 비교해서도 본 논문에서 제안하는 방법이 더 나은 결과를 보였다." }, { "title": "[paper-review] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "url": "/posts/Vision_Transformer/", "categories": "paper-review, Computer Vision", "tags": "deep_learning, vision_transformer, ViT, self-attention, transformer", "date": "2021-08-10 12:20:00 +0900", "snippet": "Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., … &amp;amp; Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.개인적인 논문해석을 포함하고 있으며, 의역 및 오역이 남발할 수 있습니다. 올바르지 못한 내용에 대한 피드백을 환영합니다 :)1 Introduction[@ 개요]Transformer로 불리우는 Self-attention 기반의 모델은 자연어 처리(NLP; Natural Language Processing)에서 주류가 되었다. 거대한 말뭉치(corpus)에 학습되어 여러 작은 특정 태스크에 미세조정되기 간편해지며 많은 연구에서 사랑받고 있는 모델이다.하지만 컴퓨터 비전(Computer Vision)에서는 여전히 CNN이 폭넓게 사용되고 있다. NLP에서의 Transformer의 성공을 기반으로 많은 연구들이 CNN을 일부분 혹은 전부 대체하려는 시도들이 많이 있었는데, CNN을 Transformer로 전부 대체하려는 연구들은 컴퓨팅 비용 측면에서는 비용을 많이 줄이는 등의 효과가 있었지만 성능면에서 ResNet 기반의 아키텍처 등에 뒤떨어지는 모습을 보였다.[@ Vision Transformer (ViT)]NLP에서의 Transformer scaling successes에 영감을 받아 가능한 최소한의 수정을 거친 Standard Transformer를 곧바로 이미지에 적용하는 실험을 진행했다.이미지를 몇몇 패치(patch)들로 나누고 이 패치들을 선형 임베딩(Linear embedding)에 통과시켜 임베딩으로 만들고 이를 시퀀스로 만들어 Transformer의 입력으로 주어질 수 있도록 했다. 이렇게 입력 이미지에서 쪼개진 패치들은 NLP에서의 “token”들과 같은 단위로 취급될 수 있다.ImageNet과 같은 mid-sized 규모의 데이터셋에서 별다른 강력한 제약없이 위 모델을 학습시킬 경우 ResNet 계열의 모델보다 몇 퍼센트나 낮은 결과를 보였다. 여기서 저자들은 Transformer에는 inductive bias가 CNN에 비해 많이 부족하다고 생각했다. Inductive bias translation equivariance: 이미지 내에서 물체의 위치가 바뀌어도 그 특성을 똑같이 잡아낼 수 있는 능력 - 참고: https://ganghee-lee.tistory.com/43 localty: Transformer는 역시 global한 상관관계(멀리 떨어진 요소들 간의 상관관계)를 훌륭하게 잡아낼 수 있지만 지역적인 상관관계(가깝게 붙어있는 요소들 간의 상관관계)를 잡아내는 데에는 CNN만한 것이 없다. 하지만 거대한(14M ~ 300M) 규모의 데이터셋에서 모델이 학습된다면 상황이 바뀐다. 저자들은 이 경우 대규모 학습이 영향이 inductive bias의 부족함에 대한 영향을 충분히 압도한다는 것을 관찰했다.또한 대규모 데이터셋에서의 사전 학습 이후에 소규모 데이터가 있는 작업으로 전이학습(transfer learning)에 사용될 때 우수한 결과를 보임을 확인했다.2 Related Work[@ Transformer] Vaswani et al. - 기계 번역(Machine translation) task에 Transformer가 처음으로 제안되었지만 이후 많은 NLP분야 state-of-the-art 방법론의 기조가 되었다. Devlin et al. &amp;amp; *Radford et al. - Transformer는 대규모 말뭉치에서 사전 학습되고 이후 각각 task에 맞게 미세조정(fine-tune)하여 사용되고 있다. * Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding with unsupervised learning. Technical Report, 2018. [@ Naive application of self-attention to images]Transformer의 근간이 되는 Self-attention을 이미지에 사용하는 가장 Naive한 방법은 이미지의 모든 픽셀에 대해 다른 모든 픽셀에 관한 상관관계를 계산하는 것이다. 이 방법은 이미지의 픽셀 수에 대해 4제곱으로 계산 복잡도가 증가하기 때문에 실해상도에 해당하는 이미지에 대해선 적용되기 어려웠다. Parmar et al. - 각 픽셀에 대해 다른 모든 픽셀에 대해 계산하지 않고 local neighborhood 픽셀 들에 대해서만 self-attention을 계산했다. Hu et al., Ramachandran et al., Zhao et al. - 이렇게 지역적으로만 self-attention을 계산하는 것은 convolution을 대체할 수 있을 것이라 생각한 연구도 있었다. Child et al. - Sparse Transformer는 scalable approximations를 global self-attention에 적용하여 이미지에 사용될 수 있도록 했다. Weissenborn et al., Ho et al., Wang et al. - scale attention을 사용하는 또다른 방법은 각기 다른 크기의 block에 self-attention을 적용하는 것이다.[@ Related works with ViT] Cordonnier et al. - $2\\times 2$ 크기의 patch를 입력 이미지에서 추출하고 마지막에 full self-attention을 적용했다. 본 논문의 연구에선 바닐라 Transformer 모델이 대규모 사전 학습을 거쳐 CNN 기반 모델보다 더 나아질 수 있음에 초점을 맞추고 있다. Chen et al. - image GPT (iGPT)는 이미지의 해상도와 색 영역을 줄인 후에 Transformer를 적용하는 모델이다. 이 모델은 생성 모델처럼 비지도학습 방법으로 학습된다.3 MethodVision Transformer의 구조3.1 Vision Transformer (ViT) 입력 이미지 $\\mathbf x \\in \\mathbb R^{H\\times W\\times C}$를 Transformer의 입력 형태인 1D 토큰 시퀀스로 변환 $\\mathbf x_p\\in \\mathbb R^{N\\times(P^2\\cdot C)}$ $(H, W)$, 기존 입력 이미지의 해상도 $C$, 기존 입력 이미지 채널 수 $(P, P)$, 입력 이미지를 쪼갠 패치의 크기 $N = HW/P^2$, 패치의 수 이미지 패치를 flatten 하는 과정이것을 다시 Transformer의 선형 임베딩 $\\mathbf E \\in \\mathbb R^{(P^2\\cdot C)\\times D}$를 통해 입력 차원 $D$로 매핑한다.\\[\\mathbf z_0 = [\\mathbf x_{\\mathrm {class}};\\mathbf x_p^1\\mathbf E; \\mathbf x_p^2\\mathbf E; ... ;\\mathbf x_p^N\\mathbf E] + \\mathbf E_{pos},\\]\\[\\mathbf E \\in \\mathbb R^{(P^2\\cdot C)\\times D}\\][@ Position embedding]또한 ViT에는 Self-attention만 있기 때문에 위치 정보를 제공해줄 position embedding ($\\mathbf E_{pos}$)을 더해주어야 한다. position embedding에는 2D 위치정보를 제공해주는 것도 실험해봤지만 눈에 띄는 성능 향상이 없어 기존 1D position embedding을 사용했다고 언급하고 있다.\\[\\mathbf E_{pos} \\in \\mathbb R^{(N+1) \\times D}\\][@ CLS token]concatenated patch embedding에 $\\mathbf x_{\\mathrm{class}}$가 추가되고 positional embedding, $\\mathbf E_{pos}$의 차원도 $(N+1)$로 1이 추가되어 있음을 확인할 수 있는데, BERT에서의 [class]토큰처럼 클래스 정보를 모델에 제공해주었기 때문이다.[@ Transformer encoder]기존 Transformer 구조와 비슷하게 구성되어 있다. 위와 같은 Transformer encoder block이 $L$번 반복되어 feature를 만들게 된다. $\\mathrm{MSA}$: Multi-head Self-Attention $\\mathrm{LN}$: Layer Norm, Normalization Layer $\\mathrm{MLP}$: Multi-Layer Perceptron, 논문에서는 “GELU” activation function을 사용한다. $\\mathbf z_L^0$: output of Transformer encoder blocks\\[\\mathbf z&#39;_l = \\mathrm{MSA}(\\mathrm{LN}(\\mathbf z_{l-1})) + \\mathbf z_{l-1},\\]\\[\\mathbf z_l = \\mathrm{MLP}(\\mathrm{LN}(\\mathbf z&#39;_l)) + \\mathbf z&#39;_l,\\]\\[\\mathbf y = \\mathrm{LN}(\\mathbf z^0_L), l = 1 ... L\\][@ Inductive bias]앞서 저자들은 Vision Transformer가 이미지에서의 inductive bias가 더 적다고 언급했다.CNN에서는 전체 모델에 걸쳐 localty 및 2차원 neightborhood structure나 translation equivatiance 등의 inductive bias가 각 레이어에서 반영된다.하지만 ViT에선 MLP에서만 이러한 특징들이 반영될 수 있으며 self-attention 레이어들은 모두 전역적인 상관관계를 잡아내는 데 특화된 레이어이다.[@ Hybrid Architecture]이미지 패치를 단순 시퀀스로 펼쳐서 사용하는 대신 CNN의 feature map을 사용하는 방법이 대안이 될 수 있다. 입력 이미지를 다양한 CNN을 통해 feature map을 추출하고 flatten -&amp;gt; Transformer의 입력 차원으로 projection을 수행하면 Transformer의 입력 시퀀스의 형태로 변환할 수 있다.3.2 Fine-Tuning and Higher ResolutionViT는 이전의 Transformer가 그랬던 것처럼 사전에 거대한 데이터에 학습되고 downstream task 목적에 맞게 미세 조정되어 사용되는 것이 더욱더 효과적이다. 사전에 학습된 prediction head를 제거하고 0으로 초기화된 $D\\times K$ feedforward 레이어를 추가하여 downstream task의 클래스 수에 맞게 결과값의 형태를 변형해주었다.[@ position embedding interpolation] 높은 해상도의 이미지로 미세 조정을 할 때 유용한 점이 많았다. 이 때에도 patch size는 동일하게 유지했고 그 결과로 입력 시퀀스의 길이가 달라지게 된다. 입력 시퀀스는 길이가 달라지더라도 괜찮지만 이와 동반하는 position embedding은 더 이상 의미를 가지지 못하게 된다. 따라서 2D interpolation을 통해 사전학습된 position embedding에 변화를 주었다. 4 Experiments4.1 Setup[@ Datasets] Train ILSVRC-2012 ImageNet: 1K classes, 1.3M images ImageNet-21K: 21K classes, 14M images JFT: 18K classes, 303M high-resolution images transfer learning ImageNet: original validation set, ReaL labels는 제거 CIFAR-10/100 Oxford-IIIT Pets Oxford Flowers-102 VTAB: 1000개의 작은 학습 샘플을 통한 미세조정을 통해 전이학습의 성능을 파악하는 classification suite이다. 세 가지 task는 아래와 같이 구성된다. Natural - Pets, CIFAR, etc. Specialized - medical and satelite imagery Stuructured - geometric understanding [@ model variants]Devlin et al.에서의 BERT 세팅과 동일하게 설정ViT-L/16은 Large모델의 $16\\times 16$ patch size를 적용한 모델을 나타낸다. 저자들은 patch size가 줄어들수록 입력 이미지가 더 잘게 쪼개지며 Transformer encoder의 입력 토큰 수가 늘어나므로 계산량이 더 늘어난다고 강조하고 있다.[@ Baseline CNNs] ResNet (BiT) ResNet 베이스 Batch Normalization을 Group Normalization으로 대체 대체함으로써 전이 학습의 성능을 올릴 수 있음 Hybrids model (Section 3.1의 [@ Hybrid Architecture]) CNN feature map의 한 픽셀을 하나의 Transformer 입력 토큰의 크기(=patch size)으로 함 (i). regular ResNet50의 Stage 4 output을 사용 (ii). (i)에서의 모델에서 Stage 4를 제거하고 Stage 3의 output을 사용단, (i)과 (ii)의 output 레이어 수를 동일하게 하기 위해 (ii)의 output인 Stage 3의 레이어 수를 Stage 4의 레이어 수와 같게 하였음 [@ Training &amp;amp; Fine-tuning] From scratch Adam optimizer, $\\beta_1 =0.9, \\beta_2=0.999$ batch size $=4096$ weight decay $=0.1$ Fine-tuning SGD optimizer batch size $=512$ High resolution $512$ for ViT-L/16 $518$ for ViT-H/14 [@ Metrics]Fine-tuning에 대한 결과를 few-shot accuracy 또는 fine-tuning accuracy로 평가했다. Fine-tuning accuracy - 각각 fine-tuning 대상인 downstream 데이터셋에 대한 정확도를 의미한다. Few-show accuracy Fine-tuning accuracy를 계산하기에 downstream 데이터셋이 너무 큰 경우에 대체적인 척도 학습 이미지의 특징 표현(representation)을 $[-1, 1]^K$로 정의되는 target vector로 “Regulizerd least-squares regression”으로 풀어내는 과정에서 퍼포먼스를 측정한다. $K$는 downstream 데이터셋의 클래스 수 4.2 Comparison to State of the Art본 논문에서 제안하는 ViT-L/16이 기존 State-of-the-art 방법들에 비해 성능도 좋았으며 컴퓨팅 비용 면에서도 효율적이었다. 심지어 더 큰 모델 ViT-H/14는 이를 더 뛰어넘는 성능을 보였다.4.3 Pre-training Data RequirementsFigure 3. Vision Transformer는 거대한 대규모 데이터셋인 JFT-300M에 사전 학습했을 때 좋은 성능을 보였다. ImageNet에 학습할 때에는 weight decay, dropout, label smoothing과 같은 다양한 regularization을 적용했음에도 기존 ResNet 기반 모델보다 성능이 좋지 않았다. ImageNet-21K에 학습할 때에는 기존 ResNet과 성능이 비슷하다. JFT-300M에 학습했을 때에만 기존 모델의 성능을 뛰어넘을 수 있었다.Figure 4. 두 번째로 JFT-300M에 랜덤 샘플링을 거쳐 각각 9M, 30M, 90M 크기의 subset을 만들어 실험을 진행했다. 다른 추가적인 regularization이나 hyper-parameters의 세팅을 모두 동일하게 진행했다. 역시 작은 크기의 데이터셋에서 학습할 경우 기존 방법들이 더 성능이 좋았다. CNN의 inductive bias가 작은 데이터셋에는 효과적으로 작동할지 모르지만 데이터가 굉장한 규모로 주어질 때는 이미지 내의 패턴들을 곧바로 학습하는 것이 효과적임을 알 수 있었다.4.4 Scaling Study여기에서는 JFT-300M에서 사전학습하고 전이 학습에서의 성능을 비교한다. ResNet 기반의 세팅 7가지, Vision Transformer 6가지, Hybrids 모델 5가지 세팅을 비교한다.[@ Figure 5.] Vision Transformer는 ResNets 보다 performance/compute의 trade-off를 두 배에서 네 배 가량 압도하는 모습을 보였다. Hybrid 모델들은 계산 비용이 적었을 때 Vision Transformer의 성능을 앞서는 모습을 보였다. 이는 계산 비용이 늘어남에 따라 그 차이가 줄어들기는 한다.4.5 Inspecting Vision Transformer[@ How the Vision Transformer processes image data]Vision Transformer의 첫 레이어는 flattened 이미지를 저차원으로 매핑하게 된다. 이 과정에서 학습된 embedding filter의 상위 구성요소를 시각화 했을 때(Figure 7. Left), CNN에서 볼 수 있었던 저차원에서의 basis function 들을 닯아있음을 볼 수 있다.[@ position embedding]Transformer encoder로 입력이 주어지기 전에 position embedding을 각각 토큰에 추가해주었다. 이 각 패치에서 학습된 position embedding의 유사도를 시각화 했을 때(Figure 7. Center), 각 패치들이 비슷한 위치에 있을 때 비슷한 position embedding을 가지는 것을 볼 수 있다.이를 통해 이미지 내부의 거리 개념을 encoding하고 있음을 알 수 있다.[@ Can Self-attention allows ViT to see the entire image?]Self-attention이 실제로 이미지 전체에 대해서 정보를 취합할 수 있도록 하는지 살펴본다.이미지 내부에서 정보가 결합되는 공간 간의 거리의 평균을 attention 가중치를 기반으로 계산했고 이를 시각화 했다.(Figure 7. Right)모델의 낮은 레이어에서부터 이미지 전체에서 정보를 결합함을 확인할 수 있다. 점점 레이어 위치가 높아질수록 평균 attention distance가 좁아지며 local하게 정보를 결합한다.attend하는 물체도 의미있는 단위로 모델이 잘 attend하고 있음을 Figure 6.에서 확인할 수 있다." }, { "title": "TFRecord 파일 읽고 쓰기", "url": "/posts/tfrecord/", "categories": "project-review", "tags": "tensorflow, tfrecord, data_loader", "date": "2021-05-29 12:00:00 +0900", "snippet": "학부 졸업 프로젝트를 수행하며 tensorflow를 통해 딥러닝 모델을 구현하던 중 tensorflow의 학습 데이터 포맷인 tfrecord라는 파일을 만날 수 있었습니다. 딥러닝 모델의 구현을 위해 github을 뒤져보던 중 *.tfrecord라는 새로운 파일 확장자를 만났고, 용량도 별로 되지 않는 것이 모든 데이터를 담고 있다는 것을 보니 머릿속에 “다뤄야 한다”라는 생각이 들었습니다.그렇다면 tfrecord 파일 포맷은 무엇인가??tfrecord tfrecord 파일은 tensorflow의 학습 데이터 등을 저장하기 위한 바이너리 데이터 포맷으로, 구글의 protocol buffer 포맷으로 데이터를 파일에 serialize하여 저장하게 됩니다. tfrecord 파일의 필요성은 아래와 같습니다. (from. 조대협님의 블로그) csv파일에서와 같이 숫자나 텍스트 데이터를 읽을 때는 크게 지장이 없지만, 이미지 데이터를 읽을 경우 이미지는 jpeg나 png 형태의 파일로 저장되어 있고 이에 대한 메타데이터와 레이블은 별도의 파일에 저장되어 있기 때문에, 학습 데이터를 읽을 때 메타데이터나 레이블 파일 하나만 읽는 것이 아니라 이미지 파일도 별도로 읽어야 하기 때문에 코드가 복잡해진다. 이미지를 jpg난 png 포맷으로 읽어서 매번 디코딩을 하게되면, 그 성능이 저하되서 학습단계에서 데이터를 읽는 부분에서 많은 성능 저하가 발생한다. 이는 저 역시도 딥러닝을 공부하고 모델을 돌려보면서 많이 느꼈던 애로사항입니다. 좋은 하드웨어를 구하지 못해서 구글의 colab 환경을 많이 사용했는데, 데이터를 구글 드라이브에 옮기는 시간은 물론이고 이를 다시 학습 환경으로 가져오는 시간도 길어지는 경우가 종종 있었습니다.tfrecord 파일 생성아래의 내용은 tensorflow 공식문서와 ‘peteryuX’님의 얼굴인식 github 레포지토리를 참고했습니다.tfrecord 파일 생성은 기록하고자 하는 데이터의 feature들을 python dictionary 형태로 정의한 후, 데이터 하나씩 tf.train.Example 객체로 만들어 tf.io.TFRecordWriter를 통해 파일로 저장합니다.1. 데이터 변환tensorflow 공식문서에 따르면 tf.train.Example 객체는 아래와 같은 유형의 데이터 타입을 담을 수 있습니다. tf.train.BytesList string byte tf.train.FloatList float (float32) double (float64) tf.train.Int64List bool enum int32 uint32 int64 uint64 기록하고자하는 데이터의 타입에 따라 아래와 같은 변환 함수를 사용해 데이터를 기록할 수 있습니다. 예를 들어 문자열 타입의 데이터는 tf.train.BytesList의 객체로 기록하는 것이죠.# The following functions can be used to convert a value to a type compatible# with tf.train.Example.def _bytes_feature(value): &quot;&quot;&quot;Returns a bytes_list from a string / byte.&quot;&quot;&quot; if isinstance(value, type(tf.constant(0))): value = value.numpy() # BytesList won&#39;t unpack a string from an EagerTensor. return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))def _float_feature(value): &quot;&quot;&quot;Returns a float_list from a float / double.&quot;&quot;&quot; return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))def _int64_feature(value): &quot;&quot;&quot;Returns an int64_list from a bool / enum / int / uint.&quot;&quot;&quot; return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))2. tf.train.Example 객체로 저장def make_example(img_str, source_id, filename): # Create a dictionary with features that may be relevant. feature = {&#39;image/source_id&#39;: _int64_feature(source_id), &#39;image/filename&#39;: _bytes_feature(filename), &#39;image/encoded&#39;: _bytes_feature(img_str)} return tf.train.Example(features=tf.train.Features(feature=feature))기록할 데이터들을 python dictionary 형태로 묶어 tf.train.Example 객체로 변환합니다. 위 예제코드에서는 얼굴인식을 위한 데이터를 저장하는 것인데, 각각 source_id는 클래스 ID를, filename은 이미지 파일의 이름, encoded는 이미지 파일 자체를 의미합니다.3. tf.io.TFRecordWriter로 파일 생성원본 데이터의 디렉토리 구조는 아래와 같이 일반적인 Classification 데이터의 구조를 가진다고 하겠습니다..|-- 0| |-- ****.jpg| |-- ****.jpg| |-- ...||-- 1| |-- ****.jpg| |-- ***.jpg| |-- ...||-- 2| |-- ****.jpg| |-- ......아래는 위 디렉토리 구조에서 파일의 이름을 입력받아 tfrecord 파일을 생성하는 코드입니다. 0, 1, 2, ...의 디렉토리 이름을 클래스 ID로 하고 차례로 sample python list에 담아 하나씩 저장합니다. 그 다음 list에 담은 샘플들을 섞고 다시 차례로 *.tfrecord 파일에 적습니다.def main(dataset_path, output_path): samples = [] print(&quot;Reading data list...&quot;) for id_name in tqdm(os.listdir(dataset_path)): img_paths = glob(os.path.join(dataset_path, id_name, &#39;*.jpg&#39;)) for img_path in img_paths: filename = os.path.join(id_name, os.path.basename(img_path)) samples.append((img_path, id_name, filename)) random.shuffle(samples) print(&quot;Writing tfrecord file...&quot;) with tf.io.TFRecordWriter(output_path) as writer: for img_path, id_name, filename in tqdm(samples): tf_example = make_example(img_str=open(img_path, &#39;rb&#39;).read(), source_id=int(id_name), filename=str.encode(filename)) writer.write(tf_example.SerializeToString())회귀 문제에 대한 tfrecord 파일 생성 예제를 하나 더 보면 좋을 것 같습니다.회귀 문제에 대한 label은 .csv나 .txt 같은 별도의 파일에 저장하는 경우가 많습니다. 아래는 이미지 파일명과 그 label 값을 함께 적어놓은 .csv 파일의 예시입니다.filename,label0000971160_1, 6.0944670652239750000971160_2, -8.2135488998097830000971160_3, -16.8114679438633770000971160_4, -0.444349466961666550000971160_5, -26.6815450045318520000971160_6, 17.0063921024588750000971160_7, -18.6969816887455130000971160_8, 16.358331838072285위와 같이 저장된 파일도 같은 방식으로 tfrecord 파일로 저장할 수 있습니다. 달라진 점이라면 메타데이터에서 정보를 가져와 활용한다는 것과 그에 맞는 label의 type에 맞게 make_example을 조정했다는 점입니다.def make_example(img_str, label, filename): feature = {&#39;encoded&#39;: _bytes_feature(img_str), &#39;label&#39;: _float_feature(label), &#39;filename&#39;: _bytes_feature(filename)} return tf.train.Example(features=tf.train.Features(feature=feature)) def main(dataset_path, output_path): samples = [] with open(f&quot;{dataset_path}/metadata.csv&quot;) as f: f.readline() lines = f.readlines() for line in tqdm(lines): filename, label = line.split(&#39;,&#39;) img_path = os.path.join(dataset_path, mode.split(&#39;_&#39;)[0], f&quot;{filename}.png&quot;) label = float(label.replace(&#39;\\n&#39;, &#39;&#39;)) samples.append((img_path, label, filename)) random.shuffle(samples) with tf.io.TFRecordWriter(output_path) as writer: for img_path, label, filename in tqdm(samples): tf_example = make_example(img_str=open(img_path, &#39;rb&#39;).read(), label=label, filename=str.encode(filename+&quot;.png&quot;)) writer.write(tf_example.SerializeToString())tfrecord 파일 tf.data.Dataset으로 변환이렇게 만든 tfrecord 파일을 읽는 방법 역시 간단합니다.def _parse_tfrecord(): def parse_tfrecord(tfrecord): features = {&#39;image/source_id&#39;: tf.io.FixedLenFeature([], tf.int64), &#39;image/filename&#39;: tf.io.FixedLenFeature([], tf.string), &#39;image/encoded&#39;: tf.io.FixedLenFeature([], tf.string)} x = tf.io.parse_single_example(tfrecord, features) x_train = tf.image.decode_jpeg(x[&#39;image/encoded&#39;], channels=3) y_train = tf.cast(x[&#39;image/source_id&#39;], tf.float32) x_train = _transform_images()(x_train) y_train = _transform_targets(y_train) return (x_train, y_train), y_train return parse_tfrecorddef _transform_images(): def transform_images(x_train): x_train = tf.image.resize(x_train, (128, 128)) x_train = tf.image.random_crop(x_train, (112, 112, 3)) x_train = tf.image.random_flip_left_right(x_train) x_train = tf.image.random_saturation(x_train, 0.6, 1.4) x_train = tf.image.random_brightness(x_train, 0.4) x_train = x_train / 255 return x_train return transform_imagesdef _transform_targets(y_train): return y_train기록한 tfrecord 파일을 입력받아 tf.io.parse_single_example()로 하나씩 풀어주면 됩니다. 이렇게 불러온 데이터를 사용하고자하는 모델에 맞게 전처리해주고 알맞은 형태의 return 값을 반환합니다.def load_tfrecord_dataset(tfrecord_name, batch_size, shuffle=True, buffer_size=10240): &quot;&quot;&quot;load dataset from tfrecord&quot;&quot;&quot; raw_dataset = tf.data.TFRecordDataset(tfrecord_name) raw_dataset = raw_dataset.repeat() if shuffle: raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size) dataset = raw_dataset.map( _parse_tfrecord(), num_parallel_calls=tf.data.experimental.AUTOTUNE ) dataset = dataset.batch(batch_size) dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE) return datasettf.data.TFRecordDataset의 객체를 생성해 .map()을 통해 앞서 정의한 변환과정을 적용해주면 데이터 사용 준비가 마무리됩니다.tfrecord와 Generator의 속도 비교그렇다면 python generator를 사용한 tensorflow 데이터셋과 tfrecord를 사용한 tensorflow 데이터셋의 시간 차이를 살펴보겠습니다.colab 환경에서 진행했고 위에서 소개한 classification 문제에 대한 데이터셋을 비교에 사용했습니다. 총 이미지 수는 9360장, 클래스의 종류는 390가지입니다.왼쪽은 tfrecord를 사용한 시간으로 5초 남짓 소요되었지만, 오른쪽 Generator를 통해 디렉토리에서 직접 데이터를 가져오는 방식은 90분 정도의 어마어마한 차이를 보였습니다." } ]
